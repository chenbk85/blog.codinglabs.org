<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>CodingLabs</title>
        <link>http://blog.codinglabs.org</link>
        <description>keep coding, keep foolish</description>
        <lastBuildDate>Thu, 24 Jan 2013 13:57:51 GMT</lastBuildDate>
        <language>zh-cn</language>
        <item>
<title>解读Cardinality Estimation算法（第四部分：HyperLogLog Counting及Adaptive Counting）</title>
<link>http://blog.codinglabs.org/articles/algorithms-for-cardinality-estimation-part-iv.html</link>
<guid>http://blog.codinglabs.org/articles/algorithms-for-cardinality-estimation-part-iv.html</guid>
<pubDate>Tue, 08 Jan 2013 16:00:00 GMT</pubDate>
<description>&lt;p&gt;在&lt;a href=&quot;http://www.codinglabs.org/html/algorithms-for-cardinality-estimation-part-iii.html&quot; target=&quot;_blank&quot;&gt;前一篇文章&lt;/a&gt;中，我们了解了LogLog Counting。LLC算法的空间复杂度为\(O(log_2(log_2(N_{max})))\)，并且具有较高的精度，因此非常适合用于大数据场景的基数估计。不过LLC也有自己的问题，就是当基数不太大时，估计值的误差会比较大。这主要是因为当基数不太大时，可能存在一些空桶，这些空桶的\(\rho_{max}\)为0。由于LLC的估计值依赖于各桶\(\rho_{max}\)的几何平均数，而几何平均数对于特殊值（这里就是指0）非常敏感，因此当存在一些空桶时，LLC的估计效果就变得较差。&lt;/p&gt;
&lt;p&gt;这一篇文章中将要介绍的HyperLogLog Counting及Adaptive Counting算法均是对LLC算法的改进，可以有效克服LLC对于较小基数估计效果差的缺点。&lt;/p&gt;
&lt;!--more--&gt;
&lt;h1&gt;评价基数估计算法的精度&lt;/h1&gt;
&lt;p&gt;首先我们来分析一下LLC的问题。一般来说LLC最大问题在于当基数不太大时，估计效果比较差。上文说过，LLC的渐近标准误差为\(1.30/\sqrt{m}\)，看起来貌似只和分桶数m有关，那么为什么基数的大小也会导致效果变差呢？这就需要重点研究一下如何评价基数估计算法的精度，以及“渐近标准误差”的意义是什么。&lt;/p&gt;
&lt;h2&gt;标准误差&lt;/h2&gt;
&lt;p&gt;首先需要明确标准误差的意义。例如标准误差为0.02，到底表示什么意义。&lt;/p&gt;
&lt;p&gt;标准误差是针对一个统计量（或估计量）而言。在分析基数估计算法的精度时，我们关心的统计量是\(\hat{n}/n\)。注意这个量分子分母均为一组抽样的统计量。下面正式描述一下这个问题。&lt;/p&gt;
&lt;p&gt;设S是我们要估计基数的可重复有限集合。S中每个元素都是来自值服从均匀分布的样本空间的一个独立随机抽样样本。这个集合共有C个元素，但其基数不一定是C，因为其中可能存在重复元素。设\(f_n\)为定义在S上的函数：&lt;/p&gt;
&lt;p&gt;\(f_n(S) = Cardinality\;of\;S\)&lt;/p&gt;
&lt;p&gt;同时定义\(f_\hat{n}\)也是定义在S上的函数：&lt;/p&gt;
&lt;p&gt;\(f_\hat{n}(S)=LogLog\;estimate\;value\;of\;S\)&lt;/p&gt;
&lt;p&gt;我们想得到的第一个函数值，但是由于第一个函数值不好计算，所以我们计算同样集合的第二个函数值来作为第一个函数值得估计。因此最理想的情况是对于任意一个集合两个函数值是相等的，如果这样估计就是100%准确了。不过显然没有这么好的事，因此我们退而求其次，只希望\(f_\hat{n}(S)\)是一个无偏估计，即：&lt;/p&gt;
&lt;p&gt;\(E(\frac{f_\hat{n}(S)}{f_n(S)})=1\)&lt;/p&gt;
&lt;p&gt;这个在上一篇文章中已经说明了。同时也可以看到，\(\frac{f_\hat{n}(S)}{f_n(S)}\)实际上是一个随机变量，并且服从正态分布。对于正态分布随机变量，一般可以通过标准差\(\sigma\)度量其稳定性，直观来看，标准差越小，则整体分布越趋近于均值，所以估计效果就越好。这是定性的，那么定量来看标准误差\(\sigma\)到底表达了什么意思呢。它的意义是这样的：&lt;/p&gt;
&lt;p&gt;对于无偏正态分布而言，随机变量的一次随机取值落在均值一个标准差范围内的概率是68.2%，而落在两个和三个标准差范围内的概率分别为95.4%和99.6%，如下图所示（图片来自&lt;a href=&quot;http://zh.wikipedia.org/wiki/%E6%A0%87%E5%87%86%E8%AF%AF&quot; target=&quot;_blank&quot;&gt;维基百科&lt;/a&gt;）：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/algorithms-for-cardinality-estimation-part-iv/1.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;因此，假设标准误差是0.02（2%），它实际的意义是：假设真实基数为n，n与估计值之比落入(0.98, 1.02)的概率是68.2%，落入(0.96, 1.04)的概率是95.4%，落入(0.94, 1.06)的概率是99.6%。显然这个比值越大则估计值越不准，因此对于0.02的标准误差，这个比值大于1.06或小于0.94的概率不到0.004。&lt;/p&gt;
&lt;p&gt;再直观一点，假设真实基数为10000，则一次估计值有99.6%的可能不大于10600且不小于9400。&lt;/p&gt;
&lt;h2&gt;组合计数与渐近分析&lt;/h2&gt;
&lt;p&gt;如果LLC能够做到绝对服从\(1.30/\sqrt{m}\)，那么也算很好了，因为我们只要通过控制分桶数m就可以得到一个一致的标准误差。这里的一致是指标准误差与基数无关。不幸的是并不是这样，上面已经说过，这是一个“渐近”标注误差。下面解释一下什么叫渐近。&lt;/p&gt;
&lt;p&gt;在计算数学中，有一个非常有用的分支就是组合计数。组合计数简单来说就是分析自然数的组合函数随着自然数的增长而增长的量级。可能很多人已经意识到这个听起来很像算法复杂度分析。没错，算法复杂度分析就是组合计数在算法领域的应用。&lt;/p&gt;
&lt;p&gt;举个例子，设A是一个有n个元素的集合（这里A是严格的集合，不存在重复元素），则A的幂集（即由A的所有子集组成的集合）有\(2^n\)个元素。&lt;/p&gt;
&lt;p&gt;上述关于幂集的组合计数是一个非常整齐一致的组合计数，也就是不管n多大，A的幂集总有\(2^n\)个元素。&lt;/p&gt;
&lt;p&gt;可惜的是现实中一般的组合计数都不存在如此干净一致的解。LLC的偏差和标准差其实都是组合函数，但是论文中已经分析出，LLC的偏差和标准差都是渐近组合计数，也就是说，随着n趋向于无穷大，标准差趋向于\(1.30/\sqrt{m}\)，而不是说n多大时其值都一致为\(1.30/\sqrt{m}\)。另外，其无偏性也是渐近的，只有当n远远大于m时，其估计值才近似无偏。因此当n不太大时，LLC的效果并不好。&lt;/p&gt;
&lt;p&gt;庆幸的是，同样通过统计分析方法，我们可以得到n具体小到什么程度我们就不可忍受了，另外就是当n太小时可不可以用别的估计方法替代LLC来弥补LLC这个缺陷。HyperLogLog Counting及Adaptive Counting都是基于这个思想实现的。&lt;/p&gt;
&lt;h1&gt;Adaptive Counting&lt;/h1&gt;
&lt;p&gt;Adaptive Counting（简称AC）在“Fast and accurate traffic matrix measurement using adaptive cardinality counting”一文中被提出。其思想也非常简单直观：实际上AC只是简单将LC和LLC组合使用，根据基数量级决定是使用LC还是LLC。具体是通过分析两者的标准差，给出一个阈值，根据阈值选择使用哪种估计。&lt;/p&gt;
&lt;h2&gt;基本算法&lt;/h2&gt;
&lt;p&gt;如果分析一下LC和LLC的存储结构，可以发现两者是兼容的，区别仅仅在于LLC关心每个桶的\(\rho_{max}\)，而LC仅关心此桶是否为空。因此只要简单认为\(\rho_{max}\)值不为0的桶为非空，0为空就可以使用LLC的数据结构做LC估计了。&lt;/p&gt;
&lt;p&gt;而我们已经知道，LC在基数不太大时效果好，基数太大时会失效；LLC恰好相反，因此两者有很好的互补性。&lt;/p&gt;
&lt;p&gt;回顾一下，LC的标准误差为：&lt;/p&gt;
&lt;p&gt;\(SE_{lc}(\hat{n}/n)=\sqrt{e^t-t-1}/(t\sqrt{m})\)&lt;/p&gt;
&lt;p&gt;LLC的标准误差为：&lt;/p&gt;
&lt;p&gt;\(SE_{llc}(\hat{n}/n)=1.30/\sqrt{m}\)&lt;/p&gt;
&lt;p&gt;将两个公式联立：&lt;/p&gt;
&lt;p&gt;\(\sqrt{e^t-t-1}/(t\sqrt{m})=1.30/\sqrt{m}\)&lt;/p&gt;
&lt;p&gt;解得\(t \approx 2.89\)。注意m被消掉了，说明这个阈值与m无关。其中\(t=n/m\)。&lt;/p&gt;
&lt;p&gt;设\(\beta\)为空桶率，根据LC的估算公式，带入上式可得：&lt;/p&gt;
&lt;p&gt;\(\beta = e^{-t} \approx 0.051\)&lt;/p&gt;
&lt;p&gt;因此可以知道，当空桶率大于0.051时，LC的标准误差较小，而当小于0.051时，LLC的标准误差较小。&lt;/p&gt;
&lt;p&gt;完整的AC算法如下：&lt;/p&gt;
&lt;p&gt;\(\hat{n}=\left\{ \begin{eqnarray} \alpha_m m2^{\frac{1}{m}\sum{M}} &amp; if &amp; 0 \leq \beta &lt; 0.051 \\ -mlog(\beta) &amp; if &amp; 0.051 \leq \beta \leq 1 \end{eqnarray} \right.\)&lt;/p&gt;
&lt;h2&gt;误差分析&lt;/h2&gt;
&lt;p&gt;因为AC只是LC和LLC的简单组合，所以误差分析可以依照LC和LLC进行。值得注意的是，当\(\beta &lt; 0.051\)时，LLC最大的偏差不超过0.17%，因此可以近似认为是无偏的。&lt;/p&gt;
&lt;h1&gt;HyperLogLog Counting&lt;/h1&gt;
&lt;p&gt;HyperLogLog Counting（以下简称HLLC）的基本思想也是在LLC的基础上做改进，不过相对于AC来说改进的比较多，所以相对也要复杂一些。本文不做具体细节分析，具体细节请参考“HyperLogLog: the analysis of a near-optimal cardinality estimation algorithm”这篇论文。&lt;/p&gt;
&lt;h2&gt;基本算法&lt;/h2&gt;
&lt;p&gt;HLLC的第一个改进是使用调和平均数替代几何平均数。注意LLC是对各个桶取算数平均数，而算数平均数最终被应用到2的指数上，所以总体来看LLC取得是几何平均数。由于几何平均数对于离群值（例如这里的0）特别敏感，因此当存在离群值时，LLC的偏差就会很大，这也从另一个角度解释了为什么n不太大时LLC的效果不太好。这是因为n较小时，可能存在较多空桶，而这些特殊的离群值强烈干扰了几何平均数的稳定性。&lt;/p&gt;
&lt;p&gt;因此，HLLC使用调和平均数来代替几何平均数，调和平均数的定义如下：&lt;/p&gt;
&lt;p&gt;\(H = \frac{n}{\frac{1}{x_1} + \frac{1}{x_2} + ... + \frac{1}{x_n}} = \frac{n}{\sum_{i=1}^n \frac{1}{x_i}}\)&lt;/p&gt;
&lt;p&gt;调和平均数可以有效抵抗离群值的扰动。使用调和平均数代替几何平均数后，估计公式变为如下：&lt;/p&gt;
&lt;p&gt;\(\hat{n}=\frac{\alpha_m m^2}{\sum{2^{-M}}}\)&lt;/p&gt;
&lt;p&gt;其中：&lt;/p&gt;
&lt;p&gt;\(\alpha_m=(m\int _0^\infty (log_2(\frac{2+u}{1+u}))^m du)^{-1}\)&lt;/p&gt;
&lt;h2&gt;偏差分析&lt;/h2&gt;
&lt;p&gt;根据论文中的分析结论，与LLC一样HLLC是渐近无偏估计，且其渐近标准差为：&lt;/p&gt;
&lt;p&gt;\(SE_{hllc}(\hat{n}/n)=1.04/\sqrt{m}\)&lt;/p&gt;
&lt;p&gt;因此在存储空间相同的情况下，HLLC比LLC具有更高的精度。例如，对于分桶数m为2^13（8k字节）时，LLC的标准误差为1.4%，而HLLC为1.1%。&lt;/p&gt;
&lt;h2&gt;分段偏差修正&lt;/h2&gt;
&lt;p&gt;在HLLC的论文中，作者在实现建议部分还给出了在n相对于m较小或较大时的偏差修正方案。具体来说，设E为估计值：&lt;/p&gt;
&lt;p&gt;当\(E \leq \frac{5}{2}m\)时，使用LC进行估计。&lt;/p&gt;
&lt;p&gt;当\(\frac{5}{2}m &lt; E \leq \frac{1}{30}2^{32}\)是，使用上面给出的HLLC公式进行估计。&lt;/p&gt;
&lt;p&gt;当\(E &gt; \frac{1}{30}2^{32}\)时，估计公式如为\(\hat{n}=-2^{32}log(1-E/2^{32})\)。&lt;/p&gt;
&lt;p&gt;关于分段偏差修正效果分析也可以在原论文中找到。&lt;/p&gt;
&lt;h1&gt;小结&lt;/h1&gt;
&lt;p&gt;本文首先介绍了基数估计算法标准误差的意义，并据此说明了为什么LLC在基数较小时效果不好。然后，以此介绍了两种对LLC的改进算法：HyperLogLog Counting及Adaptive Counting。到此为止，常见的四种基数估计算法就介绍完了。在本系列最后一篇文章中，我会介绍一淘数据部的基数估计实现&lt;a href=&quot;https://github.com/chaoslawful/ccard-lib&quot; target=&quot;_blank&quot;&gt;ccard-lib&lt;/a&gt;的一些实现细节和使用方式。并做一些实验。&lt;/p&gt;
</description>
</item>
<item>
<title>解读Cardinality Estimation算法（第三部分：LogLog Counting）</title>
<link>http://blog.codinglabs.org/articles/algorithms-for-cardinality-estimation-part-iii.html</link>
<guid>http://blog.codinglabs.org/articles/algorithms-for-cardinality-estimation-part-iii.html</guid>
<pubDate>Wed, 02 Jan 2013 16:00:00 GMT</pubDate>
<description>&lt;p&gt;&lt;a href=&quot;/articles/algorithms-for-cardinality-estimation-part-ii.html&quot; target=&quot;_blank&quot;&gt;上一篇文章&lt;/a&gt;介绍的Linear Counting算法相较于直接映射bitmap的方法能大大节省内存（大约只需后者1/10的内存），但毕竟只是一个常系数级的降低，空间复杂度仍然为\(O(N_{max})\)。而本文要介绍的LogLog Counting却只有\(O(log_2(log_2(N_{max})))\)。例如，假设基数的上限为1亿，原始bitmap方法需要12.5M内存，而LogLog Counting只需不到1K内存（640字节）就可以在标准误差不超过4%的精度下对基数进行估计，效果可谓十分惊人。&lt;/p&gt;
&lt;p&gt;本文将介绍LogLog Counting。
&lt;!--more--&gt;
&lt;/p&gt;
&lt;h1&gt;简介&lt;/h1&gt;
&lt;p&gt;LogLog Counting（以下简称LLC）出自论文“Loglog Counting of Large Cardinalities”。LLC的空间复杂度仅有\(O(log_2(log_2(N_{max})))\)，使得通过KB级内存估计数亿级别的基数成为可能，因此目前在处理大数据的基数计算问题时，所采用算法基本为LLC或其几个变种。下面来具体看一下这个算法。&lt;/p&gt;
&lt;h1&gt;基本算法&lt;/h1&gt;
&lt;h2&gt;均匀随机化&lt;/h2&gt;
&lt;p&gt;与LC一样，在使用LLC之前需要选取一个哈希函数H应用于所有元素，然后对哈希值进行基数估计。H必须满足如下条件（定性的）：&lt;/p&gt;
&lt;p&gt;1、H的结果具有很好的均匀性，也就是说无论原始集合元素的值分布如何，其哈希结果的值几乎服从均匀分布（完全服从均匀分布是不可能的，D. Knuth已经证明不可能通过一个哈希函数将一组不服从均匀分布的数据映射为绝对均匀分布，但是很多哈希函数可以生成几乎服从均匀分布的结果，这里我们忽略这种理论上的差异，认为哈希结果就是服从均匀分布）。&lt;/p&gt;
&lt;p&gt;2、H的碰撞几乎可以忽略不计。也就是说我们认为对于不同的原始值，其哈希结果相同的概率非常小以至于可以忽略不计。&lt;/p&gt;
&lt;p&gt;3、H的哈希结果是固定长度的。&lt;/p&gt;
&lt;p&gt;以上对哈希函数的要求是随机化和后续概率分析的基础。后面的分析均认为是针对哈希后的均匀分布数据进行。&lt;/p&gt;
&lt;h2&gt;思想来源&lt;/h2&gt;
&lt;p&gt;下面非正式的从直观角度描述LLC算法的思想来源。&lt;/p&gt;
&lt;p&gt;设a为待估集合（哈希后）中的一个元素，由上面对H的定义可知，a可以看做一个长度固定的比特串（也就是a的二进制表示），设H哈希后的结果长度为L比特，我们将这L个比特位从左到右分别编号为1、2、…、L：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/algorithms-for-cardinality-estimation-part-iii/1.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;又因为a是从服从均与分布的样本空间中随机抽取的一个样本，因此a每个比特位服从如下分布且相互独立。&lt;/p&gt;
&lt;p&gt;\(P(x=k)=\left\{\begin{matrix} 0.5 (k=0)\\ 0.5 (k=1)\end{matrix}\right.\)&lt;/p&gt;
&lt;p&gt;通俗说就是a的每个比特位为0和1的概率各为0.5，且相互之间是独立的。&lt;/p&gt;
&lt;p&gt;设\(\rho(a)\)为a的比特串中第一个“1”出现的位置，显然\(1 \leq \rho(a) \leq L\)，这里我们忽略比特串全为0的情况（概率为\(1/2^L\)）。如果我们遍历集合中所有元素的比特串，取\(\rho_{max}\)为所有\(\rho(a)\)的最大值。&lt;/p&gt;
&lt;p&gt;此时我们可以将\(2^{\rho_{max}}\)作为基数的一个粗糙估计，即：&lt;/p&gt;
&lt;p&gt;\(\hat{n} = 2^{\rho_{max}}\)&lt;/p&gt;
&lt;p&gt;下面解释为什么可以这样估计。注意如下事实：&lt;/p&gt;
&lt;p&gt;由于比特串每个比特都独立且服从0-1分布，因此从左到右扫描上述某个比特串寻找第一个“1”的过程从统计学角度看是一个伯努利过程，例如，可以等价看作不断投掷一个硬币（每次投掷正反面概率皆为0.5），直到得到一个正面的过程。在一次这样的过程中，投掷一次就得到正面的概率为\(1/2\)，投掷两次得到正面的概率是\(1/2^2\)，…，投掷k次才得到第一个正面的概率为\(1/2^k\)。&lt;/p&gt;
&lt;p&gt;现在考虑如下两个问题：&lt;/p&gt;
&lt;p&gt;1、进行n次伯努利过程，所有投掷次数都不大于k的概率是多少？&lt;/p&gt;
&lt;p&gt;2、进行n次伯努利过程，至少有一次投掷次数等于k的概率是多少？&lt;/p&gt;
&lt;p&gt;首先看第一个问题，在一次伯努利过程中，投掷次数大于k的概率为\(1/2^k\)，即连续掷出k个反面的概率。因此，在一次过程中投掷次数不大于k的概率为\(1-1/2^k\)。因此，n次伯努利过程投掷次数均不大于k的概率为：&lt;/p&gt;
&lt;p&gt;\(P_n(X \leq k)=(1-1/2^k)^n\)&lt;/p&gt;
&lt;p&gt;显然第二个问题的答案是：&lt;/p&gt;
&lt;p&gt;\(P_n(X \geq k)=1-(1-1/2^{k-1})^n\)&lt;/p&gt;
&lt;p&gt;从以上分析可以看出，当\(n \ll 2^k\)时，\(P_n(X \geq k)\)的概率几乎为0，同时，当\(n \gg 2^k\)时，\(P_n(X \leq k)\)的概率也几乎为0。用自然语言概括上述结论就是：当伯努利过程次数远远小于\(2^k\)时，至少有一次过程投掷次数等于k的概率几乎为0；当伯努利过程次数远远大于\(2^k\)时，没有一次过程投掷次数大于k的概率也几乎为0。&lt;/p&gt;
&lt;p&gt;如果将上面描述做一个对应：一次伯努利过程对应一个元素的比特串，反面对应0，正面对应1，投掷次数k对应第一个“1”出现的位置，我们就得到了下面结论：&lt;/p&gt;
&lt;p&gt;设一个集合的基数为n，\(\rho_{max}\)为所有元素中首个“1”的位置最大的那个元素的“1”的位置，如果n远远小于\(2^{\rho_{max}}\)，则我们得到\(\rho_{max}\)为当前值的概率几乎为0（它应该更小），同样的，如果n远远大于\(2^{\rho_{max}}\)，则我们得到\(\rho_{max}\)为当前值的概率也几乎为0（它应该更大），因此\(2^{\rho_{max}}\)可以作为基数n的一个粗糙估计。&lt;/p&gt;
&lt;h2&gt;分桶平均&lt;/h2&gt;
&lt;p&gt;上述分析给出了LLC的基本思想，不过如果直接使用上面的单一估计量进行基数估计会由于偶然性而存在较大误差。因此，LLC采用了分桶平均的思想来消减误差。具体来说，就是将哈希空间平均分成m份，每份称之为一个桶（bucket）。对于每一个元素，其哈希值的前k比特作为桶编号，其中\(2^k=m\)，而后L-k个比特作为真正用于基数估计的比特串。桶编号相同的元素被分配到同一个桶，在进行基数估计时，首先计算每个桶内元素最大的第一个“1”的位置，设为M[i]，然后对这m个值取平均后再进行估计，即：&lt;/p&gt;
&lt;p&gt;\(\hat{n}=2^{\frac{1}{m}\sum{M[i]}}\)&lt;/p&gt;
&lt;p&gt;这相当于物理试验中经常使用的多次试验取平均的做法，可以有效消减因偶然性带来的误差。&lt;/p&gt;
&lt;p&gt;下面举一个例子说明分桶平均怎么做。&lt;/p&gt;
&lt;p&gt;假设H的哈希长度为16bit，分桶数m定为32。设一个元素哈希值的比特串为“0001001010001010”，由于m为32，因此前5个bit为桶编号，所以这个元素应该归入“00010”即2号桶（桶编号从0开始，最大编号为m-1），而剩下部分是“01010001010”且显然\(\rho(01010001010)=2\)，所以桶编号为“00010”的元素最大的\(\rho\)即为M[2]的值。&lt;/p&gt;
&lt;h2&gt;偏差修正&lt;/h2&gt;
&lt;p&gt;上述经过分桶平均后的估计量看似已经很不错了，不过通过数学分析可以知道这并不是基数n的无偏估计。因此需要修正成无偏估计。这部分的具体数学分析在“Loglog Counting of Large Cardinalities”中，过程过于艰涩这里不再具体详述，有兴趣的朋友可以参考原论文。这里只简要提一下分析框架：&lt;/p&gt;
&lt;p&gt;首先上文已经得出：&lt;/p&gt;
&lt;p&gt;\(P_n(X \leq k)=(1-1/2^k)^n\)&lt;/p&gt;
&lt;p&gt;因此：&lt;/p&gt;
&lt;p&gt;\(P_n(X = k)=(1-1/2^k)^n - (1-1/2^{k-1})^n\)&lt;/p&gt;
&lt;p&gt;这是一个未知通项公式的递推数列，研究这种问题的常用方法是使用生成函数（generating function）。通过运用指数生成函数和poissonization得到上述估计量的Poisson期望和方差为：&lt;/p&gt;
&lt;p&gt;\(\varepsilon _n\sim [(\Gamma (-1/m)\frac{1-2^{1/m}}{log2})^m+\epsilon _n]n\)&lt;/p&gt;
&lt;p&gt;\(\nu _n\sim [(\Gamma (-2/m)\frac{1-2^{2/m}}{log2})^m - (\Gamma (-1/m)\frac{1-2^{1/m}}{log2})^{2m}+\eta _n]n^2\)&lt;/p&gt;
&lt;p&gt;其中\(|\epsilon _n|\)和\(|\eta _n|\)不超过\(10^{-6}\)。&lt;/p&gt;
&lt;p&gt;最后通过depoissonization得到一个渐进无偏估计量：&lt;/p&gt;
&lt;p&gt;\(\hat{n}=\alpha _m 2^{\frac{1}{m}\sum{M[i]}}\)&lt;/p&gt;
&lt;p&gt;其中：&lt;/p&gt;
&lt;p&gt;\(\alpha _m = (\Gamma (-1/m)\frac{1-2^{1/m}}{log2})^{-m}\)&lt;/p&gt;
&lt;p&gt;\(\Gamma (s)=\frac{1}{s}\int_{0}^{\infty }e^{-t}t^sdt\)&lt;/p&gt;
&lt;p&gt;其中m是分桶数。这就是LLC最终使用的估计量。&lt;/p&gt;
&lt;h2&gt;误差分析&lt;/h2&gt;
&lt;p&gt;不加证明给出如下结论：&lt;/p&gt;
&lt;p&gt;\(E_n(\hat{n})/n = 1 + \theta_{1,n} + o(1)\)&lt;/p&gt;
&lt;p&gt;\(\sqrt{Var_n(E)}/n = \beta_m / \sqrt{m} + \theta_{2,n} + o(1)\)&lt;/p&gt;
&lt;p&gt;其中\(|\theta_{1,n}|\)和\(|\theta_{2,n}|\)不超过\(10^{-6}\)。&lt;/p&gt;
&lt;p&gt;当m不太小（不小于64）时，\(\beta\)大约为1.30。因此：&lt;/p&gt;
&lt;p&gt;\(StdError(\hat{n}/n) \approx \frac{1.30}{\sqrt{m}}\)&lt;/p&gt;
&lt;h1&gt;算法应用&lt;/h1&gt;
&lt;h2&gt;误差控制&lt;/h2&gt;
&lt;p&gt;在应用LLC时，主要需要考虑的是分桶数m，而这个m主要取决于误差。根据上面的误差分析，如果要将误差控制在\(\epsilon\)之内，则：&lt;/p&gt;
&lt;p&gt;\(m &gt; (\frac{1.30}{\epsilon})^2\)&lt;/p&gt;
&lt;h2&gt;内存使用分析&lt;/h2&gt;
&lt;p&gt;内存使用与m的大小及哈希值得长度（或说基数上限）有关。假设H的值为32bit，由于\(\rho_{max} \leq 32\)，因此每个桶需要5bit空间存储这个桶的\(\rho_{max}\)，m个桶就是\(5 \times m/8\)字节。例如基数上限为一亿（约\(2^{27}\)），当分桶数m为1024时，每个桶的基数上限约为\(2^{27} / 2^{10} = 2^{17}\)，而\(log_2(log_2(2^{17}))=4.09\)，因此每个桶需要5bit，需要字节数就是\(5 \times 1024 / 8 = 640\)，误差为\(1.30 / \sqrt{1024} = 0.040625\)，也就是约为4%。&lt;/p&gt;
&lt;h2&gt;合并&lt;/h2&gt;
&lt;p&gt;与LC不同，LLC的合并是以桶为单位而不是bit为单位，由于LLC只需记录桶的\(\rho_{max}\)，因此合并时取相同桶编号数值最大者为合并后此桶的数值即可。&lt;/p&gt;
&lt;h1&gt;小结&lt;/h1&gt;
&lt;p&gt;本文主要介绍了LogLog Counting算法，相比LC其最大的优势就是内存使用极少。不过LLC也有自己的问题，就是当n不是特别大时，其估计误差过大，因此目前实际使用的基数估计算法都是基于LLC改进的算法，这些改进算法通过一定手段抑制原始LLC在n较小时偏差过大的问题。后面要介绍的HyperLogLog Counting和Adaptive Counting就是这类改进算法。&lt;/p&gt;
</description>
</item>
<item>
<title>解读Cardinality Estimation算法（第二部分：Linear Counting）</title>
<link>http://blog.codinglabs.org/articles/algorithms-for-cardinality-estimation-part-ii.html</link>
<guid>http://blog.codinglabs.org/articles/algorithms-for-cardinality-estimation-part-ii.html</guid>
<pubDate>Sun, 30 Dec 2012 16:00:00 GMT</pubDate>
<description>&lt;p&gt;在&lt;a href=&quot;/articles/algorithms-for-cardinality-estimation-part-i.html&quot; target=&quot;_blank&quot;&gt;上一篇文章&lt;/a&gt;中，我们知道传统的精确基数计数算法在数据量大时会存在一定瓶颈，瓶颈主要来自于数据结构合并和内存使用两个方面。因此出现了很多基数估计的概率算法，这些算法虽然计算出的结果不是精确的，但误差可控，重要的是这些算法所使用的数据结构易于合并，同时比传统方法大大节省内存。&lt;/p&gt;
&lt;p&gt;在这一篇文章中，我们讨论Linear Counting算法。
&lt;!--more--&gt;
&lt;/p&gt;
&lt;h1&gt;简介&lt;/h1&gt;
&lt;p&gt;Linear Counting（以下简称LC）在1990年的一篇论文“A linear-time probabilistic counting algorithm for database applications”中被提出。作为一个早期的基数估计算法，LC在空间复杂度方面并不算优秀，实际上LC的空间复杂度与上文中简单bitmap方法是一样的（但是有个常数项级别的降低），都是\(O(N_{max})\)，因此目前很少单独使用LC。不过作为Adaptive Counting等算法的基础，研究一下LC还是比较有价值的。&lt;/p&gt;
&lt;h1&gt;基本算法&lt;/h1&gt;
&lt;h2&gt;思路&lt;/h2&gt;
&lt;p&gt;LC的基本思路是：设有一哈希函数H，其哈希结果空间有m个值（最小值0，最大值m-1），并且哈希结果服从均匀分布。使用一个长度为m的bitmap，每个bit为一个桶，均初始化为0，设一个集合的基数为n，此集合所有元素通过H哈希到bitmap中，如果某一个元素被哈希到第k个比特并且第k个比特为0，则将其置为1。当集合所有元素哈希完成后，设bitmap中还有u个bit为0。则：&lt;/p&gt;
&lt;p&gt;\(\hat{n}=-mlog\frac{u}{m}\)&lt;/p&gt;
&lt;p&gt;为n的一个估计，且为最大似然估计（MLE）。&lt;/p&gt;
&lt;p&gt;示意图如下：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/algorithms-for-cardinality-estimation-part-ii/1.png&quot;/&gt;&lt;/p&gt;
&lt;h2&gt;推导及证明&lt;/h2&gt;
&lt;p&gt;（对数学推导不感兴趣的读者可以跳过本节）&lt;/p&gt;
&lt;p&gt;由上文对H的定义已知n个不同元素的哈希值服从独立均匀分布。设\(A_j\)为事件“经过n个不同元素哈希后，第j个桶值为0”，则：&lt;/p&gt;
&lt;p&gt;\(P(A_j)=(1-\frac{1}{m})^n\)&lt;/p&gt;
&lt;p&gt;又每个桶是独立的，则u的期望为：&lt;/p&gt;
&lt;p&gt;\(E(u)=\sum_{j=1}^mP(A_j)=m(1-\frac{1}{m})^n=m((1+\frac{1}{-m})^{-m})^{-n/m}\)&lt;/p&gt;
&lt;p&gt;当n和m趋于无穷大时，其值约为\(me^{-n/m}\)&lt;/p&gt;
&lt;p&gt;令：&lt;/p&gt;
&lt;p&gt;\(E(u)=me^{-n/m}\)&lt;/p&gt;
&lt;p&gt;得：&lt;/p&gt;
&lt;p&gt;\(n=-mlog\frac{E(u)}{m}\)&lt;/p&gt;
&lt;p&gt;显然每个桶的值服从参数相同0-1分布，因此u服从二项分布。由概率论知识可知，当n很大时，可以用正态分布逼近二项分布，因此可以认为当n和m趋于无穷大时u渐进服从正态分布。&lt;/p&gt;
&lt;p&gt;因此u的概率密度函数为：&lt;/p&gt;
&lt;p&gt;\(f(x) = {1 \over \sigma\sqrt{2\pi} }\,e^{- {{(x-\mu )^2 \over 2\sigma^2}}}\)&lt;/p&gt;
&lt;p&gt;由于我们观察到的空桶数u是从正态分布中随机抽取的一个样本，因此它就是\(\mu\)的最大似然估计（正态分布的期望的最大似然估计是样本均值）。&lt;/p&gt;
&lt;p&gt;又由如下定理：&lt;/p&gt;
&lt;p&gt;设\(f(x)\)是可逆函数\(\hat{x}\)是\(x\)的最大似然估计，则\(f(\hat{x})\)是\(f(x)\)的最大似然估计。&lt;/p&gt;
&lt;p&gt;且\(-mlog\frac{x}{m}\)是可逆函数，则\(\hat{n}=-mlog\frac{u}{m}\)是\(-mlog\frac{E(u)}{m}=n\)的最大似然估计。&lt;/p&gt;
&lt;h2&gt;偏差分析&lt;/h2&gt;
&lt;p&gt;下面不加证明给出如下结论：&lt;/p&gt;
&lt;p&gt;\(Bias(\frac{\hat{n}}{n})=E(\frac{\hat{n}}{n})-1=\frac{e^t-t-1}{2n}\)&lt;/p&gt;
&lt;p&gt;\(StdError(\frac{\hat{n}}{n})=\frac{\sqrt{m}(e^t-t-1)^{1/2}}{n}\)&lt;/p&gt;
&lt;p&gt;其中\(t=n/m\)&lt;/p&gt;
&lt;p&gt;以上结论的推导在“A linear-time probabilistic counting algorithm for database applications”可以找到。&lt;/p&gt;
&lt;h1&gt;算法应用&lt;/h1&gt;
&lt;p&gt;在应用LC算法时，主要需要考虑的是bitmap长度m的选择。这个选择主要受两个因素的影响：基数n的量级以及容许的误差。这里假设估计基数n的量级大约为N，允许的误差为\(\epsilon\)，则m的选择需要遵循如下约束。&lt;/p&gt;
&lt;h2&gt;误差控制&lt;/h2&gt;
&lt;p&gt;这里以标准差作为误差。由上面标准差公式可以推出，当基数的量级为N，容许误差为\(\epsilon\)时，有如下限制：&lt;/p&gt;
&lt;p&gt;\(m &gt; \frac{e^t-t-1}{(\epsilon t)^2}\)&lt;/p&gt;
&lt;p&gt;将量级和容许误差带入上式，就可以得出m的最小值。&lt;/p&gt;
&lt;h2&gt;满桶控制&lt;/h2&gt;
&lt;p&gt;由LC的描述可以看到，如果m比n小太多，则很有可能所有桶都被哈希到了，此时u的值为0，LC的估计公式就不起作用了（变成无穷大）。因此m的选择除了要满足上面误差控制的需求外，还要保证满桶的概率非常小。&lt;/p&gt;
&lt;p&gt;上面已经说过，u满足二项分布，而当n非常大，p非常小时，可以用泊松分布近似逼近二项分布。因此这里我们可以认为u服从泊松分布（注意，上面我们说u也可以近似服从正态分布，这并不矛盾，实际上泊松分布和正态分布分别是二项分布的离散型和连续型概率逼近，且泊松分布以正态分布为极限）：&lt;/p&gt;
&lt;p&gt;当n、m趋于无穷大时：&lt;/p&gt;
&lt;p&gt;\(Pr(u=k)=(\frac{\lambda^k}{k!})e^{-\lambda}\)&lt;/p&gt;
&lt;p&gt;因此：&lt;/p&gt;
&lt;p&gt;\(Pr(u=0)&lt;e^{-5}=0.007\)&lt;/p&gt;
&lt;p&gt;由于泊松分布的方差为\(\lambda\)，因此只要保证u的期望偏离0点\(\sqrt{5}\)的标准差就可以保证满桶的概率不大约0.7%。因此可得：&lt;/p&gt;
&lt;p&gt;\(m &gt; 5(e^t-t-1)\)&lt;/p&gt;
&lt;p&gt;综上所述，当基数量级为N，可接受误差为\(\epsilon\)，则m的选取应该遵从&lt;/p&gt;
&lt;p&gt;\(m &gt; \beta (e^t-t-1)\)&lt;/p&gt;
&lt;p&gt;其中\(\beta = max(5, 1/(\epsilon t)^2)\)&lt;/p&gt;
&lt;p&gt;下图是论文作者预先计算出的关于不同基数量级和误差情况下，m的选择表：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/algorithms-for-cardinality-estimation-part-ii/2.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;可以看出精度要求越高，则bitmap的长度越大。随着m和n的增大，m大约为n的十分之一。因此LC所需要的空间只有传统的bitmap直接映射方法的1/10，但是从渐进复杂性的角度看，空间复杂度仍为\(O(N_{max})\)。&lt;/p&gt;
&lt;h2&gt;合并&lt;/h2&gt;
&lt;p&gt;LC非常方便于合并，合并方案与传统bitmap映射方法无异，都是通过按位或的方式。&lt;/p&gt;
&lt;h1&gt;小结&lt;/h1&gt;
&lt;p&gt;这篇文章主要介绍了Linear Counting。LC算法虽然由于空间复杂度不够理想已经很少被单独使用，但是由于其在元素数量较少时表现非常优秀，因此常被用于弥补LogLog Counting在元素较少时误差较大的缺陷，实际上LC及其思想是组成HyperLogLog Counting和Adaptive Counting的一部分。&lt;/p&gt;
&lt;p&gt;在下一篇文章中，我会介绍空间复杂度仅有\(O(log_2(log_2(N_{max})))\)的基数估计算法LogLog Counting。&lt;/p&gt;
</description>
</item>
<item>
<title>解读Cardinality Estimation算法（第一部分：基本概念）</title>
<link>http://blog.codinglabs.org/articles/algorithms-for-cardinality-estimation-part-i.html</link>
<guid>http://blog.codinglabs.org/articles/algorithms-for-cardinality-estimation-part-i.html</guid>
<pubDate>Sat, 29 Dec 2012 16:00:00 GMT</pubDate>
<description>&lt;p&gt;基数计数（cardinality counting）是实际应用中一种常见的计算场景，在数据分析、网络监控及数据库优化等领域都有相关需求。精确的基数计数算法由于种种原因，在面对大数据场景时往往力不从心，因此如何在误差可控的情况下对基数进行估计就显得十分重要。目前常见的基数估计算法有Linear Counting、LogLog Counting、HyperLogLog Counting及Adaptive Counting等。这几种算法都是基于概率统计理论所设计的概率算法，它们克服了精确基数计数算法的诸多弊端（如内存需求过大或难以合并等），同时可以通过一定手段将误差控制在所要求的范围内。&lt;/p&gt;
&lt;p&gt;作为“解读Cardinality Estimation算法”系列文章的第一部分，本文将首先介绍基数的概念，然后通过一个电商数据分析的例子说明基数如何在具体业务场景中发挥作用以及为什么在大数据面前基数的计算是困难的，在这一部分也同时会详述传统基数计数的解决方案及遇到的难题。&lt;/p&gt;
&lt;p&gt;后面在第二部分-第四部分会分别详细介绍Linear Counting、LogLog Counting、HyperLogLog Counting及Adaptive Counting四个算法，会涉及算法的基本思路、概率分析及论文关键部分的解读。&lt;/p&gt;
&lt;p&gt;最后在第五部分会介绍一淘数据部的开源基数估计算法库&lt;a href=&quot;https://github.com/chaoslawful/ccard-lib&quot; target=&quot;_blank&quot;&gt;ccard-lib&lt;/a&gt;，这个算法库由一淘数据部工程师&lt;a href=&quot;http://weibo.com/u/1919389283&quot; target=&quot;_blank&quot;&gt;清无&lt;/a&gt;（王晓哲）、&lt;a href=&quot;http://weibo.com/u/1447857772&quot; target=&quot;_blank&quot;&gt;民瞻&lt;/a&gt;（张维）及我开发，并已用于一淘数据部多个业务线，ccard-lib实现了上述四种算法，整个库使用C写成，并附带PHP扩展模块，我会在这一部分介绍ccard-lib的实现重点及使用方法。&lt;/p&gt;
&lt;!--more--&gt;
文章索引：
&lt;p&gt;&lt;a href=&quot;/articles/algorithms-for-cardinality-estimation-part-i.html&quot; target=&quot;_blank&quot;&gt;第一部分：基本概念&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;/articles/algorithms-for-cardinality-estimation-part-ii.html&quot; target=&quot;_blank&quot;&gt;第二部分：Linear Counting&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;/articles/algorithms-for-cardinality-estimation-part-iii.html&quot;&gt;第三部分：LogLog Counting&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;/articles/algorithms-for-cardinality-estimation-part-iv.html&quot; target=&quot;_blank&quot;&gt;第四部分：HyperLogLog Counting及Adaptive Counting&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;第五部分：ccard-lib介绍&lt;/p&gt;
&lt;h1&gt;基数的定义&lt;/h1&gt;
&lt;p&gt;简单来说，基数（cardinality，也译作势），是指一个集合（这里的集合允许存在重复元素，与集合论对集合严格的定义略有不同，如不做特殊说明，本文中提到的集合均允许存在重复元素）中不同元素的个数。例如看下面的集合：&lt;/p&gt;
&lt;p&gt;\(\{1, 2, 3, 4, 5, 2, 3, 9, 7\}\)&lt;/p&gt;
&lt;p&gt;这个集合有9个元素，但是2和3各出现了两次，因此不重复的元素为1,2,3,4,5,9,7，所以这个集合的基数是7。&lt;/p&gt;
&lt;p&gt;如果两个集合具有相同的基数，我们说这两个集合等势。基数和等势的概念在有限集范畴内比较直观，但是如果扩展到无限集则会比较复杂，一个无限集可能会与其真子集等势（例如整数集和偶数集是等势的）。不过在这个系列文章中，我们仅讨论有限集的情况，关于无限集合基数的讨论，有兴趣的同学可以参考实变分析相关内容。&lt;/p&gt;
&lt;p&gt;容易证明，如果一个集合是有限集，则其基数是一个自然数。&lt;/p&gt;
&lt;h1&gt;基数的应用实例&lt;/h1&gt;
&lt;p&gt;下面通过一个实例说明基数在电商数据分析中的应用。&lt;/p&gt;
&lt;p&gt;假设一个淘宝网店在其店铺首页放置了10个宝贝链接，分别从Item01到Item10为这十个链接编号。店主希望可以在一天中随时查看从今天零点开始到目前这十个宝贝链接分别被多少个独立访客点击过。所谓独立访客（Unique Visitor，简称UV）是指有多少个自然人，例如，即使我今天点了五次Item01，我对Item01的UV贡献也是1，而不是5。&lt;/p&gt;
&lt;p&gt;用术语说这实际是一个实时数据流统计分析问题。&lt;/p&gt;
&lt;p&gt;要实现这个统计需求。需要做到如下三点：&lt;/p&gt;
&lt;p&gt;1、对独立访客做标识&lt;/p&gt;
&lt;p&gt;2、在访客点击链接时记录下链接编号及访客标记&lt;/p&gt;
&lt;p&gt;3、对每一个要统计的链接维护一个数据结构和一个当前UV值，当某个链接发生一次点击时，能迅速定位此用户在今天是否已经点过此链接，如果没有则此链接的UV增加1&lt;/p&gt;
&lt;p&gt;下面分别介绍三个步骤的实现方案&lt;/p&gt;
&lt;h2&gt;对独立访客做标识&lt;/h2&gt;
&lt;p&gt;客观来说，目前还没有能在互联网上准确对一个自然人进行标识的方法，通常采用的是近似方案。例如通过登录用户+cookie跟踪的方式：当某个用户已经登录，则采用会员ID标识；对于未登录用户，则采用跟踪cookie的方式进行标识。为了简单起见，我们假设完全采用跟踪cookie的方式对独立访客进行标识。&lt;/p&gt;
&lt;h2&gt;记录链接编号及访客标记&lt;/h2&gt;
&lt;p&gt;这一步可以通过javascript埋点及记录accesslog完成，具体原理和实现方案可以参考我之前的一篇文章：&lt;a href=&quot;http://www.codinglabs.org/html/how-web-analytics-data-collection-system-work.html&quot; target=&quot;_blank&quot;&gt;网站统计中的数据收集原理及实现&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;实时UV计算&lt;/h2&gt;
&lt;p&gt;可以看到，如果将每个链接被点击的日志中访客标识字段看成一个集合，那么此链接当前的UV也就是这个集合的基数，因此UV计算本质上就是一个基数计数问题。&lt;/p&gt;
&lt;p&gt;在实时计算流中，我们可以认为任何一次链接点击均触发如下逻辑（伪代码描述）：&lt;/p&gt;&lt;pre class=&quot;prettyprint linenums languague-c&quot;&gt;cand_counting(item_no, user_id) {
    if (user_id is not in the item_no visitor set) {
        add user_id to item_no visitor set;
        cand[item_no]++;
    }
}&lt;/pre&gt;
&lt;p&gt;逻辑非常简单，每当有一个点击事件发生，就去相应的链接被访集合中寻找此访客是否已经在里面，如果没有则将此用户标识加入集合，并将此链接的UV加1。&lt;/p&gt;
&lt;p&gt;虽然逻辑非常简单，但是在实际实现中尤其面临大数据场景时还是会遇到诸多困难，下面一节我会介绍两种目前被业界普遍使用的精确算法实现方案，并通过分析说明当数据量增大时它们面临的问题。&lt;/p&gt;
&lt;h1&gt;传统的基数计数实现&lt;/h1&gt;
&lt;p&gt;接着上面的例子，我们看一下目前常用的基数计数的实现方法。&lt;/p&gt;
&lt;h2&gt;基于B树的基数计数&lt;/h2&gt;
&lt;p&gt;对上面的伪代码做一个简单分析，会发现关键操作有两个：查找-迅速定位当前访客是否已经在集合中，插入-将新的访客标识插入到访客集合中。因此，需要为每一个需要统计UV的点（此处就是十个宝贝链接）维护一个查找效率较高的数据结构，又因为实时数据流的关系，这个数据结构需要尽量在内存中维护，因此这个数据结构在空间复杂度上也要比较适中。综合考虑一种传统的做法是在实时计算引擎采用了B树来组织这个集合。下图是一个示意图：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/algorithms-for-cardinality-estimation-part-i/1.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;之所以选用B树是因为B树的查找和插入相关高效，同时空间复杂度也可以接受（关于B树具体的性能分析请参考&lt;a href=&quot;http://en.wikipedia.org/wiki/B-tree&quot; target=&quot;_blank&quot;&gt;这里&lt;/a&gt;）。&lt;/p&gt;
&lt;p&gt;这种实现方案为一个基数计数器维护一棵B树，由于B树在查找效率、插入效率和内存使用之间非常平衡，所以算是一种可以接受的解决方案。但是当数据量特别巨大时，例如要同时统计几万个链接的UV，如果要将几万个链接一天的访问记录全部维护在内存中，这个内存使用量也是相当可观的（假设每个B树占用1M内存，10万个B树就是100G！）。一种方案是在某个时间点将内存数据结构写入磁盘（双十一和双十二大促时一淘数据部的效果平台是每分钟将数据写入HBase）然后将内存中的计数器和数据结构清零，但是B树并不能高效的进行合并，这就使得内存数据落地成了非常大的难题。&lt;/p&gt;
&lt;p&gt;另一个需要数据结构合并的场景是查看并集的基数，例如在上面的例子中，如果我想查看Item1和Item2的总UV，是没有办法通过这种B树的结构快速得到的。当然可以为每一种可能的组合维护一棵B树。不过通过简单的分析就可以知道这个方案基本不可行。N个元素集合的非空幂集数量为\(2^N-1\)，因此要为10个链接维护1023棵B树，而随着链接的增加这个数量会以幂指级别增长。&lt;/p&gt;
&lt;h2&gt;基于bitmap的基数计数&lt;/h2&gt;
&lt;p&gt;为了克服B树不能高效合并的问题，一种替代方案是使用bitmap表示集合。也就是使用一个很长的bit数组表示集合，将bit位顺序编号，bit为1表示此编号在集合中，为0表示不在集合中。例如“00100110”表示集合 {2，5，6}。bitmap中1的数量就是这个集合的基数。&lt;/p&gt;
&lt;p&gt;显然，与B树不同bitmap可以高效的进行合并，只需进行按位或（or）运算就可以，而位运算在计算机中的运算效率是很高的。但是bitmap方式也有自己的问题，就是内存使用问题。&lt;/p&gt;
&lt;p&gt;很容易发现，bitmap的长度与集合中元素个数无关，而是与基数的上限有关。例如在上面的例子中，假如要计算上限为1亿的基数，则需要12.5M字节的bitmap，十个链接就需要125M。关键在于，这个内存使用与集合元素数量无关，即使一个链接仅仅有一个1UV，也要为其分配12.5M字节。&lt;/p&gt;
&lt;p&gt;由此可见，虽然bitmap方式易于合并，却由于内存使用问题而无法广泛用于大数据场景。&lt;/p&gt;
&lt;h1&gt;小结&lt;/h1&gt;
&lt;p&gt;本文重点在于通过电商数据分析中UV计算的例子，说明基数的应用、传统的基数计数算法及这些算法在大数据面前遇到的问题。实际上目前还没有发现更好的在大数据场景中准确计算基数的高效算法，因此在不追求绝对准确的情况下，使用概率算法算是一个不错的解决方案。在后续文章中，我将逐一解读常用的基数估计概率算法。&lt;/p&gt;
</description>
</item>
<item>
<title>基数估计算法概览</title>
<link>http://blog.codinglabs.org/articles/cardinality-estimation.html</link>
<guid>http://blog.codinglabs.org/articles/cardinality-estimation.html</guid>
<pubDate>Thu, 22 Nov 2012 16:00:00 GMT</pubDate>
<description>&lt;p&gt;翻译自《&lt;a href=&quot;http://blog.notdot.net/2012/09/Dam-Cool-Algorithms-Cardinality-Estimation&quot; target=&quot;_blank&quot;&gt;Damn Cool Algorithms: Cardinality Estimation&lt;/a&gt;》，原文链接：&lt;a title=&quot;http://blog.notdot.net/2012/09/Dam-Cool-Algorithms-Cardinality-Estimation&quot; href=&quot;http://blog.notdot.net/2012/09/Dam-Cool-Algorithms-Cardinality-Estimation&quot; target=&quot;_blank&quot;&gt;http://blog.notdot.net/2012/09/Dam-Cool-Algorithms-Cardinality-Estimation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;译注：给定一个数据集，求解数据集的基数（Cardinality，也译作“势”，表示一个数据集中不同数据项的数量）是非常普遍的一个需求。许多业务需求最终可以归结为基数求解，如网站访问分析中的UV（访客数，指一段时间内访问网站的不同用户的数量）。由于数据集基数是不可聚集指标（两个数据集总的基数无法通过分别的基数简单计算），因此如果要得到N个数据集任意组合的基数，需要\(2^N\)次数据集去重计算，是一个复杂度非常高的计算过程。当数据量较小时，可以采取bitmap“按位或”方法获得较高的计算速度；而当数据量很大时，一般会采取概率算法对基数进行估计。这篇文章是对基数估计算法的一个非常好的概览。&lt;/p&gt;
&lt;!--more--&gt;
以下为译文
&lt;p&gt;--------------------&lt;/p&gt;
&lt;p&gt;假如你有一个巨大的含有重复数据项数据集，这个数据集过于庞大以至于无法全部放到内存中处理。现在你想知道这个数据集里有多少不同的元素，但是数据集没有排好序，而且对如此大的一个数据集进行排序和计数几乎是不可行的。你要如何估计数据集中有多少不同的数据项？很多应用场景都涉及这个问题，例如设计数据库的查询策略：一个良好的数据库查询策略不但和总的数据量有关，同时也依赖于数据中不同数据项的数量。&lt;/p&gt;
&lt;p&gt;我建议在继续阅读本文前你可以稍微是思考一下这个问题，因为接下来我们要谈的算法相当有创意，而且实在是不怎么直观。&lt;/p&gt;
&lt;h1&gt;一个简单直观的基数估计方法&lt;/h1&gt;
&lt;p&gt;让我们从一个简单直观的例子开始吧。假设你通过如下步骤生成了一个数据集：&lt;/p&gt;
&lt;p&gt;1、随机生成n个服从均匀分布的数字&lt;/p&gt;
&lt;p&gt;2、随便重复其中一些数字，重复的数字和重复次数都不确定&lt;/p&gt;
&lt;p&gt;3、打乱这些数字的顺序，得到一个数据集&lt;/p&gt;
&lt;p&gt;我们要如何估计这个数据集中有多少不同的数字呢？因为知道这些数字是服从均匀分布的随机数字，一个比较简单的可行方案是：找出数据集中最小的数字。假如m是数值上限，x是找到的最小的数，则\(m/x\)是基数的一个估计。例如，我们扫描一个包含0到1之间数字组成的数据集，其中最小的数是0.01，则一个比较合理的推断是数据集中大约有100个不同的元素，否则我们应该预期能找到一个更小的数。注意这个估计值和重复次数无关：就如最小值重复多少次都不改变最小值的数值。&lt;/p&gt;
&lt;p&gt;这个估计方法的优点是十分直观，但是准确度一般。例如，一个只有很少不同数值的数据集却拥有很小的最小值；类似的一个有很多不同值的数据集可能最小值并不小。最后一点，其实只有很少的数据集符合随机均匀分布这一前提。尽管如此，这个原型算法仍然是了解基数估计思想的一个途径；后面我们会了解一些更加精巧的算法。&lt;/p&gt;
&lt;h1&gt;基数估计的概率算法&lt;/h1&gt;
&lt;p&gt;最早研究高精度基数估计的论文是Flajolet和Martin的&lt;a href=&quot;http://www.cse.unsw.edu.au/~cs9314/07s1/lectures/Lin_CS9314_References/fm85.pdf&quot;&gt;Probabilistic Counting Algorithms for Data Base Applications&lt;/a&gt;，后来Flajolet又发表了&lt;a href=&quot;http://www.ic.unicamp.br/~celio/peer2peer/math/bitmap-algorithms/durand03loglog.pdf&quot;&gt;LogLog counting of large cardinalities&lt;/a&gt;和&lt;a href=&quot;http://algo.inria.fr/flajolet/Publications/FlFuGaMe07.pdf&quot;&gt;HyperLogLog: The analysis of a near-optimal cardinality estimation algorithm&lt;/a&gt;两篇论文对算法进行了进一步改进。通过逐篇阅读这些论文来了解算法的发展和细节固然有趣，不过在这篇文章中我会忽略一些算法的理论细节，把精力主要放在如何通过论文中的算法解决问题。有兴趣的读者可以读一下这三篇论文；本文不会介绍其中的数学细节。&lt;/p&gt;
&lt;p&gt;Flajolet和Martin最早发现通过一个良好的哈希函数，可以将任意数据集映射成服从均匀分布的（伪）随机值。根据这一事实，可以将任意数据集变换为均匀分布的随机数集合，然后就可以使用上面的方法进行估计了，不过只是这样是远远不够的。&lt;/p&gt;
&lt;p&gt;接下来，他们陆续发现一些其它的基数估计方法，而其中一些方法的效果优于之前提到的方法。Flajolet和Martin计算了哈希值的二进制表示的0前缀，结果发现在随机数集合中，通过计算每一个元素的二进制表示的0前缀，设k为最长的0前缀的长度，则平均来说集合中大约有\(2^k\)个不同的元素；我们可以用这个方法估计基数。但是，这仍然不是很理想的估计方法，因为和基于最小值的估计一样，这个方法的方差很大。不过另一方面，这个估计方法比较节省资源：对于32位的哈希值来说，只需要5比特去存储0前缀的长度。&lt;/p&gt;
&lt;p&gt;值得一提的是，Flajolet-Martin在最初的论文里通过一种基于bitmap的过程去提高估计算法的准确度。关于这点我就不再详述了，因为这种方法已经被后续论文中更好的方法所取代；对这个细节有兴趣的读者可以去阅读原始论文。&lt;/p&gt;
&lt;p&gt;到目前为止，我们这种基于位模式的估计算法给出的结果仍然不够理想。如何进行改进呢？一个直观的改进方法就是使用多个相互独立的哈希函数：通过计算每个哈希函数所产生的最长0前缀，然后取其平均值可以提高算法的精度。&lt;/p&gt;
&lt;p&gt;实践表明从统计意义来说这种方法确实可以提高估计的准确度，但是计算哈希值的消耗比较大。另一个更高效的方法就是随机平均（stochastic averaging）。这种方法不是使用多个哈希函数，而是使用一个哈希函数，但是将哈希值的区间按位切分成多个桶（bucket）。例如我们希望取1024个数进行平均，那么我们可以取哈希值的前10比特作为桶编号，然后计算剩下部分的0前缀长度。这种方法的准确度和多哈希函数方法相当，但是比计算多个哈希效率高很多。&lt;/p&gt;
&lt;p&gt;根据上述分析，我们可以给出一个简单的算法实现。这个实现等价于Durand-Flajolet的论文中提出的LogLog算法；不过为了方便，这个实现中统计的是0尾缀而不是0前缀；其效果是等价的。&lt;/p&gt;&lt;pre class=&quot;prettyprint linenums languague-python&quot;&gt;def trailing_zeroes(num):
    &quot;&quot;&quot;Counts the number of trailing 0 bits in num.&quot;&quot;&quot;
    if num == 0:
        return 32 # Assumes 32 bit integer inputs!
    p = 0
    while (num &gt;&gt; p) &amp; 1 == 0:
        p += 1
    return p

def estimate_cardinality(values, k):
    &quot;&quot;&quot;Estimates the number of unique elements in the input set values.

    Arguments:
        values: An iterator of hashable elements to estimate the cardinality of.
        k: The number of bits of hash to use as a bucket number; there will be 2**k buckets.
    &quot;&quot;&quot;
    num_buckets = 2 ** k
    max_zeroes = [0] * num_buckets
    for value in values:
        h = hash(value)
        bucket = h &amp; (num_buckets - 1) # Mask out the k least significant bits as bucket ID
        bucket_hash = h &gt;&gt; k
        max_zeroes[bucket] = max(max_zeroes[bucket], trailing_zeroes(bucket_hash))
    return 2 ** (float(sum(max_zeroes)) / num_buckets) * num_buckets * 0.79402&lt;/pre&gt;
    &lt;p&gt;这段代码实现了我们上面讨论的估计算法：我们计算每个桶的0前缀（或尾缀）的最长长度；然后计算这些长度的平均数；假设平均数是x，桶数量是m，则最终的估计值是\(2^x \times m\)。其中一个没提过的地方是魔法数字0.79402。统计分析显示这种预测方法存在一个可预测的偏差；这个魔法数字是对这个偏差的修正。实际经验表明计算值随着桶数量的不同而变化，不过当桶数量不太小时（大于64），计算值会收敛于估计值。原论文中描述了这个结论的推导过程。&lt;/p&gt;
    &lt;p&gt;这个方法给出的估计值比较精确 —— 在分桶数为m的情况下，平均误差为\(1.3/\sqrt{m}\)。因此对于分桶数为1024的情况（所需内存1024*5 = 5120位，或640字节），大约会有4%的平均误差；每桶5比特的存储已经足以估计\(2^{27}\)的数据集，而我们只用的不到1k的内存！&lt;/p&gt;
    &lt;p&gt;让我们看一下试验结果：&lt;/p&gt;&lt;pre class=&quot;prettyprint linenums languague-python&quot;&gt;&gt;&gt;&gt; [100000/estimate_cardinality([random.random() for i in range(100000)], 10) for j in range(10)]
[0.9825616152548807, 0.9905752876839672, 0.979241749110407, 1.050662616357679, 0.937090578752079, 0.9878968276629505, 0.9812323203117748, 1.0456960262467019, 0.9415413413873975, 0.9608567203911741]&lt;/pre&gt;
    &lt;p&gt;不错！虽然有些估计误差大于4%的平均误差，但总体来说效果良好。如果你准备自己做一下这个试验，有一点需要注意：Python内置的 hash() 方法将整数哈希为它自己。因此诸如 estimate_cardinality(range(10000), 10) 这种方式得到的结果不会很理想，因为内置 hash() 对于这种情况并不能生成很好的散列。但是像上面例子中使用随机数会好很多。&lt;/p&gt;
    &lt;h1&gt;提升准确度：SuperLogLog和HyperLogLog&lt;/h1&gt;
    &lt;p&gt;虽然我们已经有了一个不错的估计算法，但是我们还能进一步提升算法的准确度。Durand和Flajolet发现离群点会大大降低估计准确度；如果在计算平均值前丢弃一些特别大的离群值，则可以提高精确度。特别的，通过丢弃最大的30%的桶的值，只使用较小的70%的桶的值来进行平均值计算，则平均误差可以从\(1.3/\sqrt{m}\)降低到\(1.05/\sqrt{m}\)！这意味着在我们上面的例子中，使用640个字节可情况下可以将平均误差从4%降低到3.2%，而所需内存并没有增加。&lt;/p&gt;
    &lt;p&gt;最后，Flajolet等人在HyperLogLog论文中给出一种不同的平均值，使用调和平均数取代几何平均数（译注：原文有误，此处应该是算数平均数）。这一改进可以将平均误差降到\(1.04/\sqrt{m}\)，而且并没不需要额外资源。但是这个算法比前面的算法复杂很多，因为对于不同基数的数据集要做不同的修正。有兴趣的读者可以阅读原论文。&lt;/p&gt;
    &lt;h1&gt;并行化&lt;/h1&gt;
    &lt;p&gt;这些基数估计算法的一个好处就是非常容易并行化。对于相同分桶数和相同哈希函数的情况，多台机器节点可以独立并行的执行这个算法；最后只要将各个节点计算的同一个桶的最大值做一个简单的合并就可以得到这个桶最终的值。而且这种并行计算的结果和单机计算结果是完全一致的，所需的额外消耗仅仅是小于1k的字节在不同节点间的传输。&lt;/p&gt;
    &lt;h1&gt;结论&lt;/h1&gt;
    &lt;p&gt;基数估计算法使用很少的资源给出数据集基数的一个良好估计，一般只要使用少于1k的空间存储状态。这个方法和数据本身的特征无关，而且可以高效的进行分布式并行计算。估计结果可以用于很多方面，例如流量监控（多少不同IP访问过一个服务器）以及数据库查询优化（例如我们是否需要排序和合并，或者是否需要构建哈希表）。&lt;/p&gt;
</description>
</item>
<item>
<title>从抛硬币试验看概率论的基本内容及统计方法</title>
<link>http://blog.codinglabs.org/articles/basis-of-probability-and-statistics.html</link>
<guid>http://blog.codinglabs.org/articles/basis-of-probability-and-statistics.html</guid>
<pubDate>Mon, 19 Nov 2012 16:00:00 GMT</pubDate>
<description>&lt;p&gt;一般说到概率，就喜欢拿抛硬币做例子。大多数时候，会简单认为硬币正背面的概率各为二分之一，其实事情远没有这么简单。这篇文章会以抛硬币试验为例子并贯穿全文，引出一系列概率论和数理统计的基本内容。这篇文章会涉及的有古典概型、公理化概率、二项分布、正态分布、最大似然估计和假设检验等一系列内容。主要目的是以抛硬币试验为例说明现代数学观点下的概率是什么样子以及以概率论为基础的一些基本数理统计方法。&lt;/p&gt;
&lt;!--more--&gt;
&lt;h1&gt;概率的存在性&lt;/h1&gt;
&lt;p&gt;好吧，首先我们要回答一个基本问题就是概率为什么是存在的。其实这不是个数学问题，而是哲学问题（貌似一般存在不存在啥的都是哲学问题）。之所以要先讨论这个问题，是因为任何数学活动都是在一定哲学观点前提下进行的，如果不明确哲学前提，数学活动就无法进行了（例如如果在你的哲学观点下概率根本不存在，那还讨论啥概率论啊）。&lt;/p&gt;
&lt;p&gt;概率的存在是在一定哲学观点前提下的，我不想用哲学术语拽文，简单来说，就是你首先得承认事物是客观存在的，并可以通过大量的观察和实践被抽象总结。举个例子，我们经常会讨论“身高”，为什么我们都认为身高是存在的？因为我们经过长期的观察实践发现一个人身体的高度在短期内不会出现大幅度的变动，因此我们可以用一个有单位的数字来描述一个人的身体在一段不算长的时间内相对稳定的高度。这就是“身高”作为被普遍承认存在的哲学前提。&lt;/p&gt;
&lt;p&gt;与此相似，人们在长期的生活中，发现世界上有一些事情的结果是无法预料的，例如抛硬币得到正面还是背面，但是，后来有些人发现，虽然单次的结果不可预料，但是如果我不断抛，抛很多次，正面结果占全部抛硬币次数的比率是趋于稳定的，而且次数越多越接近某个固定的数值。换句话说，抛硬币这件事，单次结果不可预料，但是多次试验的结果却在总体上是有规律可循的（术语叫统计规律）。&lt;/p&gt;
&lt;p&gt;下面是历史上一些著名的抛硬币试验的数据记录：&lt;/p&gt;
&lt;table border=&quot;1&quot; cellspacing=&quot;0&quot; cellpadding=&quot;2&quot; width=&quot;400&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;&lt;strong&gt;试验者&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;&lt;strong&gt;试验次数&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;&lt;strong&gt;正面次数&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;&lt;strong&gt;正面占比&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;德摩根&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;4092&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;2048&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;50.05%&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;蒲丰&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;4040&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;2048&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;50.69%&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;费勒&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;10000&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;4979&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;49.79%&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;皮尔逊&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;24000&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;12012&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;50.05%&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;罗曼洛夫斯基&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;80640&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;39699&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;49.23%&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;可以看到，虽然这些试验在不同时间、不同地点由不同的人完成，但是冥冥中似乎有一股力量将正面的占比固定在50%附近。&lt;/p&gt;
&lt;p&gt;后来，人们发现还有很多其它不可预测的事情都与抛硬币类似，例如掷骰子、买六合彩等等，甚至渐渐发现不只这些简单的事情，人类社会方方面面从简单到复杂的很多不可预测的事情宏观上看都具有统计规律。于是人们推测，在某些条件下的一些不可预测事件，都是有统计规律的，或者直观说很多不可预测结果的试验在多次进行后总体上看结果会趋近于一些常数（这个现象后来被严格定义为大数定律，成为概率论最基础的定理之一，下文会提到）。这种可观测现象，成为概率存在的哲学基础，而这些常数就是概率在朴素观点下的定义。&lt;/p&gt;
&lt;h1&gt;概率模型&lt;/h1&gt;
&lt;p&gt;在认识到上述事实后，人们希望将这种规律加以利用（人类文明的发展不就是发现和利用规律么，呵呵），但是想要利用就首先要对概率进行严格的形式化定义，也就是要建立数学模型。比较知名的数学模型有古典概型、几何概率模型和公理化概率，本文将会讨论古典概型和公理化概率。&lt;/p&gt;
&lt;h2&gt;古典概型&lt;/h2&gt;
&lt;p&gt;古典概型是人类对概率和统计规律最早的建模尝试，表达了朴素的数学原则下人们对概率的认识。在表述古典概型之前，需要先定义一些概念。&lt;/p&gt;
&lt;p&gt;首先是随机试验。&lt;/p&gt;
&lt;p&gt;如果一个同时试验满足下面三条原则，则这个试验称为随机试验：&lt;/p&gt;
&lt;p&gt;1、可在相同条件下（相对来说）重复进行。&lt;/p&gt;
&lt;p&gt;2、可能出现的结果不止一个，但事先明确知道所有可能的结果（可以是无限个，例如所有自然数，但必须事先明确知道结果的取值范围）。&lt;/p&gt;
&lt;p&gt;3、事先无法预测在一次试验中哪一个结果会出现。&lt;/p&gt;
&lt;p&gt;显然上面的抛硬币试验是一个随机试验。&lt;/p&gt;
&lt;p&gt;然后需要定义样本空间和样本点。一个随机试验的样本空间是这个试验所有可能结果组成的集合，而其中每个元素是一个样本点。例如，抛硬币试验中，样本空间为\(\{F, B\}\)，其中F表示正面，B表示背面，而F、B就是两个样本点。&lt;/p&gt;
&lt;p&gt;另一个非常重要的概念就是随机事件（简称事件）：样本空间的一个子集称为一个事件。例如，抛硬币试验有四个不同的事件：\(\emptyset\)，\(\{F\}\)，\(\{B\}\)，\(\{F, B\}\)，分别表示“既不出现正面也不出现反面”，“出现正面”，“出现反面”和“出现正面或反面”。在不考虑硬币立起来等特殊情况时，第一个事件不可能出现，但它确实是一个合乎定义的事件，叫不可能事件；而最后一个事件必然出现，叫必然事件。&lt;/p&gt;
&lt;p&gt;有了上面概念，就可以定义古典概型了：&lt;/p&gt;
&lt;p&gt;如果一个概率模型满足 1）样本空间是一个有限集合，2）每一个基本事件（只包含一个样本点的事件）出现的概率相同，则这是一个古典概型。例如，在上面的抛硬币试验中，再定义\(\{F\}\)，\(\{B\}\)的概率均为0.5，则就构成了一个古典概型。&lt;/p&gt;
&lt;p&gt;古典概型简单、直观，在早期的概率研究中广泛被使用。但是这个模型太朴素太不严格了，在这种不完善的定义下，根本没有办法做严格的数学推理，而且有限样本空间和等可能性在很多现实随机试验中并不满足，甚至对等可能不同定义会导致不同结论。因此必须使用一个更严格的定义，以符合现代数学公理化推导的要求，这就是公理化概率。&lt;/p&gt;
&lt;h2&gt;公理化概率&lt;/h2&gt;
&lt;p&gt;公理化概率对概率做如下定义：&lt;/p&gt;
&lt;p&gt;概率是事件集合到实数域的一个函数，设事件集合为E，则如若\(A\in E\overset{p}{\rightarrow}P(A)\in \mathbb{R}\)满足：&lt;/p&gt;
&lt;p&gt;对于任意事件A，\(P(A)=0\)。&lt;/p&gt;
&lt;p&gt;对于必然事件S，\(P(S)=1\)。&lt;/p&gt;
&lt;p&gt;对于两两互斥的事件，有\(P(A_1\cup A_2\cup\cdots \cup A_n) = P(A_1)+P(A_2)+\cdots +P(A_n)\)。&lt;/p&gt;
&lt;p&gt;公理化概率对概率做了严格的数学定义，可以较好的基于公理系统进行推导和证明。但是，概率模型只是给出了概率“是什么”（定性），没有回答“是多少”（定量）这个问题。也就是说，仅有概率模型，是不能定量回答抛硬币问题的。下面介绍对概率进行定量分析的方法。&lt;/p&gt;
&lt;h1&gt;度量与估计概率&lt;/h1&gt;
&lt;p&gt;从公理化概率的角度，我们可以这样定义抛硬币试验的概率：设\(N\)是全部抛硬币的次数，而\(C_F\)是正面向上的次数，则如下函数定义了这个概率：&lt;/p&gt;
&lt;p&gt;\(P(A)=\left\{\begin{align} 0 &amp; A=\emptyset\\ \frac{C_F}{N} &amp; A=\{F\}\\ 1-\frac{C_F}{N} &amp; A=\{B\}\\ 1 &amp; A=\{F,B\} \end{align}\right.\)&lt;/p&gt;
&lt;p&gt;容易验证，这个定义完全符合公理化概率的所有条件。下面就是确定\(N\)和\(C_F\)。不幸的是，显然N是无法穷尽的，因为理论上你不可能抛无数次硬币。由于不能精确度量这个概率，因此你必须通过某个可以精确度量的值去估计这个概率，而且还要从数学上证明这个估计方法是靠谱的，最好能定量给出这个估计量的可信程度。而对不可直接观测概率的一个估计度量值就是频率。&lt;/p&gt;
&lt;h2&gt;频率估计&lt;/h2&gt;
&lt;p&gt;频率是这样定义的：事件A的频率是在相同条件下重复一个实验n次，事件A发生的次数在n次实验中的占比。一种简单的估计概率的方法就是用频率当做概率的估计。&lt;/p&gt;
&lt;p&gt;例如，我刚刚抛完十次硬币，其中六次正面，四次背面，因此根据此次实验，我估计我这枚硬币出现正面的概率为0.6。这就是频率估计。&lt;/p&gt;
&lt;p&gt;不过你一定有疑惑，为什么可以使用频率估计概率？有上面理论依据？如何对估计的准确性做出定理的分析？下面解答这些问题。&lt;/p&gt;
&lt;h2&gt;大数定律&lt;/h2&gt;
&lt;p&gt;频率估计的理论基础是大数定律。毫不夸张的说，大数定律是整个现代概率论和统计学的最重要基石，几乎一切统计方法的正确性都依赖于大数定律的正确，因此大数定律被有些人称为概率论的首要定律。&lt;/p&gt;
&lt;p&gt;大数定律直观来看表述了这样一种事实：在相同条件下，随着随机试验次数的增多，频率越来越接近于概率。注意大数定律陈述的是一个随着n趋向于无穷大时频率对真实概率的一种无限接近的趋势。&lt;/p&gt;
&lt;p&gt;下面给出大数定律的数理表述，大数定律有多重数学表述，这里取伯努利大数定律：&lt;/p&gt;
&lt;p&gt;\(\lim_{n \to \infty}{P{\left\{ \left|\frac{n_x}{n} - p \right| &lt; \varepsilon \right\}}} = 1\)&lt;/p&gt;
&lt;p&gt;其中\(n_x\)表述在n次试验中事件x出现的次数。伯努利大数定律代表的意义是，当试验次数越来越多，频率与概率相差较大的可能性变得很小。大数定律从数学上严格证明了频率对概率的收敛性以及稳定性。这就是频率估计的理论基础。在后面关于中心极限定理的部分，还将定量给出估计的置信度（表示这个估计有多可靠）。&lt;/p&gt;
&lt;h2&gt;最大似然估计&lt;/h2&gt;
&lt;p&gt;下面给出另一种估计概率的方法，就是最大似然估计。最大似然估计是参数估计的一种方法，用于在已知概率分布的情况下对分布函数的参数进行估计。而这里分布函数的参数刚好是要估计的概率。&lt;/p&gt;
&lt;p&gt;最大似然估计基于这样一个朴素的思想：如果已经得到一组试验数据，在概率分布已知的情况下，可以将出现这组试验数据的概率表述为分布函数参数的函数。&lt;/p&gt;
&lt;p&gt;看到上面的话很多人肯定又晕了，我还是举个具体的例子吧（非数学严格的例子，但思想一致）。我来到一所陌生的大学门口，想知道这所大学男生多还是女生多，我蹲在校门口数了走出校门的100名同学，发现80个男生20个女生，如果我认为这所学校每个学生这段时间内出校门的概率都是差不多的，那么我会推断男生多。因为男生多的学校更大可能性产生我观察的结果。所以，最大似然估计的核心思想就是：知道了结果，但不知道结果所在总体的情况，然后计算在总体在每种可能下产生这个结果的概率，哪种情况下产生已知结果的概率最大，就认为这种情况是总体的情况。&lt;/p&gt;
&lt;p&gt;下面正式使用这个方法估计硬币正面出现的概率。&lt;/p&gt;
&lt;p&gt;还是上面的实验，我已经得到“抛了十次，六次正面”这个结果，下面我想知道正面向上的概率。由于这个概率是一定存在的（第一节已经说明了哈，在既定哲学观点下），而且这个概率的取值范围应该是0到1的开区间（正面背面都出现过，所以不可能是0或1）：&lt;/p&gt;
&lt;p&gt;\(p\in (0,1)\)&lt;/p&gt;
&lt;p&gt;由一些背景知识知道，每抛十次硬币，正面出现的次数服从二项分布：&lt;/p&gt;
&lt;p&gt;\(C_n^kp^k(1-p)^{n-k}\)&lt;/p&gt;
&lt;p&gt;由于已知n=10，k=6，将其带入，得到一个函数：&lt;/p&gt;
&lt;p&gt;\(L(p)=C_{10}^6p^6(1-p)^{10-6}\)&lt;/p&gt;
&lt;p&gt;其中p的定义域为\(p\in (0,1)\)。这个函数表示的是，当出现正面的真实概率为p时，“抛十次六次正面”这个事件出现的概率。我们希望估计的p让这个函数取值最大，以下是求解过程：&lt;/p&gt;
&lt;p&gt;因为在(0,1)区间，ln(x)是x的单调递增函数，所以最大化lnL(p)就等于最大化L(p)。这样做主要是取对数可以让连乘变成连加，方便后面求导。&lt;/p&gt;
&lt;p&gt;由微积分知识可知：&lt;/p&gt;
&lt;p&gt;\(\frac{dlnF(p)}{dp}=C_{10}^6(\frac{6}{p}+\frac{4}{p-1})=C_{10}^6(\frac{10p-6}{p^2-p})\)&lt;/p&gt;
&lt;p&gt;让这个导数为0，解得p为0.6，这就是我们对概率的最大似然估计，与概率估计的结果一致。&lt;/p&gt;
&lt;h1&gt;显著性及假设检验&lt;/h1&gt;
&lt;p&gt;到此为止，我们已经说明了概率是存在的、建立了概率的数学模型，并能对不可直接观测的概率进行估计。但似乎还缺点什么。&lt;/p&gt;
&lt;p&gt;大数定律只说明了理论上我们的估计是靠谱的，但是到底有多靠谱，却无法通过大数定律定量计算。这一节，我们就来解决这个问题：定量计算出估计的可靠性（术语叫显著性）。&lt;/p&gt;
&lt;h2&gt;评估显著性&lt;/h2&gt;
&lt;p&gt;还是上面我抛那十次硬币的试验。根据最优的频率估计和最大似然估计，均估计p（出现正面的概率）为0.6。但是如果有人提出异议，说我的估计可能是错的，p实际是0.5，我那个出现六次正面是因为只是偶然性的结果。这时我需要找证据反驳他，由于不能做无数次试验，我只能给出一个较高可信度的证据，例如，我想证明至少95%的可能性出现六次正面是因为p不等于0.5，也就是说，证明如果p为0.5，则偶然出现我这个结果的可能性不超过5%（5%称作显著水平）。&lt;/p&gt;
&lt;h2&gt;中心极限定理&lt;/h2&gt;
&lt;p&gt;要评估显著性，首先要借助于中心极限定理。中心极限定理也是统计学的基石定理之一，它的一种表述是：&lt;/p&gt;
&lt;p&gt;设随机变量\(X_1, X_2, \cdots ,X_n\)独立同分布，且数学期望为\(\mu\)，方差为\(\sigma^2 \neq 0\)。则其均值\(\bar{X}=\frac{1}{n}\sum_{i=1}^{n}X_{i}\)近似服从期望为\(\mu\)，方差为\(\sigma/n\)的正态分布。等价的，\(\zeta_n=\frac{\bar{X} -\mu}{\sigma/\sqrt{n}}\)近似服从标准正态分布。&lt;/p&gt;
&lt;p&gt;中心极限定理的直观意义是，随便一个服从什么的总体中，你独立随机的抽取一组样本，那么样本的均值服从正态分布，并且可以根据总体的期望和方差推导出这个均值服从的正态分布的期望和方差，然后简单变换一下就可以得到一个服从标准正态分布的随机量。由于标准正态分布的概率密度函数是已知的，那么就可以得到这个量出现的概率。&lt;/p&gt;
&lt;p&gt;这样说貌似太抽象了，我们下面还是看这个定理的应用实例吧。&lt;/p&gt;
&lt;h2&gt;假设检验&lt;/h2&gt;
&lt;p&gt;上面说过，我要反驳的是抛硬币得到正面的实际概率是0.5，那么我就要证明如果p是0.5，则得到这组结果的概率是很小的（上面要求小于5%）。&lt;/p&gt;
&lt;p&gt;设正面取值为1，背面取值为0。如果p是0.5，则每一次抛硬币的取值服从一个p为0.5的0-1分布。由期望及方差的定义可知，这个分布的期望和方差分别为：&lt;/p&gt;
&lt;p&gt;\(\mu = p\times 1 + (1-p)\times 0 = 0.5\times 1 + (1-0.5)\times 0=0.5\)&lt;/p&gt;
&lt;p&gt;\(\sigma^2= (1-\mu)^2\times 0.5 + (0-\mu)^2\times 0.5 = 0.25\times 0.5 + 0.25\times 0.5 = 0.25\)&lt;/p&gt;
&lt;p&gt;由中心极限定理\(\frac{\bar{X}-0.5}{\sqrt{0.25/10}}\)近似服从标准正态分布。&lt;/p&gt;
&lt;p&gt;而我抛的十次硬币可以看做十个独立随机抽样，它们的均值是0.6，变换后的值为\(\frac{0.6-0.5}{\sqrt{0.25/10}}\approx 0.632\)。&lt;/p&gt;
&lt;p&gt;标准正态分布的概率密度公式为：&lt;/p&gt;
&lt;p&gt;\(f(x) = {1 \over \sigma\sqrt{2\pi} }\,e^{- {{(x-\mu )^2 \over 2\sigma^2}}}\)&lt;/p&gt;
&lt;p&gt;上面说过，我们希望显著水平是5%，所以，我需要找到x=z，使得此概率密度函数从-z到z的定积分为0.95，然后看0.632在不在[-z, z]内，如果在的话，我会认为我确实错了，至少我没有95%以上的把握说p不等于0.5，而如果0.632不再这个范围内，则我可以拍着胸脯说，我已经从理论上证明我有95%以上的把握，p不是0.5（换句话说，如果p是0.5，抛十次六次正面的可能性不足5%）。&lt;/p&gt;
&lt;p&gt;坦白说这个z不是很好算，不过还好由于这东西特别常用，任何一本概率课本后面都可以找到标准正态分布表（或者很多工具如R语言可以直接计算分位点），下面就是我在网上找到的一个（来源&lt;a title=&quot;http://www.mathsisfun.com/data/standard-normal-distribution-table.html&quot; href=&quot;http://www.mathsisfun.com/data/standard-normal-distribution-table.html&quot;&gt;http://www.mathsisfun.com/data/standard-normal-distribution-table.html&lt;/a&gt;）：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/basis-of-probability-and-statistics/1.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;这是一个单侧表，要保证显著水平为5%，则单侧积分上限不能低于0.475，通过查上表，可知0.475对应的z是1.96，远大于我们算出的0.632。很不幸，我在5%的显著水平下无法拒绝p=0.5的假设。同时通过上表可以看到，0.63对应的单侧概率是0.2357，也就是说，通过抛十次得到六次正面，我们只有约50%的把握说出现正面的概率不是0.5。换句话说，抛十次硬币来做频率估计是不太合适的，于是，我们需要增加试验次数。&lt;/p&gt;
&lt;p&gt;假如，我又做了100次实验，抛出了60次正面，40次背面。那么这个试验结果可以显著的认为p不是0.5吗？用同样的方法算出\(\frac{0.6-0.5}{\sqrt{0.25/100}} = 2.0\)。很显然，2.0大于1.96，所以这个试验结果可以充分（超过95%的可能）说明这枚硬币正面朝上的概率确实不是0.5。通过查表可以看到，2.0的显著水平约为0.046，换句话说，这次试验结果95.4%以上表明硬币正面出现的概率不是0.5。当然，也有可能结论是错误的，因为毕竟还有4.6%的可能这是在p=0.5的情况下偶然出现的。&lt;/p&gt;
&lt;p&gt;通过假设检验理论，可以通过增加试验次数，将犯错的概率缩小到任意小的值。&lt;/p&gt;
&lt;h1&gt;总结&lt;/h1&gt;
&lt;p&gt;这篇文章以抛硬币试验为引子引出了一系列现代数学中概率的基本模型、定理及基本的估计及显著性检验方法。写这篇文章是我无聊抛硬币时一时兴起，其中对很多东西只是给出一个轮廓，没有处处给出严格的定义和证明，不过大约说明了常用的一些统计方法及其理论基础，限于篇幅不能面面俱到，例如一个假设检验如果展开写可以单独写一篇文章。目前随着大数据概念的热炒，基于互联网的数据挖掘和机器学习也变得火热，其实很多数据挖掘和机器学习都是基于概率和统计理论的，很多方法甚至只是传统统计方法的应用。因此如果准备在这方面深入学习，不妨考虑先在概率论和数理统计方面打好基础。&lt;/p&gt;
</description>
</item>
<item>
<title>x86-64体系下一个奇怪问题的定位</title>
<link>http://blog.codinglabs.org/articles/trouble-of-x86-64-platform.html</link>
<guid>http://blog.codinglabs.org/articles/trouble-of-x86-64-platform.html</guid>
<pubDate>Mon, 12 Nov 2012 16:00:00 GMT</pubDate>
<description>&lt;p&gt;问题来源于一个朋友在百度的笔试题。上周六我一个朋友参加了百度举行的专场招聘会，其中第一道笔试题是这样的：&lt;/p&gt;
&lt;p&gt;
&lt;hr&gt;
&lt;p&gt;给出下面一段代码&lt;/p&gt;&lt;pre class=&quot;prettyprint linenums languague-c&quot;&gt;#include &lt;stdio.h&gt;
main() {
    double a = 10; 
    printf(&quot;a = %d\n&quot;, a); 
}&lt;/pre&gt;
&lt;p&gt;请问代码的运行结果以及原因。&lt;/p&gt;
&lt;p&gt;
&lt;hr&gt;
&lt;p&gt;当朋友参加完笔试和我聊起这道题时，我第一反应是这道题考察的是浮点数的内存表示，当然，在不同的CPU体系下，运行结果可能会有所不同，主要是受CPU位数和字节序的影响。&lt;/p&gt;
&lt;!--more--&gt;
&lt;h1&gt;最初分析&lt;/h1&gt;
&lt;p&gt;不妨以目前最普遍的x86-64体系（64位，小端序）考虑此问题。在64位机器上，double是符合&lt;a href=&quot;http://zh.wikipedia.org/wiki/IEEE_754&quot; target=&quot;_blank&quot;&gt;IEEE754标准&lt;/a&gt;的双精度浮点数。根据IEEE标准，双精度浮点数由8个字节共64位组成，其中最高位为符号位，次高的11位为指数位，余下的52位为尾数位。示意见下图：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/trouble-of-x86-64-platform/1.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;各位段意义如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;S = 0表示正数，S = 1表示负数。 
&lt;li&gt;E可以看成一个无符号整数，当其二进制位为全0或全1时，表示非规约浮点数或特殊值，此处不讨论，仅讨论其不全为0或全为1的情况。当E不全为零或全为1时，浮点数是规约的，此时E表示以2为底的指数加上一个固定的偏移量。偏移量被定义为\(2^{(E) - 1} - 1\)，其中(E)表示E所占的比特数，此处为11，所以偏移量为\(2^{(11) - 1} - 1=1023\)。因此实际的指数值要在E的基础上减1023，例如E的位表示是10000000000（十进制1024），则表示实际指数值为\(1024-1023=1\)。 
&lt;li&gt;M在规约形式下，表示一个二进制小数，实际值是这个小数加1。例如，M=101000…0表示\(2^{-1} + 2^{-3} + 1 = 1.625\)。 &lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;一个规约的IEEE双精度浮点数的实际值为：&lt;/p&gt;
&lt;p&gt;\(V = (-1)^S \times 2^{E - 1023} \times (1 + M)\)&lt;/p&gt;
&lt;p&gt;根据以上分析，10可以表示为\(1.25 \times 8\)，因此取S = 0，E = 10000000010，M = 0100…0，则整个浮点数的二进制表示为：&lt;/p&gt;
&lt;p&gt;01000000, 00100100, 00000000, 00000000, 00000000, 00000000, 00000000, 00000000&lt;/p&gt;
&lt;p&gt;为了便于观察我在每8bit之间插入了分隔符。当printf使用“%d”输出时，由于int类型是4字节，所以只能取其中四个字节。当a被当做参数传递给printf时，有两种可能保存a的地方：寄存器或栈帧中。&lt;/p&gt;
&lt;p&gt;如果是寄存器，则printf会取低四字节。&lt;/p&gt;
&lt;p&gt;如果是栈，在小端序中，高字节存放在高地址，低字节放在低地址，而栈是从高地址向低地址增长的，所以入栈后每个字节的位置如下：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/trouble-of-x86-64-platform/2.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;printf会从低地址到高地址读取4个字节当做int型数据去解释并输出，所以，经过分析这段代码的输出应该为“a = 0”。&lt;/p&gt;
&lt;h1&gt;奇怪的结果&lt;/h1&gt;
&lt;p&gt;分析完了，下一步当然是通过实践验证，我在我的VPS上（CentOS 64位）用gcc编译。结果非常出乎意料，不但不是0，而且每次运行的结果都不一样！（见下图）&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/trouble-of-x86-64-platform/3.png&quot;/&gt;&lt;/p&gt;
&lt;h1&gt;定位问题&lt;/h1&gt;
&lt;p&gt;在试图解释这个奇怪现象时，我最初从C的层面上进行了诸多分析，结果都无法分析出问题所在，所以我怀疑出现这个问题的原因在机器代码层面。于是我将其汇编代码打出来：&lt;/p&gt;&lt;pre class=&quot;prettyprint linenums languague-asm&quot;&gt;.file   &quot;double_as_int.c&quot;
.section    .rodata.str1.1,&quot;aMS&quot;,@progbits,1
.LC1:
.string &quot;a = %d\n&quot;
.text
.globl main
.type   main, @function
main:
.LFB11:
.cfi_startproc
subq    $8, %rsp
.cfi_def_cfa_offset 16
movsd   .LC0(%rip), %xmm0
movl    $.LC1, %edi
movl    $1, %eax
call    printf
addq    $8, %rsp
.cfi_def_cfa_offset 8
ret 
.cfi_endproc
.LFE11:
.size   main, .-main
.section    .rodata.cst8,&quot;aM&quot;,@progbits,8
.align 8
.LC0:
.long   0   
.long   1076101120
.ident  &quot;GCC: (GNU) 4.4.6 20120305 (Red Hat 4.4.6-4)&quot;
.section    .note.GNU-stack,&quot;&quot;,@progbits&lt;/pre&gt;
&lt;p&gt;&lt;br&gt;为了方便对比，我重新写了下面的C代码：&lt;br&gt;&lt;/p&gt;&lt;pre class=&quot;prettyprint linenums languague-c&quot;&gt;#include &lt;stdio.h&gt;

main() {
    int a = 10; 
    printf(&quot;a = %d\n&quot;, a); 
}&lt;/pre&gt;
&lt;p&gt;&lt;br&gt;其汇编为：&lt;br&gt;&lt;/p&gt;&lt;pre class=&quot;prettyprint linenums languague-asm&quot;&gt;.file   &quot;int_as_int.c&quot;
.section    .rodata.str1.1,&quot;aMS&quot;,@progbits,1
.LC0:
.string &quot;a = %d\n&quot;
.text
.globl main
.type   main, @function
main:
.LFB11:
.cfi_startproc
subq    $8, %rsp
.cfi_def_cfa_offset 16
movl    $10, %esi
movl    $.LC0, %edi
movl    $0, %eax
call    printf
addq    $8, %rsp
.cfi_def_cfa_offset 8
ret 
.cfi_endproc
.LFE11:
.size   main, .-main
.ident  &quot;GCC: (GNU) 4.4.6 20120305 (Red Hat 4.4.6-4)&quot;
.section    .note.GNU-stack,&quot;&quot;,@progbits&lt;/pre&gt;
&lt;p&gt;将注意力集中在main函数中调用printf之前的行为，可以看到，在第一段代码中，LC0有个常数1076101120，将其转换为二进制刚好是我们上面分析的双精度10的二进制表示，而汇编代码将这个数送入了一个叫xmm0寄存器。通过查阅x86-64处理器的相关资料，知道这个寄存器和&lt;a href=&quot;http://en.wikipedia.org/wiki/SIMD&quot; target=&quot;_blank&quot;&gt;SIMD&lt;/a&gt;（单指令多数据流）扩展指令集有关。简单来说，在64位操作系统下，x86-64通过SIMD机制提高浮点运算能力，所以double类型的a被送入了xmm0（SIMD会用到8个128bit寄存器，xmm0 - xmm7）。&lt;/p&gt;
&lt;p&gt;对比一下第二段代码，当a被声明是int类型时，立即数10被送入了esi（一个通用寄存器，在64位CPU中表示rsi的低32位）。其它部分似乎没有区别。&lt;/p&gt;
&lt;p&gt;通过对比，我猜测64位操作系统下由于启用了SIMD，浮点数会被送入mmx寄存器，而整形会被送入通用寄存器。为了证实我的想法，我查阅了&lt;a href=&quot;http://www.x86-64.org/documentation/abi.pdf&quot; target=&quot;_blank&quot;&gt;x86-64的ABI文档&lt;/a&gt;，在“3.2.3 Parameter Passing”一小节找到了如下的文字：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;INTEGER&lt;/strong&gt; This class consists of integral types that ﬁt into one of the general purpose registers.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SSE&lt;/strong&gt; The class consists of types that ﬁt into a vector register.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;这段话和相关汇编代码基本印证了我的猜测。为了进一步验证，我考虑手工改一下汇编代码，将movsd .LC0(%rip), %xmm0改为将数据送入rsi（其低32位就是esi），修改后代码如下，注意第13行代码是我修改过的：&lt;/p&gt;&lt;pre class=&quot;prettyprint linenums languague-asm&quot;&gt;.file   &quot;double_as_int.c&quot;
.section    .rodata.str1.1,&quot;aMS&quot;,@progbits,1
.LC1:
.string &quot;a = %d\n&quot;
.text
.globl main
.type   main, @function
main:
.LFB11:
.cfi_startproc
subq    $8, %rsp
.cfi_def_cfa_offset 16
movq    .LC0(%rip), %rsi
movl    $.LC1, %edi
movl    $1, %eax
call    printf
addq    $8, %rsp
.cfi_def_cfa_offset 8
ret 
.cfi_endproc
.LFE11:
.size   main, .-main
.section    .rodata.cst8,&quot;aM&quot;,@progbits,8
.align 8
.LC0:
.long   0   
.long   1076101120
.ident  &quot;GCC: (GNU) 4.4.6 20120305 (Red Hat 4.4.6-4)&quot;
.section    .note.GNU-stack,&quot;&quot;,@progbits&lt;/pre&gt;
&lt;p&gt;编译这段汇编代码执行，果然结果固定为0：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/trouble-of-x86-64-platform/4.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;最后，我用-m32指令编译成32位代码，结果也固定为0，并且汇编代码中没有看到mmx相关寄存器的使用。然后，我手工用movl将12345送入esi，结果为输出总为12345，证明printf默认认为第一个int参数放在esi中。至此问题原因基本确定。&lt;/p&gt;
&lt;h1&gt;总结&lt;/h1&gt;
&lt;p&gt;从上述过程知道，最初的笔试代码，在64位环境下，浮点数参数被送入mmx寄存器，而%d告诉printf第一个参数为int类型，所以printf仍然去默认的esi中寻找第一个int参数，所以从esi中读取了一个未确定的32bit数据并按int解释，最终造成结果的不确定。&lt;/p&gt;
&lt;p&gt;所以这道题的正确答案（小端序）是，在32位下，输出为“a = 0”；在64位启用SIMD情况下，输出结果不确定。&lt;/p&gt;
&lt;p&gt;特别需要说明的是，由于汇编代码在不同CPU、不同操作系统、不同gcc选项下可能会有差异，所以你得到的汇编代码未必和我的相同，但原因是一致的：64位环境下int和double放置的位置不同，double告诉a放到一个地方，而%d告诉printf到另一个地方取数据，结果自然无法取到变量a。&lt;/p&gt;
&lt;p&gt;由此也可以看出，printf最好不要将占位符和实际参数设为不同的类型，因为这样会造成不可预料的结果。&lt;/p&gt;
&lt;h1&gt;参考文献&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;http://www.x86-64.org/documentation/abi.pdf&quot; target=&quot;_blank&quot;&gt;System V Application Binary Interface AMD64 Architecture Processor Supplement&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://grouper.ieee.org/groups/754/&quot; target=&quot;_blank&quot;&gt;IEEE 754: Standard for Binary Floating-Point Arithmetic&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.cs.virginia.edu/~evans/cs216/guides/x86.html&quot; target=&quot;_blank&quot;&gt;x86 Assembly Guide&lt;/a&gt;&lt;/p&gt;
</description>
</item>
<item>
<title>网站统计中的数据收集原理及实现</title>
<link>http://blog.codinglabs.org/articles/how-web-analytics-data-collection-system-work.html</link>
<guid>http://blog.codinglabs.org/articles/how-web-analytics-data-collection-system-work.html</guid>
<pubDate>Tue, 23 Oct 2012 16:00:00 GMT</pubDate>
<description>&lt;p&gt;网站数据统计分析工具是网站站长和运营人员经常使用的一种工具，比较常用的有&lt;a href=&quot;http://www.google.com/analytics/&quot; target=&quot;_blank&quot;&gt;谷歌分析&lt;/a&gt;、&lt;a href=&quot;http://tongji.baidu.com&quot; target=&quot;_blank&quot;&gt;百度统计&lt;/a&gt;和&lt;a href=&quot;http://ta.qq.com&quot; target=&quot;_blank&quot;&gt;腾讯分析&lt;/a&gt;等等。所有这些统计分析工具的第一步都是网站访问数据的收集。目前主流的数据收集方式基本都是基于javascript的。本文将简要分析这种数据收集的原理，并一步一步实际搭建一个实际的数据收集系统。
&lt;!--more--&gt;
&lt;/p&gt;
&lt;h1&gt;数据收集原理分析&lt;/h1&gt;
&lt;p&gt;简单来说，网站统计分析工具需要收集到用户浏览目标网站的行为（如打开某网页、点击某按钮、将商品加入购物车等）及行为附加数据（如某下单行为产生的订单金额等）。早期的网站统计往往只收集一种用户行为：页面的打开。而后用户在页面中的行为均无法收集。这种收集策略能满足基本的流量分析、来源分析、内容分析及访客属性等常用分析视角，但是，随着ajax技术的广泛使用及电子商务网站对于电子商务目标的统计分析的需求越来越强烈，这种传统的收集策略已经显得力不能及。&lt;/p&gt;
&lt;p&gt;后来，Google在其产品谷歌分析中创新性的引入了可定制的数据收集脚本，用户通过谷歌分析定义好的可扩展接口，只需编写少量的javascript代码就可以实现自定义事件和自定义指标的跟踪和分析。目前百度统计、搜狗分析等产品均照搬了谷歌分析的模式。&lt;/p&gt;
&lt;p&gt;其实说起来两种数据收集模式的基本原理和流程是一致的，只是后一种通过javascript收集到了更多的信息。下面看一下现在各种网站统计工具的数据收集基本原理。&lt;/p&gt;
&lt;h2&gt;流程概览&lt;/h2&gt;
&lt;p&gt;首先通过一幅图总体看一下数据收集的基本流程。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/how-web-analytics-data-collection-system-work/1.png&quot;/&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;strong&gt;图1. 网站统计数据收集基本流程&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;首先，用户的行为会触发浏览器对被统计页面的一个http请求，这里姑且先认为行为就是打开网页。当网页被打开，页面中的埋点javascript片段会被执行，用过相关工具的朋友应该知道，一般网站统计工具都会要求用户在网页中加入一小段javascript代码，这个代码片段一般会动态创建一个script标签，并将src指向一个单独的js文件，此时这个单独的js文件（图1中绿色节点）会被浏览器请求到并执行，这个js往往就是真正的数据收集脚本。数据收集完成后，js会请求一个后端的数据收集脚本（图1中的backend），这个脚本一般是一个伪装成图片的动态脚本程序，可能由php、python或其它服务端语言编写，js会将收集到的数据通过http参数的方式传递给后端脚本，后端脚本解析参数并按固定格式记录到访问日志，同时可能会在http响应中给客户端种植一些用于追踪的cookie。&lt;/p&gt;
&lt;p&gt;上面是一个数据收集的大概流程，下面以谷歌分析为例，对每一个阶段进行一个相对详细的分析。&lt;/p&gt;
&lt;h2&gt;埋点脚本执行阶段&lt;/h2&gt;
&lt;p&gt;若要使用谷歌分析（以下简称GA），需要在页面中插入一段它提供的javascript片段，这个片段往往被称为埋点代码。下面是我的博客中所放置的谷歌分析埋点代码截图：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/how-web-analytics-data-collection-system-work/2.png&quot;/&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;strong&gt;图2. 谷歌分析埋点代码&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;其中_gaq是GA的的全局数组，用于放置各种配置，其中每一条配置的格式为：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-js&quot;&gt;_gaq.push(['Action', 'param1', 'param2', ...]);&lt;/pre&gt;
&lt;p&gt;Action指定配置动作，后面是相关的参数列表。GA给的默认埋点代码会给出两条预置配置，_setAccount用于设置网站标识ID，这个标识ID是在注册GA时分配的。_trackPageview告诉GA跟踪一次页面访问。更多配置请参考：&lt;a title=&quot;https://developers.google.com/analytics/devguides/collection/gajs/&quot; href=&quot;https://developers.google.com/analytics/devguides/collection/gajs/&quot; target=&quot;_blank&quot;&gt;https://developers.google.com/analytics/devguides/collection/gajs/&lt;/a&gt;。实际上，这个_gaq是被当做一个FIFO队列来用的，配置代码不必出现在埋点代码之前，具体请参考上述链接的说明。&lt;/p&gt;
&lt;p&gt;就本文来说，_gaq的机制不是重点，重点是后面匿名函数的代码，这才是埋点代码真正要做的。这段代码的主要目的就是引入一个外部的js文件（ga.js），方式是通过document.createElement方法创建一个script并根据协议（http或https）将src指向对应的ga.js，最后将这个element插入页面的dom树上。&lt;/p&gt;
&lt;p&gt;注意ga.async = true的意思是异步调用外部js文件，即不阻塞浏览器的解析，待外部js下载完成后异步执行。这个属性是HTML5新引入的。&lt;/p&gt;
&lt;h2&gt;数据收集脚本执行阶段&lt;/h2&gt;
&lt;p&gt;数据收集脚本（ga.js）被请求后会被执行，这个脚本一般要做如下几件事：&lt;/p&gt;
&lt;p&gt;1、通过浏览器内置javascript对象收集信息，如页面title（通过document.title）、referrer（上一跳url，通过document.referrer）、用户显示器分辨率（通过windows.screen）、cookie信息（通过document.cookie）等等一些信息。&lt;/p&gt;
&lt;p&gt;2、解析_gaq收集配置信息。这里面可能会包括用户自定义的事件跟踪、业务数据（如电子商务网站的商品编号等）等。&lt;/p&gt;
&lt;p&gt;3、将上面两步收集的数据按预定义格式解析并拼接。&lt;/p&gt;
&lt;p&gt;4、请求一个后端脚本，将信息放在http request参数中携带给后端脚本。&lt;/p&gt;
&lt;p&gt;这里唯一的问题是步骤4，javascript请求后端脚本常用的方法是ajax，但是ajax是不能跨域请求的。这里ga.js在被统计网站的域内执行，而后端脚本在另外的域（GA的后端统计脚本是&lt;a href=&quot;http://www.google-analytics.com/__utm.gif&quot;&gt;http://www.google-analytics.com/__utm.gif&lt;/a&gt;），ajax行不通。一种通用的方法是js脚本创建一个Image对象，将Image对象的src属性指向后端脚本并携带参数，此时即实现了跨域请求后端。这也是后端脚本为什么通常伪装成gif文件的原因。通过http抓包可以看到ga.js对__utm.gif的请求：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/how-web-analytics-data-collection-system-work/3.png&quot;/&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;strong&gt;图3. 后端脚本请求的http包&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;可以看到ga.js在请求__utm.gif时带了很多信息，例如utmsr=1280×1024是屏幕分辨率，utmac=UA-35712773-1是_gaq中解析出的我的GA标识ID等等。&lt;/p&gt;
&lt;p&gt;值得注意的是，__utm.gif未必只会在埋点代码执行时被请求，如果用_trackEvent配置了事件跟踪，则在事件发生时也会请求这个脚本。&lt;/p&gt;
&lt;p&gt;由于ga.js经过了压缩和混淆，可读性很差，我们就不分析了，具体后面实现阶段我会实现一个功能类似的脚本。&lt;/p&gt;
&lt;h2&gt;后端脚本执行阶段&lt;/h2&gt;
&lt;p&gt;GA的__utm.gif是一个伪装成gif的脚本。这种后端脚本一般要完成以下几件事情：&lt;/p&gt;
&lt;p&gt;1、解析http请求参数的到信息。&lt;/p&gt;
&lt;p&gt;2、从服务器（WebServer）中获取一些客户端无法获取的信息，如访客ip等。&lt;/p&gt;
&lt;p&gt;3、将信息按格式写入log。&lt;/p&gt;
&lt;p&gt;5、生成一副1×1的空gif图片作为响应内容并将响应头的Content-type设为image/gif。&lt;/p&gt;
&lt;p&gt;5、在响应头中通过Set-cookie设置一些需要的cookie信息。&lt;/p&gt;
&lt;p&gt;之所以要设置cookie是因为如果要跟踪唯一访客，通常做法是如果在请求时发现客户端没有指定的跟踪cookie，则根据规则生成一个全局唯一的cookie并种植给用户，否则Set-cookie中放置获取到的跟踪cookie以保持同一用户cookie不变（见图4）。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/how-web-analytics-data-collection-system-work/4.png&quot;/&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;strong&gt;图4. 通过cookie跟踪唯一用户的原理&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;这种做法虽然不是完美的（例如用户清掉cookie或更换浏览器会被认为是两个用户），但是是目前被广泛使用的手段。注意，如果没有跨站跟踪同一用户的需求，可以通过js将cookie种植在被统计站点的域下（GA是这么做的），如果要全网统一定位，则通过后端脚本种植在服务端域下（我们待会的实现会这么做）。&lt;/p&gt;
&lt;h1&gt;系统的设计实现&lt;/h1&gt;
&lt;p&gt;根据上述原理，我自己搭建了一个访问日志收集系统。总体来说，搭建这个系统要做如下的事：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/how-web-analytics-data-collection-system-work/5.png&quot;/&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;strong&gt;图5. 访问数据收集系统工作分解&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;下面详述每一步的实现。我将这个系统叫做MyAnalytics。&lt;/p&gt;
&lt;h2&gt;确定收集的信息&lt;/h2&gt;
&lt;p&gt;为了简单起见，我不打算实现GA的完整数据收集模型，而是收集以下信息。&lt;/p&gt;
&lt;table width=&quot;588&quot; border=&quot;1&quot; cellspacing=&quot;0&quot; cellpadding=&quot;2&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot; width=&quot;133&quot;&gt;名称&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;133&quot;&gt;途径&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;320&quot;&gt;备注&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot; width=&quot;133&quot;&gt;访问时间&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;133&quot;&gt;web server&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;320&quot;&gt;Nginx $msec&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot; width=&quot;133&quot;&gt;IP&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;133&quot;&gt;web server&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;320&quot;&gt;Nginx $remote_addr&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot; width=&quot;133&quot;&gt;域名&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;133&quot;&gt;javascript&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;320&quot;&gt;document.domain&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot; width=&quot;133&quot;&gt;URL&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;133&quot;&gt;javascript&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;320&quot;&gt;document.URL&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot; width=&quot;133&quot;&gt;页面标题&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;133&quot;&gt;javascript&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;320&quot;&gt;document.title&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot; width=&quot;133&quot;&gt;分辨率&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;133&quot;&gt;javascript&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;320&quot;&gt;window.screen.height &amp; width&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot; width=&quot;133&quot;&gt;颜色深度&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;133&quot;&gt;javascript&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;320&quot;&gt;window.screen.colorDepth&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot; width=&quot;133&quot;&gt;Referrer&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;133&quot;&gt;javascript&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;320&quot;&gt;document.referrer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot; width=&quot;133&quot;&gt;浏览客户端&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;133&quot;&gt;web server&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;320&quot;&gt;Nginx $http_user_agent&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot; width=&quot;133&quot;&gt;客户端语言&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;133&quot;&gt;javascript&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;320&quot;&gt;navigator.language&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot; width=&quot;133&quot;&gt;访客标识&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;133&quot;&gt;cookie&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;320&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot; width=&quot;133&quot;&gt;网站标识&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;133&quot;&gt;javascript&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;320&quot;&gt;自定义对象&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;埋点代码&lt;/h2&gt;
&lt;p&gt;埋点代码我将借鉴GA的模式，但是目前不会将配置对象作为一个FIFO队列用。一个埋点代码的模板如下：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-js&quot;&gt;&lt;script type=&quot;text/javascript&quot;&gt;
var _maq = _maq || [];
_maq.push(['_setAccount', '网站标识']);

(function() {
 var ma = document.createElement('script'); ma.type = 'text/javascript'; ma.async = true;
 ma.src = ('https:' == document.location.protocol ? 'https://analytics' : 'http://analytics') + '.codinglabs.org/ma.js';
 var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ma, s);
 })();
&lt;/script&gt;&lt;/pre&gt;
&lt;p&gt;这里我启用了二级域名analytics.codinglabs.org，统计脚本的名称为ma.js。当然这里有一点小问题，因为我并没有https的服务器，所以如果一个https站点部署了代码会有问题，不过这里我们先忽略吧。&lt;/p&gt;
&lt;h2&gt;前端统计脚本&lt;/h2&gt;
&lt;p&gt;我写了一个不是很完善但能完成基本工作的统计脚本ma.js：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-js&quot;&gt;(function () {
        var params = {};
        //Document对象数据
        if(document) {
        params.domain = document.domain || ''; 
        params.url = document.URL || ''; 
        params.title = document.title || ''; 
        params.referrer = document.referrer || ''; 
        }   
        //Window对象数据
        if(window &amp;&amp; window.screen) {
        params.sh = window.screen.height || 0;
        params.sw = window.screen.width || 0;
        params.cd = window.screen.colorDepth || 0;
        }   
        //navigator对象数据
        if(navigator) {
        params.lang = navigator.language || ''; 
        }   
        //解析_maq配置
        if(_maq) {
            for(var i in _maq) {
                switch(_maq[i][0]) {
                    case '_setAccount':
                        params.account = _maq[i][1];
                        break;
                    default:
                        break;
                }   
            }   
        }   
        //拼接参数串
        var args = ''; 
        for(var i in params) {
            if(args != '') {
                args += '&amp;';
            }   
            args += i + '=' + encodeURIComponent(params[i]);
        }   

        //通过Image对象请求后端脚本
        var img = new Image(1, 1); 
        img.src = 'http://analytics.codinglabs.org/1.gif?' + args;
})();&lt;/pre&gt;
&lt;p&gt;整个脚本放在匿名函数里，确保不会污染全局环境。功能在原理一节已经说明，不再赘述。其中1.gif是后端脚本。&lt;/p&gt;
&lt;h2&gt;日志格式&lt;/h2&gt;
&lt;p&gt;日志采用每行一条记录的方式，采用不可见字符^A（ascii码0x01，Linux下可通过ctrl + v ctrl + a输入，下文均用“^A”表示不可见字符0x01），具体格式如下：&lt;/p&gt;
&lt;p&gt;时间&lt;span style=&quot;color: #4bacc6;&quot;&gt;^A&lt;/span&gt;IP&lt;span style=&quot;color: #4bacc6;&quot;&gt;^A&lt;/span&gt;域名&lt;span style=&quot;color: #4bacc6;&quot;&gt;^A&lt;/span&gt;URL&lt;span style=&quot;color: #4bacc6;&quot;&gt;^A&lt;/span&gt;页面标题&lt;span style=&quot;color: #4bacc6;&quot;&gt;^A&lt;/span&gt;&lt;span style=&quot;color: #000000;&quot;&gt;Referrer&lt;/span&gt;&lt;span style=&quot;color: #4bacc6;&quot;&gt;^A&lt;/span&gt;分辨率高&lt;span style=&quot;color: #4bacc6;&quot;&gt;^A&lt;/span&gt;分辨率宽&lt;span style=&quot;color: #4bacc6;&quot;&gt;^A&lt;/span&gt;颜色深度&lt;span style=&quot;color: #4bacc6;&quot;&gt;^A&lt;/span&gt;语言&lt;span style=&quot;color: #4bacc6;&quot;&gt;^A&lt;/span&gt;客户端信息&lt;span style=&quot;color: #4bacc6;&quot;&gt;^A&lt;/span&gt;用户标识&lt;span style=&quot;color: #4bacc6;&quot;&gt;^A&lt;/span&gt;网站标识&lt;/p&gt;
&lt;h2&gt;后端脚本&lt;/h2&gt;
&lt;p&gt;为了简单和效率考虑，我打算直接使用nginx的access_log做日志收集，不过有个问题就是nginx配置本身的逻辑表达能力有限，所以我选用了&lt;a href=&quot;http://openresty.org/&quot; target=&quot;_blank&quot;&gt;OpenResty&lt;/a&gt;做这个事情。OpenResty是一个基于Nginx扩展出的高性能应用开发平台，内部集成了诸多有用的模块，其中的核心是通过ngx_lua模块集成了Lua，从而在nginx配置文件中可以通过Lua来表述业务。关于这个平台我这里不做过多介绍，感兴趣的同学可以参考其官方网站&lt;a title=&quot;http://openresty.org/&quot; href=&quot;http://openresty.org/&quot; target=&quot;_blank&quot;&gt;http://openresty.org/&lt;/a&gt;，或者这里有其作者章亦春（agentzh）做的一个非常有爱的介绍OpenResty的slide：&lt;a href=&quot;http://agentzh.org/misc/slides/ngx-openresty-ecosystem/&quot; target=&quot;_blank&quot;&gt;http://agentzh.org/misc/slides/ngx-openresty-ecosystem/&lt;/a&gt;，关于ngx_lua可以参考：&lt;a title=&quot;https://github.com/chaoslawful/lua-nginx-module&quot; href=&quot;https://github.com/chaoslawful/lua-nginx-module&quot; target=&quot;_blank&quot;&gt;https://github.com/chaoslawful/lua-nginx-module&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;首先，需要在nginx的配置文件中定义日志格式：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-bash&quot;&gt;log_format tick &quot;$msec^A$remote_addr^A$u_domain^A$u_url^A$u_title^A$u_referrer^A$u_sh^A$u_sw^A$u_cd^A$u_lang^A$http_user_agent^A$u_utrace^A$u_account&quot;;&lt;/pre&gt;
&lt;p&gt;注意这里以u_开头的是我们待会会自己定义的变量，其它的是nginx内置变量。&lt;/p&gt;
&lt;p&gt;然后是核心的两个location：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-bash&quot;&gt;location /1.gif {
#伪装成gif文件
    default_type image/gif;    
#本身关闭access_log，通过subrequest记录log
    access_log off;

    access_by_lua &quot;
        -- 用户跟踪cookie名为__utrace
        local uid = ngx.var.cookie___utrace        
        if not uid then
            -- 如果没有则生成一个跟踪cookie，算法为md5(时间戳+IP+客户端信息)
                uid = ngx.md5(ngx.now() .. ngx.var.remote_addr .. ngx.var.http_user_agent)
                end 
                ngx.header['Set-Cookie'] = {'__utrace=' .. uid .. '; path=/'}
    if ngx.var.arg_domain then
        -- 通过subrequest到/i-log记录日志，将参数和用户跟踪cookie带过去
            ngx.location.capture('/i-log?' .. ngx.var.args .. '&amp;utrace=' .. uid)
            end 
            &quot;;  

#此请求不缓存
            add_header Expires &quot;Fri, 01 Jan 1980 00:00:00 GMT&quot;;
    add_header Pragma &quot;no-cache&quot;;
    add_header Cache-Control &quot;no-cache, max-age=0, must-revalidate&quot;;

#返回一个1×1的空gif图片
    empty_gif;
}   

location /i-log {
#内部location，不允许外部直接访问
    internal;

#设置变量，注意需要unescape
    set_unescape_uri $u_domain $arg_domain;
    set_unescape_uri $u_url $arg_url;
    set_unescape_uri $u_title $arg_title;
    set_unescape_uri $u_referrer $arg_referrer;
    set_unescape_uri $u_sh $arg_sh;
    set_unescape_uri $u_sw $arg_sw;
    set_unescape_uri $u_cd $arg_cd;
    set_unescape_uri $u_lang $arg_lang;
    set_unescape_uri $u_utrace $arg_utrace;
    set_unescape_uri $u_account $arg_account;

#打开日志
    log_subrequest on;
#记录日志到ma.log，实际应用中最好加buffer，格式为tick
    access_log /path/to/logs/directory/ma.log tick;

#输出空字符串
    echo '';
}&lt;/pre&gt;
&lt;p&gt;要完全解释这段脚本的每一个细节有点超出本文的范围，而且用到了诸多第三方ngxin模块（全都包含在OpenResty中了），重点的地方我都用注释标出来了，可以不用完全理解每一行的意义，只要大约知道这个配置完成了我们在原理一节提到的后端逻辑就可以了。&lt;/p&gt;
&lt;h2&gt;日志轮转&lt;/h2&gt;
&lt;p&gt;真正的日志收集系统访问日志会非常多，时间一长文件变得很大，而且日志放在一个文件不便于管理。所以通常要按时间段将日志切分，例如每天或每小时切分一个日志。我这里为了效果明显，每一小时切分一个日志。我是通过crontab定时调用一个shell脚本实现的，shell脚本如下：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-bash&quot;&gt;_prefix=&quot;/path/to/nginx&quot;
time=`date +%Y%m%d%H`

mv ${_prefix}/logs/ma.log ${_prefix}/logs/ma/ma-${time}.log
kill -USR1 `cat ${_prefix}/logs/nginx.pid`&lt;/pre&gt;
&lt;p&gt;这个脚本将ma.log移动到指定文件夹并重命名为ma-{yyyymmddhh}.log，然后向nginx发送USR1信号令其重新打开日志文件。&lt;/p&gt;
&lt;p&gt;然后再/etc/crontab里加入一行：&lt;/p&gt;
&lt;pre class=&quot;brush:bash&quot;&gt;59  *  *  *  * root /path/to/directory/rotatelog.sh&lt;/pre&gt;
&lt;p&gt;在每个小时的59分启动这个脚本进行日志轮转操作。&lt;/p&gt;
&lt;h2&gt;测试&lt;/h2&gt;
&lt;p&gt;下面可以测试这个系统是否能正常运行了。我昨天就在我的博客中埋了相关的点，通过http抓包可以看到ma.js和1.gif已经被正确请求：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/how-web-analytics-data-collection-system-work/6.png&quot;/&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;strong&gt;图6. http包分析ma.js和1.gif的请求&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;同时可以看一下1.gif的请求参数：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/how-web-analytics-data-collection-system-work/7.png&quot;/&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;strong&gt;图7. 1.gif的请求参数&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;相关信息确实也放在了请求参数中。&lt;/p&gt;
&lt;p&gt;然后我tail打开日志文件，然后刷新一下页面，因为没有设access log buffer， 我立即得到了一条新日志：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-bash&quot;&gt;1351060731.360^A0.0.0.0^Awww.codinglabs.org^Ahttp://www.codinglabs.org/^ACodingLabs^A^A1024^A1280^A24^Azh-CN^AMozilla/5.0 (Macintosh; Intel Mac OS X 10_8_2) AppleWebKit/537.4 (KHTML, like Gecko) Chrome/22.0.1229.94 Safari/537.4^A4d612be64366768d32e623d594e82678^AU-1-1&lt;/pre&gt;
&lt;p&gt;注意实际上原日志中的^A是不可见的，这里我用可见的^A替换为方便阅读，另外IP由于涉及隐私我替换为了0.0.0.0。&lt;/p&gt;
&lt;p&gt;看一眼日志轮转目录，由于我之前已经埋了点，所以已经生成了很多轮转文件：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/how-web-analytics-data-collection-system-work/8.png&quot;/&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;strong&gt;图8. 轮转日志&lt;/strong&gt;&lt;/p&gt;
&lt;h1&gt;关于分析&lt;/h1&gt;
&lt;p&gt;通过上面的分析和开发可以大致理解一个网站统计的日志收集系统是如何工作的。有了这些日志，就可以进行后续的分析了。本文只注重日志收集，所以不会写太多关于分析的东西。&lt;/p&gt;
&lt;p&gt;注意，原始日志最好尽量多的保留信息而不要做过多过滤和处理。例如上面的MyAnalytics保留了毫秒级时间戳而不是格式化后的时间，时间的格式化是后面的系统做的事而不是日志收集系统的责任。后面的系统根据原始日志可以分析出很多东西，例如通过IP库可以定位访问者的地域、user agent中可以得到访问者的操作系统、浏览器等信息，再结合复杂的分析模型，就可以做流量、来源、访客、地域、路径等分析了。当然，一般不会直接对原始日志分析，而是会将其清洗格式化后转存到其它地方，如MySQL或HBase中再做分析。&lt;/p&gt;
&lt;p&gt;分析部分的工作有很多开源的基础设施可以使用，例如实时分析可以使用&lt;a href=&quot;https://github.com/nathanmarz/storm&quot; target=&quot;_blank&quot;&gt;Storm&lt;/a&gt;，而离线分析可以使用&lt;a href=&quot;http://hadoop.apache.org/&quot; target=&quot;_blank&quot;&gt;Hadoop&lt;/a&gt;。当然，在日志比较小的情况下，也可以通过shell命令做一些简单的分析，例如，下面三条命令可以分别得出我的博客在今天上午8点到9点的访问量（PV），访客数（UV）和独立IP数（IP）：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-bash&quot;&gt;awk -F^A '{print $1}' ma-2012102409.log | wc -l
awk -F^A '{print $12}' ma-2012102409.log | uniq | wc -l
awk -F^A '{print $2}' ma-2012102409.log | uniq | wc -l&lt;/pre&gt;
&lt;p&gt;其它好玩的东西朋友们可以慢慢挖掘。&lt;/p&gt;
&lt;h1&gt;参考&lt;/h1&gt;
&lt;p&gt;GA的开发者文档：&lt;a title=&quot;https://developers.google.com/analytics/devguides/collection/gajs/&quot; href=&quot;https://developers.google.com/analytics/devguides/collection/gajs/&quot; target=&quot;_blank&quot;&gt;https://developers.google.com/analytics/devguides/collection/gajs/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;一篇关于实现nginx收日志的文章：&lt;a title=&quot;http://blog.linezing.com/2011/11/%E4%BD%BF%E7%94%A8nginx%E8%AE%B0%E6%97%A5%E5%BF%97&quot; href=&quot;http://blog.linezing.com/2011/11/%E4%BD%BF%E7%94%A8nginx%E8%AE%B0%E6%97%A5%E5%BF%97&quot; target=&quot;_blank&quot;&gt;http://blog.linezing.com/2011/11/%E4%BD%BF%E7%94%A8nginx%E8%AE%B0%E6%97%A5%E5%BF%97&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;关于Nginx可以参考：&lt;a title=&quot;http://wiki.nginx.org/Main&quot; href=&quot;http://wiki.nginx.org/Main&quot; target=&quot;_blank&quot;&gt;http://wiki.nginx.org/Main&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;OpenResty的官方网站为：&lt;a href=&quot;http://openresty.org&quot; target=&quot;_blank&quot;&gt;http://openresty.org&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ngx_lua模块可参考：&lt;a title=&quot;https://github.com/chaoslawful/lua-nginx-module&quot; href=&quot;https://github.com/chaoslawful/lua-nginx-module&quot; target=&quot;_blank&quot;&gt;https://github.com/chaoslawful/lua-nginx-module&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文http抓包使用&lt;a href=&quot;http://www.google.com/chrome&quot; target=&quot;_blank&quot;&gt;Chrome&lt;/a&gt;浏览器开发者工具，绘制思维导图使用&lt;a href=&quot;http://www.xmind.net/&quot; target=&quot;_blank&quot;&gt;Xmind&lt;/a&gt;，流程和结构图使用&lt;a href=&quot;http://www.texample.net/tikz/&quot; target=&quot;_blank&quot;&gt;Tikz PGF&lt;/a&gt;&lt;/p&gt;
</description>
</item>
<item>
<title>聊聊如何检测素数</title>
<link>http://blog.codinglabs.org/articles/prime-test.html</link>
<guid>http://blog.codinglabs.org/articles/prime-test.html</guid>
<pubDate>Mon, 27 Aug 2012 16:00:00 GMT</pubDate>
<description>&lt;p&gt;最近看到一则&lt;a href=&quot;http://edu.people.com.cn/n/2012/0822/c1053-18799177.html&quot; target=&quot;_blank&quot;&gt;颇为有趣的新闻&lt;/a&gt;，说北大一名大一新生，以素数为标准选手机号，受到广大网友膜拜。其实素数的检测算法是很有趣的，并且会涉及到数论、概率算法等诸多内容，一直觉得素数探测算法是了解概率算法很好的入口。本文和大家简单聊聊如何确定一个数是素数。&lt;/p&gt;
&lt;!--more--&gt;
&lt;h1&gt;素数&lt;/h1&gt;
&lt;h2&gt;素数的定义&lt;/h2&gt;
&lt;p&gt;素数是这样被定义的：&lt;/p&gt;
&lt;p&gt;一个大于1的整数，如果不能被除1和它本身外的其它正整数整除，则是素数（又称质数）。&lt;/p&gt;
&lt;p&gt;与素数相关的定义还有合数：&lt;/p&gt;
&lt;p&gt;一个大于1的整数，如果不是素数则是合数。其中能整除这个数的正整数叫做约数，不等于1也不等于合数本身的约数叫做非平凡约数。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;注意1既不是素数又不是合数。&lt;/p&gt;
&lt;p&gt;举几个例子：&lt;/p&gt;
&lt;p&gt;2是素数，因为除1和2外没有其它正整数可以整除2。&lt;/p&gt;
&lt;p&gt;3也是素数。&lt;/p&gt;
&lt;p&gt;4不是素数，因为2可以整除4。&lt;/p&gt;
&lt;p&gt;11是素数，除1和11外没有正整数可以整除它。&lt;/p&gt;
&lt;p&gt;15不是素数，3和5可以整除15。&lt;/p&gt;
&lt;h2&gt;素数的性质&lt;/h2&gt;
&lt;p&gt;素数有一些有趣的性质，下面不加证明的列几条。&lt;/p&gt;
&lt;p&gt;素数有无穷多个。&lt;/p&gt;
&lt;p&gt;设f(n)为定义在大于1的整数集合上的函数，令f(n)的值为不大于n的素数的个数，则：&lt;/p&gt;
&lt;p&gt;\(\lim_{n \to \infty }\frac{f(n)}{n/\ln{n}}=1\)&lt;/p&gt;
&lt;p&gt;这个函数叫做素数分布函数，反映了素数的分布律。换言之，可以认为大于1的前n个正整数中，素数的个数大约是\(n/\ln{n}\)。&lt;/p&gt;
&lt;h1&gt;检测素数&lt;/h1&gt;
&lt;p&gt;所谓素数检测，就是给定任意一个大于1的整数，判断这个数是否素数。&lt;/p&gt;
&lt;h2&gt;因子检测法&lt;/h2&gt;
&lt;p&gt;最直观的素数检测算法就是因子检测法。说白了，就是从2到n-1一个个拿来试，看能否整除n，如果有能整除的（找到一个因子），则输出不是质数，否则则认为n为质数。当然，实际上不需要试探到n-1，只要到\(\sqrt{n}\)就好了，原因如下：&lt;/p&gt;
&lt;p&gt;设\(n=a\times b\)，且a、b均为n的非平凡约数，显然\(a&gt;\sqrt{n}\)和\(b&gt;\sqrt{n}\)不可能同时成立，因为同时成立时a*b就会大于n，所以，如果n存在非平凡约数，则至少有一个小于等于\(\sqrt{n}\)，因此只要遍历到\(\sqrt{n}\)就可以了。&lt;/p&gt;
&lt;p&gt;因子检测法的实现代码如下（python）：&lt;/p&gt;&lt;pre class=&quot;prettyprint linenums languague-python&quot;&gt;def prime_test_factor(n):
    if n == 1:
        return False
    for i in range(2, 1 + int(floor(sqrt(n)))):
        if n % i == 0:
            return False
    return True&lt;/pre&gt;
&lt;p&gt;做几个测试：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-python&quot;&gt;print prime_test_factor(2) #True
print prime_test_factor(11) #True
print prime_test_factor(15) #False
print prime_test_factor(2147483647) #True&lt;/pre&gt;
&lt;p&gt;很明显，因子检测法的时间复杂度为\(O(\sqrt{n})\)，一般来看，这个时间复杂度已经很不错了，不过对于超级大的数（例如&lt;a href=&quot;http://en.wikipedia.org/wiki/RSA_(algorithm)&quot; target=&quot;_blank&quot;&gt;RSA&lt;/a&gt;加密中找几百位的素数是很正常的），这个复杂度还是太大了。&lt;/p&gt;
&lt;p&gt;例如对于下面的整数：&lt;/p&gt;
&lt;p&gt;6864797660130609714981900799081393217269435300143305409394463459185543183397656052122559640661454554 977296311391480858037121987999716643812574028291115057151&lt;/p&gt;
&lt;p&gt;哪位壮士可以试试用因子检测法检测这个数是质数还是合数，估计这辈子结果是出不来了，下辈子也悬。所以需要更高效的素数检测算法。&lt;/p&gt;
&lt;h2&gt;费马检测&lt;/h2&gt;
&lt;p&gt;坦白说，对于大素数的探测，目前并没有非常有效的确定性算法。不过借助费马定理，可以构造一种有效的概率算法来进行素数探测。&lt;/p&gt;
&lt;h3&gt;费马定理&lt;/h3&gt;
&lt;p&gt;首先看一下什么是费马定理。这条定理是&lt;a href=&quot;http://en.wikipedia.org/wiki/Fermat&quot; target=&quot;_blank&quot;&gt;史上最杰出的业余数学家费马&lt;/a&gt;发现的一条数论中的重要定理， 这条定理可以表述为：&lt;/p&gt;
&lt;p&gt;如果p为素数，则对任何小于p的正整数a有&lt;/p&gt;
&lt;p&gt;\(a^{p-1}\equiv 1(mod\ p)\)&lt;/p&gt;
&lt;p&gt;根据基本数理逻辑，一个命题正确，当且仅当其逆否命题正确。所以费马定理蕴含了这样一个事实：如果某个小于p的正整数不符合上述公式，则p一定不是素数；令人惊讶的是，费马定理的逆命题也“几乎正确”，也就是说如果所有小于p的正整数都符合上述公式，则p“几乎就是一个素数”。当然，“几乎正确”就意味着有出错的可能，这个话题我们后续再来讨论。至少从目前来看，费马定理给我们提供了一条检测素数的方法。&lt;/p&gt;
&lt;p&gt;下面再通过例子说明一下费马定理表达的意义，例如我们知道7是一个素数，则：&lt;/p&gt;
&lt;p&gt;\(\\ 1^6=1\equiv 1(mod\ 7) \\ 2^6=64\equiv 1(mod\ 7) \\ 3^6=729\equiv 1(mod\ 7) \\ 4^6=4096\equiv 1(mod\ 7) \\ 5^6=15625\equiv 1(mod\ 7) \\ 6^6=46656\equiv 1(mod\ 7)\)&lt;/p&gt;
&lt;p&gt;其它素数可以可以用类似方法验证，关于这个定理的严格证明本文不再给出。&lt;/p&gt;
&lt;p&gt;所以可以使用如下方法进行大素数探测：选择一个底数（例如2），对于大整数p，如果2^(p-1)与1不是模p同余数，则p&lt;strong&gt;一定&lt;/strong&gt;不是素数；否则，则p&lt;strong&gt;很可能&lt;/strong&gt;是一个素数。&lt;/p&gt;
&lt;p&gt;至于出现假阳性（即合数被判定为素数）的概率，已有研究表明，随着整数趋向于无穷，这个概率趋向于零，在以2为底的情况下，512位整数碰到假阳性的概率为1/10^20，而在1024位整数中，碰到假阳性的概率为1/10^41。因此如果使用此法检测充分大的数，碰到错误的可能性微乎其微。&lt;/p&gt;
&lt;h3&gt;模幂的快速算法&lt;/h3&gt;
&lt;p&gt;仅有费马定理还不能写检测算法，因为对于大整数p来说，a^(p - 1) (mod p)不是一个容易计算的数字，例如上上面那个超大整数来说，直接计算2的那么多次幂真是要死人了，其效果一点不比因子分解法好。所以寻找一种更有效的取模幂算法。通常来说，重复平方法是一个不错的选择。下面通过例子介绍一下这个方法。&lt;/p&gt;
&lt;p&gt;假设现在要求2的10次方，一种方法当然是将10个2连乘，不过还有这样一种计算方法：&lt;/p&gt;
&lt;p&gt;10的二进制表示是1010，因此：&lt;/p&gt;
&lt;p&gt;\(2^{10}=2^{[1010]_2}\)&lt;/p&gt;
&lt;p&gt;现初始化结果d=2^0=1，我们希望通过乘上某些数变换到2^10，变换序列如下：&lt;/p&gt;
&lt;p&gt;\(\\ 2^{[0]_2} \\ 2^{[0]_2}\times 2^{[0]_2}\times 2=2^{[1]_2} \\ 2^{[1]_2}\times 2^{[1]_2}=2^{[10]_2} \\ 2^{[10]_2}\times 2^{[10]_2}\times 2=2^{[101]_2} \\ 2^{[101]_2}\times 2^{[101]_2}=2^{[1010]_2}\)&lt;/p&gt;
&lt;p&gt;可以看到这样一个规律：对中间结果d自身进行平方，等于在二进制指数的尾部“生出”一个0；对中间结果d自身进行平方再乘以底数，等于在二进制指数尾部“生出”一个1。靠这样不断让指数“生长”，就可以构造出幂。如果在每次运算时取模，就可以得到模幂了，下面是这个算法的python实现：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-python&quot;&gt;def compute_power(a, p, m):
    result = 1
    p_bin = bin(p)[2:]
    length = len(p_bin)
    for i in range(0, length):
        result = result**2 % m
        if p_bin[i] == '1':
            result = result * a % m

    return result&lt;/pre&gt;
&lt;p&gt;这个算法的复杂度正比于a、p和m中位数最多的数的二进制位数，要远远低于朴素的模幂求解法。&lt;/p&gt;
&lt;p&gt;例如，下面的代码在我的机器上瞬间可以完成：&lt;/p&gt;&lt;pre class=&quot;prettyprint linenums languague-python&quot;&gt;compute_power(2, 6864797660130609714981900799081393217269435300143305409394463459185543183397656052122559640661454554977296311391480858037121987999716643812574028291115057150, 6864797660130609714981900799081393217269435300143305409394463459185543183397656052122559640661454554977296311391480858037121987999716643812574028291115057151)&lt;/pre&gt;
&lt;p&gt;而用直观方法计算如此大指数的幂基本是不可能的。&lt;/p&gt;
&lt;h3&gt;费马检测的实现&lt;/h3&gt;
&lt;p&gt;有了上的铺垫，下面可以实现费马检测了：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-python&quot;&gt;def prime_test_fermat(p):
    if p == 1:
        return False
    if p == 2:
        return True   
    d = compute_power(2, p - 1, p)
    if d == 1:
        return True
    return False&lt;/pre&gt;
&lt;p&gt;以下是一些测试：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-python&quot;&gt;print prime_test_fermat(7) #True
print prime_test_fermat(11) #True
print prime_test_fermat(15) #False
print prime_test_fermat(121) #False
print prime_test_fermat(561) #True
print prime_test_fermat(6864797660130609714981900799081393217269435300143305409394463459185543183397656052122559640661454554977296311391480858037121987999716643812574028291115057151) #True&lt;/pre&gt;
&lt;p&gt;需要注意的是，倒数第二个结果实际是错的，因为561可以分解为3和187。&lt;/p&gt;
&lt;p&gt;相对来说，因子分解法适合比较小的数的探测，可以给出准确的结论，但是对于大整数效率不可接受，例如上面最后一个超大整数，因子分解法基本不可行；费马测试当给出否定结论时，是准确的，但是肯定结论有可能是错误的，对于大整数的效率很高，并且误判率随着整数的增大而降低。&lt;/p&gt;
&lt;h2&gt;Miller-Rabin检测&lt;/h2&gt;
&lt;p&gt;上文说，费马检测失误的概率随着整数不断增大而趋向于0，看似是对大素数检测很好的算法。那么我们考虑另外一个问题：如果一个数p是合数，a是小于p的正整数且a不满足费马定理公式，那么a叫做p是合数的一个证据，问题是，对于任意一个合数p，是否总存在证据？&lt;/p&gt;
&lt;p&gt;答案是否定的。例如561这个数，可以分解为3乘以187，但是如果你试过会发现所有小于561的正整数均符合费马定理公式。这就意味着，费马检测对于561是完全失效的。类似561这样是合数但是可以完全欺骗费马检测的数叫做Carmichael数。Carmichael数虽然密度不大（前10亿个正整数中约600个），但是已经被证明有无穷多个。Carmichael数的存在迫使需要一种更强的检测条件配合单纯费马检测使用，其中Miller-Rabin检测是目前应用比较广泛的一种。&lt;/p&gt;
&lt;p&gt;Miller-Rabin检测依赖以下定理：&lt;/p&gt;
&lt;p&gt;如果p是素数，x是小于p的正整数，且x^2 = 1 mod p，则x要么为1，要么为p-1。&lt;/p&gt;
&lt;p&gt;简单证明：如果x^2 = 1 mod p，则p整除x^2 - 1，即整除(x+1)(x-1)，由于p是素数，所以p要么整除x+1，要么整除x-1，前者则x为p-1，后者则x为1。&lt;/p&gt;
&lt;p&gt;以上定理说明，如果对于任意一个小于p的正整数x，发现1（模p）的非平凡平方根存在，则说明p是合数。&lt;/p&gt;
&lt;p&gt;对于p-1，我们总可以将其表示为u2^t，其中u是奇数，t是正整数。此时：&lt;/p&gt;
&lt;p&gt;\(a^{p-1}=a^{u2^t}=(a^u)^{2^t}\)&lt;/p&gt;
&lt;p&gt;也就是可以通过先算出a^u，然后经过连续t次平方计算出a^(p-1)，并且，在任意一次平方时发现了非平凡平方根，则断定p是合数。&lt;/p&gt;
&lt;p&gt;例如，560 = 35 * 2^4，所以可设u=35，t=4：&lt;/p&gt;
&lt;p&gt;\(\\ 2^{35} \mod 561=263 \\ 263^2 \mod 561=166 \\ 166^2 \mod 561=67 \\ 67^2 \mod 561=1\)&lt;/p&gt;
&lt;p&gt;由于找到了一个非平凡平方根67，所以可以断言561是合数。因此2就成为了561是合数的一个证据。&lt;/p&gt;
&lt;p&gt;一般的，Miller-Rabin算法的python实现如下：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-python&quot;&gt;def miller_rabin_witness(a, p):
    if p == 1:
        return False
    if p == 2:
        return True

    n = p - 1
    t = int(floor(log(n, 2)))
    u = 1
    while t &gt; 0:
        u = n / 2**t
        if n % 2**t == 0 and u % 2 == 1:
            break
        t = t - 1

    b1 = b2 = compute_power(a, u, p)
    for i in range(1, t + 1):
        b2 = b1**2 % p
        if b2 == 1 and b1 != 1 and b1 != (p - 1):
            return False
        b1 = b2
    if b1 != 1:
        return False

    return True

def prime_test_miller_rabin(p, k):
    while k &gt; 0:
        a = randint(1, p - 1)
        if not miller_rabin_witness(a, p):
            return False
        k = k - 1
    return True&lt;/pre&gt;
&lt;p&gt;其中miller_rabin_witness用于确认a是否为p为合数的证据，prime_test_miller_rabin共探测k次，每次随机产生一个1至p-1间的整数。只要有一次发现p为合数的证据就认为p为合数，否则认为p为素数。一些测试：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-python&quot;&gt;print prime_test_miller_rabin(7, 5) #True
print prime_test_miller_rabin(21, 5) #False
print prime_test_miller_rabin(561, 50) #False
print prime_test_miller_rabin(6864797660130609714981900799081393217269435300143305409394463459185543183397656052122559640661454554977296311391480858037121987999716643812574028291115057151, 50) #True&lt;/pre&gt;
&lt;p&gt;Miller-Rabin检测也同样存在假阳性的问题，但是与费马检测不同，MR检测的正确概率不依赖被检测数p（排除了Carmichael数失效问题），而仅依赖于检测次数。已经证明，如果一个数p为合数，那么Miller-Rabin检测的证据数量不少于比其小的正整数的3/4，换言之，k次检测后得到错误结果的概率为(1/4)^k，例如上面最后一个大整数，Miller-Rabin检测认为其实素数，我设k为50，也就是说它被误认为素数的概率为(1/4)^50。这个概率有多小呢，小到你不可想象。直观来说，大约等于一个人连续中得5次双色球头奖的概率。&lt;/p&gt;
&lt;h1&gt;参考文献&lt;/h1&gt;
&lt;p&gt;[1] &lt;a href=&quot;http://www.amazon.com/Introduction-To-Algorithms-Thomas-Cormen/dp/0070131430/&quot; target=&quot;_blank&quot;&gt;算法导论&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] &lt;a title=&quot;http://www.wikipedia.org/&quot; href=&quot;http://www.wikipedia.org/&quot; target=&quot;_blank&quot;&gt;http://www.wikipedia.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3] &lt;a title=&quot;http://www.matrix67.com/blog/archives/234&quot; href=&quot;http://www.matrix67.com/blog/archives/234&quot; target=&quot;_blank&quot;&gt;http://www.matrix67.com/blog/archives/234&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4] &lt;a title=&quot;http://www.bigprimes.net/&quot; href=&quot;http://www.bigprimes.net/&quot; target=&quot;_blank&quot;&gt;http://www.bigprimes.net/&lt;/a&gt;&lt;/p&gt;
</description>
</item>
<item>
<title>浅析PageRank算法</title>
<link>http://blog.codinglabs.org/articles/intro-to-pagerank.html</link>
<guid>http://blog.codinglabs.org/articles/intro-to-pagerank.html</guid>
<pubDate>Sun, 01 Jul 2012 16:00:00 GMT</pubDate>
<description>&lt;p&gt;很早就对Google的PageRank算法很感兴趣，但一直没有深究，只有个轮廓性的概念。前几天趁团队outing的机会，在动车上看了一些相关的资料（PS：在动车上看看书真是一种享受），趁热打铁，将所看的东西整理成此文。&lt;/p&gt;
&lt;p&gt;本文首先会讨论搜索引擎的核心难题，同时讨论早期搜索引擎关于结果页面重要性评价算法的困境，借此引出PageRank产生的背景。第二部分会详细讨论PageRank的思想来源、基础框架，并结合互联网页面拓扑结构讨论PageRank处理Dead Ends及平滑化的方法。第三部分讨论Topic-Sensitive PageRank算法。最后将讨论对PageRank的Spam攻击方法：Spam Farm以及搜索引擎对Spam Farm的防御。
&lt;!--more--&gt;
&lt;/p&gt;
&lt;h1&gt;搜索引擎的难题&lt;/h1&gt;
&lt;p&gt;Google早已成为全球最成功的互联网搜索引擎，但这个当前的搜索引擎巨无霸却不是最早的互联网搜索引擎，在Google出现之前，曾出现过许多通用或专业领域搜索引擎。Google最终能击败所有竞争对手，很大程度上是因为它解决了困扰前辈们的最大难题：对搜索结果按重要性排序。而解决这个问题的算法就是PageRank。毫不夸张的说，是PageRank算法成就了Google今天的低位。要理解为什么解决这个难题如此重要，我们先来看一下搜索引擎的核心框架。&lt;/p&gt;
&lt;h2&gt;搜索引擎的核心框架&lt;/h2&gt;
&lt;p&gt;虽然搜索引擎已经发展了很多年，但是其核心却没有太大变化。从本质上说，搜索引擎是一个资料检索系统，搜索引擎拥有一个资料库（具体到这里就是互联网页面），用户提交一个检索条件（例如关键词），搜索引擎返回符合查询条件的资料列表。理论上检索条件可以非常复杂，为了简单起见，我们不妨设检索条件是一至多个以空格分隔的词，而其表达的语义是同时含有这些词的资料（等价于布尔代数的逻辑与）。例如，提交“张洋 博客”，意思就是“给我既含有‘张洋’又含有‘博客’词语的页面”，以下是Google对这条关键词的搜索结果：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/intro-to-pagerank/1.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;可以看到我的博客出现在第五条，而第四条是我之前在博客园的博客。&lt;/p&gt;
&lt;p&gt;当然，实际上现在的搜索引擎都是有分词机制的，例如如果以“张洋的博客”为关键词，搜索引擎会自动将其分解为“张洋 的 博客”三个词，而“的”作为&lt;a href=&quot;http://en.wikipedia.org/wiki/Stop_word&quot; target=&quot;_blank&quot;&gt;停止词&lt;/a&gt;（Stop Word）会被过滤掉。关于分词及词权评价算法（如&lt;a href=&quot;http://en.wikipedia.org/wiki/Tf*idf&quot; target=&quot;_blank&quot;&gt;TF-IDF算法&lt;/a&gt;）是一个很大的话题，这里就不展开讨论了，为了简单此处可以将搜索引擎想象为一个只会机械匹配词语的检索系统。&lt;/p&gt;
&lt;p&gt;这样看来，建立一个搜索引擎的核心问题就是两个：1、建立资料库；2、建立一种数据结构，可以根据关键词找到含有这个词的页面。&lt;/p&gt;
&lt;p&gt;第一个问题一般是通过一种叫&lt;a href=&quot;http://en.wikipedia.org/wiki/Web_crawler&quot; target=&quot;_blank&quot;&gt;爬虫&lt;/a&gt;（Spider）的特殊程序实现的（当然，专业领域搜索引擎例如某个学术会议的论文检索系统可能直接从数据库建立资料库），简单来说，爬虫就是从一个页面出发（例如新浪首页），通过HTTP协议通信获取这个页面的所有内容，把这个页面url和内容记录下来（记录到资料库），然后分析页面中的链接，再去分别获取这些链接链向页面的内容，记录到资料库后再分析这个页面的链接……重复这个过程，就可以将整个互联网的页面全部获取下来（当然这是理想情况，要求整个Web是一个强连通（&lt;a href=&quot;http://en.wikipedia.org/wiki/Strongly_connected&quot; target=&quot;_blank&quot;&gt;Strongly Connected&lt;/a&gt;），并且所有页面的&lt;a href=&quot;http://www.robotstxt.org/&quot; target=&quot;_blank&quot;&gt;robots协议&lt;/a&gt;允许爬虫抓取页面，为了简单，我们仍然假设Web是一个强连通图，且不考虑robots协议）。抽象来看，可以将资料库看做一个巨大的key-value结构，key是页面url，value是页面内容。&lt;/p&gt;
&lt;p&gt;第二个问题是通过一种叫倒排索引（&lt;a href=&quot;http://en.wikipedia.org/wiki/Inverted_index&quot; target=&quot;_blank&quot;&gt;inverted index&lt;/a&gt;）的数据结构实现的，抽象来说倒排索引也是一组key-value结构，key是关键词，value是一个页面编号集合（假设资料库中每个页面有唯一编号），表示这些页面含有这个关键词。本文不详细讨论倒排索引的建立方法。&lt;/p&gt;
&lt;p&gt;有了上面的分析，就可以简要说明搜索引擎的核心动作了：搜索引擎获取“张洋 博客”查询条件，将其分为“张洋”和“博客”两个词。然后分别从倒排索引中找到“张洋”所对应的集合，假设是{1， 3， 6， 8， 11， 15}；“博客”对应的集合是{1， 6， 10， 11， 12， 17， 20， 22}，将两个集合做交运算（intersection），结果是{1， 6， 11}。最后，从资料库中找出1、6、11对应的页面返回给用户就可以了。&lt;/p&gt;
&lt;h2&gt;搜索引擎的核心难题&lt;/h2&gt;
&lt;p&gt;上面阐述了一个非常简单的搜索引擎工作框架，虽然现代搜索引擎的具体细节原理要复杂的多，但其本质却与这个简单的模型并无二异。实际Google在上述两点上相比其前辈并无高明之处。其最大的成功是解决了第三个、也是最为困难的问题：如何对查询结果排序。&lt;/p&gt;
&lt;p&gt;我们知道Web页面数量非常巨大，所以一个检索的结果条目数量也非常多，例如上面“张洋 博客”的检索返回了超过260万条结果。用户不可能从如此众多的结果中一一查找对自己有用的信息，所以，一个好的搜索引擎必须想办法将“质量”较高的页面排在前面。其实直观上也可以感觉出，在使用搜索引擎时，我们并不太关心页面是否够全（上百万的结果，全不全有什么区别？而且实际上搜索引擎都是取top，并不会真的返回全部结果。），而很关心前一两页是否都是质量较高的页面，是否能满足我们的实际需求。&lt;/p&gt;
&lt;p&gt;因此，对搜索结果按重要性合理的排序就成为搜索引擎的最大核心，也是Google最终成功的突破点。&lt;/p&gt;
&lt;h2&gt;早期搜索引擎的做法&lt;/h2&gt;
&lt;h3&gt;不评价&lt;/h3&gt;
&lt;p&gt;这个看起来可能有点搞笑，但实际上早期很多搜索引擎（甚至包括现在的很多专业领域搜索引擎）根本不评价结果重要性，而是直接按照某自然顺序（例如时间顺序或编号顺序）返回结果。这在结果集比较少的情况下还说得过去，但是一旦结果集变大，用户叫苦不迭，试想让你从几万条质量参差不齐的页面中寻找需要的内容，简直就是一场灾难，这也注定这种方法不可能用于现代的通用搜索引擎。&lt;/p&gt;
&lt;h3&gt;基于检索词的评价&lt;/h3&gt;
&lt;p&gt;后来，一些搜索引擎引入了基于检索关键词去评价搜索结构重要性的方法，实际上，这类方法如TF-IDF算法在现代搜索引擎中仍在使用，但其已经不是评价质量的唯一指标。完整描述TF-IDF比较繁琐，本文这里用一种更简单的抽象模型描述这种方法。&lt;/p&gt;
&lt;p&gt;基于检索词评价的思想非常朴素：和检索词匹配度越高的页面重要性越高。“匹配度”就是要定义的具体度量。一个最直接的想法是关键词出现次数越多的页面匹配度越高。还是搜索“张洋 博客”的例子：假设A页面出现“张洋”5次，“博客”10次；B页面出现“张洋”2次，“博客”8次。于是A页面的匹配度为5 + 10 = 15，B页面为2 + 8 = 10，于是认为A页面的重要性高于B页面。很多朋友可能意识到这里的不合理性：内容较长的网页往往更可能比内容较短的网页关键词出现的次数多。因此，我们可以修改一下算法，用关键词出现次数除以页面总词数，也就是通过关键词占比作为匹配度，这样可以克服上面提到的不合理。&lt;/p&gt;
&lt;p&gt;早期一些搜索引擎确实是基于类似的算法评价网页重要性的。这种评价算法看似依据充分、实现直观简单，但却非常容易受到一种叫“Term Spam”的攻击。&lt;/p&gt;
&lt;h3&gt;Term Spam&lt;/h3&gt;
&lt;p&gt;其实从搜索引擎出现的那天起，spammer和搜索引擎反作弊的斗法就没有停止过。Spammer是这样一群人——试图通过搜索引擎算法的漏洞来提高目标页面（通常是一些广告页面或垃圾页面）的重要性，使目标页面在搜索结果中排名靠前。&lt;/p&gt;
&lt;p&gt;现在假设Google单纯使用关键词占比评价页面重要性，而我想让我的博客在搜索结果中排名更靠前（最好排第一）。那么我可以这么做：在页面中加入一个隐藏的html元素（例如一个div），然后其内容是“张洋”重复一万次。这样，搜索引擎在计算“张洋 博客”的搜索结果时，我的博客关键词占比就会非常大，从而做到排名靠前的效果。更进一步，我甚至可以干扰别的关键词搜索结果，例如我知道现在欧洲杯很火热，我就在我博客的隐藏div里加一万个“欧洲杯”，当有用户搜索欧洲杯时，我的博客就能出现在搜索结果较靠前的位置。这种行为就叫做“Term Spam”。&lt;/p&gt;
&lt;p&gt;早期搜索引擎深受这种作弊方法的困扰，加之基于关键词的评价算法本身也不甚合理，因此经常是搜出一堆质量低下的结果，用户体验大大打了折扣。而Google正是在这种背景下，提出了PageRank算法，并申请了专利保护。此举充分保护了当时相对弱小Google，也使得Google一举成为全球首屈一指的搜索引擎。&lt;/p&gt;
&lt;h1&gt;PageRank算法&lt;/h1&gt;
&lt;p&gt;上文已经说到，PageRank的作用是评价网页的重要性，以此作为搜索结果的排序重要依据之一。实际中，为了抵御spam，各个搜索引擎的具体排名算法是保密的，PageRank的具体计算方法也不尽相同，本节介绍一种最简单的基于页面链接属性的PageRank算法。这个算法虽然简单，却能揭示PageRank的本质，实际上目前各大搜索引擎在计算PageRank时链接属性确实是重要度量指标之一。&lt;/p&gt;
&lt;h2&gt;简单PageRank计算&lt;/h2&gt;
&lt;p&gt;首先，我们将Web做如下抽象：1、将每个网页抽象成一个节点；2、如果一个页面A有链接直接链向B，则存在一条有向边从A到B（多个相同链接不重复计算边）。因此，整个Web被抽象为一张有向图。&lt;/p&gt;
&lt;p&gt;现在假设世界上只有四张网页：A、B、C、D，其抽象结构如下图：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/intro-to-pagerank/2.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;显然这个图是强连通的（从任一节点出发都可以到达另外任何一个节点）。&lt;/p&gt;
&lt;p&gt;然后需要用一种合适的数据结构表示页面间的连接关系。其实，PageRank算法是基于这样一种背景思想：被用户访问越多的网页更可能质量越高，而用户在浏览网页时主要通过超链接进行页面跳转，因此我们需要通过分析超链接组成的拓扑结构来推算每个网页被访问频率的高低。最简单的，我们可以假设当一个用户停留在某页面时，跳转到页面上每个被链页面的概率是相同的。例如，上图中A页面链向B、C、D，所以一个用户从A跳转到B、C、D的概率各为1/3。设一共有N个网页，则可以组织这样一个N维矩阵：其中i行j列的值表示用户从页面j转到页面i的概率。这样一个矩阵叫做转移矩阵（Transition Matrix）。下面的转移矩阵M对应上图：&lt;/p&gt;
&lt;p&gt;\(\large M=\begin{bmatrix} 0 &amp; 1/2 &amp; 0 &amp; 1/2\\ 1/3 &amp; 0 &amp; 0 &amp; 1/2\\ 1/3 &amp; 1/2 &amp; 0 &amp; 0\\ 1/3 &amp; 0 &amp; 1 &amp; 0 \end{bmatrix}\)&lt;/p&gt;
&lt;p&gt;然后，设初始时每个页面的rank值为1/N，这里就是1/4。按A-D顺序将页面rank为向量v：&lt;/p&gt;
&lt;p&gt;\(\large v=\begin{bmatrix} 1/4\\ 1/4\\ 1/4\\ 1/4 \end{bmatrix}\)&lt;/p&gt;
&lt;p&gt;注意，M第一行分别是A、B、C和D转移到页面A的概率，而v的第一列分别是A、B、C和D当前的rank，因此用M的第一行乘以v的第一列，所得结果就是页面A最新rank的合理估计，同理，Mv的结果就分别代表A、B、C、D新rank：&lt;/p&gt;
&lt;p&gt;\(\large Mv=\begin{bmatrix} 1/4\\ 5/24\\ 5/24\\ 1/3 \end{bmatrix}\)&lt;/p&gt;
&lt;p&gt;然后用M再乘以这个新的rank向量，又会产生一个更新的rank向量。迭代这个过程，可以证明v最终会收敛，即v约等于Mv，此时计算停止。最终的v就是各个页面的pagerank值。例如上面的向量经过几步迭代后，大约收敛在（1/4, 1/4, 1/5, 1/4），这就是A、B、C、D最后的pagerank。&lt;/p&gt;
&lt;h2&gt;处理Dead Ends&lt;/h2&gt;
&lt;p&gt;上面的PageRank计算方法假设Web是强连通的，但实际上，Web并不是强连通（甚至不是联通的）。下面看看PageRank算法如何处理一种叫做Dead Ends的情况。&lt;/p&gt;
&lt;p&gt;所谓Dead Ends，就是这样一类节点：它们不存在外链。看下面的图：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/intro-to-pagerank/3.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;注意这里D页面不存在外链，是一个Dead End。上面的算法之所以能成功收敛到非零值，很大程度依赖转移矩阵这样一个性质：每列的加和为1。而在这个图中，M第四列将全为0。在没有Dead Ends的情况下，每次迭代后向量v各项的和始终保持为1，而有了Dead Ends，迭代结果将最终归零（要解释为什么会这样，需要一些矩阵论的知识，比较枯燥，此处略）。&lt;/p&gt;
&lt;p&gt;处理Dead Ends的方法如下：迭代拿掉图中的Dead Ends节点及Dead Ends节点相关的边（之所以迭代拿掉是因为当目前的Dead Ends被拿掉后，可能会出现一批新的Dead Ends），直到图中没有Dead Ends。对剩下部分计算rank，然后以拿掉Dead Ends逆向顺序反推Dead Ends的rank。&lt;/p&gt;
&lt;p&gt;以上图为例，首先拿到D和D相关的边，D被拿到后，C就变成了一个新的Dead Ends，于是拿掉C，最终只剩A、B。此时可很容易算出A、B的PageRank均为1/2。然后我们需要反推Dead Ends的rank，最后被拿掉的是C，可以看到C前置节点有A和B，而A和B的出度分别为3和2，因此C的rank为：1/2 * 1/3 + 1/2 * 1/2 = 5/12；最后，D的rank为：1/2 * 1/3 + 5/12 * 1 = 7/12。所以最终的PageRank为（1/2, 1/2, 5/12, 7/12）。&lt;/p&gt;
&lt;h2&gt;Spider Traps及平滑处理&lt;/h2&gt;
&lt;p&gt;可以预见，如果把真实的Web组织成转移矩阵，那么这将是一个极为稀疏的矩阵，从矩阵论知识可以推断，极度稀疏的转移矩阵迭代相乘可能会使得向量v变得非常不平滑，即一些节点拥有很大的rank，而大多数节点rank值接近0。而一种叫做Spider Traps节点的存在加剧了这种不平滑。例如下图：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/intro-to-pagerank/4.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;D有外链所以不是Dead Ends，但是它只链向自己（注意链向自己也算外链，当然同时也是个内链）。这种节点叫做Spider Trap，如果对这个图进行计算，会发现D的rank越来越大趋近于1，而其它节点rank值几乎归零。&lt;/p&gt;
&lt;p&gt;为了克服这种由于矩阵稀疏性和Spider Traps带来的问题，需要对PageRank计算方法进行一个平滑处理，具体做法是加入“心灵转移（teleporting）”。所谓心灵转移，就是我们认为在任何一个页面浏览的用户都有可能以一个极小的概率瞬间转移到另外一个随机页面。当然，这两个页面可能不存在超链接，因此不可能真的直接转移过去，心灵转移只是为了算法需要而强加的一种纯数学意义的概率数字。&lt;/p&gt;
&lt;p&gt;加入心灵转移后，向量迭代公式变为：&lt;/p&gt;
&lt;p&gt;\(\large {v}'=(1-\beta)Mv+e\frac{\beta}{N}\)&lt;/p&gt;
&lt;p&gt;其中β往往被设置为一个比较小的参数（0.2或更小），e为N维单位向量，加入e的原因是这个公式的前半部分是向量，因此必须将β/N转为向量才能相加。这样，整个计算就变得平滑，因为每次迭代的结果除了依赖转移矩阵外，还依赖一个小概率的心灵转移。&lt;/p&gt;
&lt;p&gt;以上图为例，转移矩阵M为：&lt;/p&gt;
&lt;p&gt;\(\large M=\begin{bmatrix} 0 &amp; 1/2 &amp; 0 &amp; 0\\ 1/3 &amp; 0 &amp; 0 &amp; 0\\ 1/3 &amp; 1/2 &amp; 0 &amp; 0\\ 1/3 &amp; 0 &amp; 1 &amp; 1 \end{bmatrix}\)&lt;/p&gt;
&lt;p&gt;设β为0.2，则加权后的M为：&lt;/p&gt;
&lt;p&gt;\(\large M=\begin{bmatrix} 0 &amp; 2/5 &amp; 0 &amp; 0\\ 4/15 &amp; 0 &amp; 0 &amp; 0\\ 4/15 &amp; 2/5 &amp; 0 &amp; 0\\ 4/15 &amp; 0 &amp; 4/5 &amp; 4/5 \end{bmatrix}\)&lt;/p&gt;
&lt;p&gt;因此：&lt;/p&gt;
&lt;p&gt;\(\large {v}'=\begin{bmatrix} 0 &amp; 2/5 &amp; 0 &amp; 0\\ 4/15 &amp; 0 &amp; 0 &amp; 0\\ 4/15 &amp; 2/5 &amp; 0 &amp; 0\\ 4/15 &amp; 0 &amp; 4/5 &amp; 4/5 \end{bmatrix}v+\begin{bmatrix} 1/20\\ 1/20\\ 1/20\\ 1/20 \end{bmatrix}\)&lt;/p&gt;
&lt;p&gt;如果按这个公式迭代算下去，会发现Spider Traps的效应被抑制了，从而每个页面都拥有一个合理的pagerank。&lt;/p&gt;
&lt;h2&gt;Topic-Sensitive PageRank&lt;/h2&gt;
&lt;p&gt;其实上面的讨论我们回避了一个事实，那就是“网页重要性”其实没一个标准答案，对于不同的用户，甚至有很大的差别。例如，当搜索“苹果”时，一个数码爱好者可能是想要看iphone的信息，一个果农可能是想看苹果的价格走势和种植技巧，而一个小朋友可能在找苹果的简笔画。理想情况下，应该为每个用户维护一套专用向量，但面对海量用户这种方法显然不可行。所以搜索引擎一般会选择一种称为Topic-Sensitive的折中方案。Topic-Sensitive PageRank的做法是预定义几个话题类别，例如体育、娱乐、科技等等，为每个话题单独维护一个向量，然后想办法关联用户的话题倾向，根据用户的话题倾向排序结果。&lt;/p&gt;
&lt;p&gt;Topic-Sensitive PageRank分为以下几步：&lt;/p&gt;
&lt;p&gt;1、确定话题分类。&lt;/p&gt;
&lt;p&gt;一般来说，可以参考&lt;a href=&quot;http://www.dmoz.org&quot; target=&quot;_blank&quot;&gt;Open Directory（DMOZ）&lt;/a&gt;的一级话题类别作为topic。目前DMOZ的一级topic有：Arts（艺术）、Business（商务）、Computers（计算机）、Games（游戏）、Health（医疗健康）、Home（居家）、Kids and Teens（儿童）、News（新闻）、Recreation（娱乐修养）、Reference（参考）、Regional（地域）、Science（科技）、Shopping（购物）、Society（人文社会）、Sports（体育）。&lt;/p&gt;
&lt;p&gt;2、网页topic归属。&lt;/p&gt;
&lt;p&gt;这一步需要将每个页面归入最合适的分类，具体归类有很多算法，例如可以使用TF-IDF基于词素归类，也可以聚类后人工归类，具体不再展开。这一步最终的结果是每个网页被归到其中一个topic。&lt;/p&gt;
&lt;p&gt;3、分topic向量计算。&lt;/p&gt;
&lt;p&gt;在Topic-Sensitive PageRank中，向量迭代公式为&lt;/p&gt;
&lt;p&gt;\(\large {v}'=(1-\beta)Mv+s\frac{\beta}{|s|}\)&lt;/p&gt;
&lt;p&gt;首先是单位向量e变为了s。s是这样一个向量：对于某topic的s，如果网页k在此topic中，则s中第k个元素为1，否则为0。注意对于每一个topic都有一个不同的s。而|s|表示s中1的数量。&lt;/p&gt;
&lt;p&gt;还是以上面的四张页面为例，假设页面A归为Arts，B归为Computers，C归为Computers，D归为Sports。那么对于Computers这个topic，s就是：&lt;/p&gt;
&lt;p&gt;\(\large s=\begin{bmatrix} 0\\ 1\\ 1\\ 0 \end{bmatrix}\)&lt;/p&gt;
&lt;p&gt;而|s|=2。因此，迭代公式为：&lt;/p&gt;
&lt;p&gt;\(\large {v}'=\begin{bmatrix} 0 &amp; 2/5 &amp; 0 &amp; 0\\ 4/15 &amp; 0 &amp; 0 &amp; 0\\ 4/15 &amp; 2/5 &amp; 0 &amp; 0\\ 4/15 &amp; 0 &amp; 4/5 &amp; 4/5 \end{bmatrix}v+\begin{bmatrix} 0\\ 1/10\\ 1/10\\ 0 \end{bmatrix}\)&lt;/p&gt;
&lt;p&gt;最后算出的向量就是Computers这个topic的rank。如果实际计算一下，会发现B、C页在这个topic下的权重相比上面非Topic-Sensitive的rank会升高，这说明如果用户是一个倾向于Computers topic的人（例如程序员），那么在给他呈现的结果中B、C会更重要，因此可能排名更靠前。&lt;/p&gt;
&lt;p&gt;4、确定用户topic倾向。&lt;/p&gt;
&lt;p&gt;最后一步就是在用户提交搜索时，确定用户的topic倾向，以选择合适的rank向量。主要方法有两种，一种是列出所有topic让用户自己选择感兴趣的项目，这种方法在一些社交问答网站注册时经常使用；另外一种方法就是通过某种手段（如cookie跟踪）跟踪用户的行为，进行数据分析判断用户的倾向，这本身也是一个很有意思的话题，按时这个话题超出本文的范畴，不再展开细说。&lt;/p&gt;
&lt;h1&gt;针对PageRank的Spam攻击与反作弊&lt;/h1&gt;
&lt;p&gt;上文说过，Spammer和搜索引擎反作弊工程师的斗法从来就没停止过。实际上，只要是算法，就一定有spam方法，不存在无懈可击的排名算法。下面看一下针对PageRank的spam。&lt;/p&gt;
&lt;h2&gt;Link Spam&lt;/h2&gt;
&lt;p&gt;回到文章开头的例子，如果我想让我的博客在搜索“张洋 博客”时排名靠前，显然在PageRank算法下靠Term Spam是无法实现的。不过既然我明白了PageRank主要靠内链数计算页面权重，那么我是不是可以考虑建立很多空架子网站，让这些网站都链接到我博客首页，这样是不是可以提高我博客首页的PageRank？很不幸，这种方法行不通。再看下PageRank算法，一个页面会将权重均匀散播给被链接网站，所以除了内链数外，上游页面的权重也很重要。而我那些空架子网站本身就没啥权重，所以来自它们的内链并不能起到提高我博客首页PageRank的作用，这样只是自娱自乐而已。&lt;/p&gt;
&lt;p&gt;所以，Spam PageRank的关键就在于想办法增加一些高权重页面的内链。下面具体看一下Link Spam怎么做。&lt;/p&gt;
&lt;p&gt;首先明确将页面分为几个类型：&lt;/p&gt;
&lt;p&gt;1、目标页&lt;/p&gt;
&lt;p&gt;目标页是spammer要提高rank的页面，这里就是我的博客首页。&lt;/p&gt;
&lt;p&gt;2、支持页&lt;/p&gt;
&lt;p&gt;支持页是spammer能完全控制的页面，例如spammer自己建立的站点中页面，这里就是我上文所谓的空架子页面。&lt;/p&gt;
&lt;p&gt;3、可达页&lt;/p&gt;
&lt;p&gt;可达页是spammer无法完全控制，但是可以有接口供spammer发布链接的页面，例如天涯社区、新浪博客等等这种用户可发帖的社区或博客站。&lt;/p&gt;
&lt;p&gt;4、不可达页&lt;/p&gt;
&lt;p&gt;这是那些spammer完全无法发布链接的网站，例如政府网站、百度首页等等。&lt;/p&gt;
&lt;p&gt;作为一个spammer，我能利用的资源就是支持页和可达页。上面说过，单纯通过支持页是没有办法spam的，因此我要做的第一件事情就是尽量找一些rank较高的可达页去加上对我博客首页的链接。例如我可以去天涯、猫扑等地方回个这样的贴：“楼主的帖子很不错！精彩内容：&lt;a href=&quot;http://codinglabs.org&quot; target=&quot;_blank&quot;&gt;http://codinglabs.org&lt;/a&gt;”。我想大家一定在各大社区没少见这种帖子，这就是有人在做spam。&lt;/p&gt;
&lt;p&gt;然后，再通过大量的支持页放大rank，具体做法是让每个支持页和目标页互链，且每个支持页只有一条链接。&lt;/p&gt;
&lt;p&gt;这样一个结构叫做Spam Farm，其拓扑图如下：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/intro-to-pagerank/5.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;其中T是目标页，A是可达页，S是支持页。下面计算一下link spam的效果。&lt;/p&gt;
&lt;p&gt;设T的总rank为y，则y由三部分组成：&lt;/p&gt;
&lt;p&gt;1、可达页的rank贡献，设为x。&lt;/p&gt;
&lt;p&gt;2、心灵转移的贡献，为β/n。其中n为全部网页的数量，β为转移参数。&lt;/p&gt;
&lt;p&gt;3、支持页的贡献：&lt;/p&gt;
&lt;p&gt;设有m个支持页，因为每个支持页只和T有链接，所以可以算出每个支持页的rank为：&lt;/p&gt;
&lt;p&gt;\(\large \frac{(1-\beta)y}{m}+\frac{\beta}{n}\)&lt;/p&gt;
&lt;p&gt;则支持页贡献的全部rank为：&lt;/p&gt;
&lt;p&gt;\(\large m(1-\beta)(\frac{(1-\beta)y}{m}+\frac{\beta}{n})\)&lt;/p&gt;
&lt;p&gt;因此可以得到：&lt;/p&gt;
&lt;p&gt;\(\large y=m(1-\beta)(\frac{(1-\beta)y}{m}+\frac{\beta}{n})+x+\frac{\beta}{n}\)&lt;/p&gt;
&lt;p&gt;由于相对β，n非常巨大，所以可以认为β/n近似于0。 简化后的方程为：&lt;/p&gt;
&lt;p&gt;\(\large y=m(1-\beta)(\frac{(1-\beta)y}{m})+x\)&lt;/p&gt;
&lt;p&gt;解方程得：&lt;/p&gt;
&lt;p&gt;\(\large y=x\frac{1}{2\beta-\beta^2}\)&lt;/p&gt;
&lt;p&gt;假设β为0.2，则1/(2β-β^2) = 2.77则这个spam farm可以将x约放大2.7倍。因此如果起到不错的spam效果。&lt;/p&gt;
&lt;h2&gt;Link Spam反作弊&lt;/h2&gt;
&lt;p&gt;针对spammer的link spam行为，搜索引擎的反作弊工程师需要想办法检测这种行为，一般来说有两类方法检测link spam。&lt;/p&gt;
&lt;h3&gt;网络拓扑分析&lt;/h3&gt;
&lt;p&gt;一种方法是通过对网页的图拓扑结构分析找出可能存在的spam farm。但是随着Web规模越来越大，这种方法非常困难，因为图的特定结构查找是时间复杂度非常高的一个算法，不可能完全靠这种方法反作弊。&lt;/p&gt;
&lt;h3&gt;TrustRank&lt;/h3&gt;
&lt;p&gt;更可能的一种反作弊方法是叫做一种TrustRank的方法。&lt;/p&gt;
&lt;p&gt;说起来TrustRank其实数学本质上就是Topic-Sensitive Rank，只不过这里定义了一个“可信网页”的虚拟topic。所谓可信网页就是上文说到的不可达页，或者说没法spam的页面。例如政府网站（被黑了的不算）、新浪、网易门户首页等等。一般是通过人力或者其它什么方式选择出一个“可信网页”集合，组成一个topic，然后通过上文的Topic-Sensitive算法对这个topic进行rank计算，结果叫做TrustRank。&lt;/p&gt;
&lt;p&gt;TrustRank的思想很直观：如果一个页面的普通rank远高于可信网页的topic rank，则很可能这个页面被spam了。&lt;/p&gt;
&lt;p&gt;设一个页面普通rank为P，TrustRank为T，则定义网页的Spam Mass为：(P – T)/P。&lt;/p&gt;
&lt;p&gt;Spam Mass越大，说明此页面为spam目标页的可能性越大。&lt;/p&gt;
&lt;h1&gt;总结&lt;/h1&gt;
&lt;p&gt;这篇文章是我对一些资料的归纳汇总，简单介绍了PageRank的背景、作用、计算方法、变种、Spam及反作弊等内容。为了突出重点我简化了搜索引擎的模型，当然在实际中搜索引擎远没有这么简单，真实算法也一定非常复杂。不过目前几乎所有现代搜索引擎页面权重的计算方法都基于PageRank及其变种。因为我没做过搜索引擎相关的开发，因此本文内容主要是基于现有文献的客观总结，稍加一点我的理解。&lt;/p&gt;
&lt;p&gt;文中的图使用PGF/TikZ for Tex绘制：&lt;a title=&quot;http://www.texample.net/tikz/&quot; href=&quot;http://www.texample.net/tikz/&quot; target=&quot;_blank&quot;&gt;http://www.texample.net/tikz/&lt;/a&gt;。&lt;/p&gt;
&lt;h1&gt;参考文献&lt;/h1&gt;
&lt;p&gt;[1] Anand Rajaraman, Jeffrey D. Ullman, Mining of Massive Datasets. 2010-2011&lt;/p&gt;
&lt;p&gt;[2] S. Brin and L. Page, “Anatomy of a large-scale hypertextual web search engine,” Proc. 7th Intl. World-Wide-Web Conference, pp. 107–117, 1998.&lt;/p&gt;
&lt;p&gt;[3] A. Broder, R. Kumar, F. Maghoul, P. Raghavan, S. Rajagopalan, R. Stata, A. Tomkins, and J. Weiner, “Graph structure in the web,” Computer Networks 33:1–6, pp. 309–320, 2000.&lt;/p&gt;
&lt;p&gt;[4] T.H. Haveliwala, “Topic-sensitive PageRank,” Proc. 11th Intl. World-Wide-Web Conference, pp. 517–526, 2002&lt;/p&gt;
&lt;p&gt;[5] Z. Gy¨ongi, H. Garcia-Molina, and J. Pedersen, “Combating link spam with trustrank,” Proc. 30th Intl. Conf. on Very Large Databases, pp. 576–587, 2004.&lt;/p&gt;
</description>
</item>
<item>
<title>PHP哈希表碰撞攻击原理</title>
<link>http://blog.codinglabs.org/articles/hash-collisions-attack-on-php.html</link>
<guid>http://blog.codinglabs.org/articles/hash-collisions-attack-on-php.html</guid>
<pubDate>Tue, 03 Jan 2012 16:00:00 GMT</pubDate>
<description>&lt;p&gt;最近哈希表碰撞攻击（Hashtable collisions as DOS attack）的话题不断被提起，各种语言纷纷中招。本文结合PHP内核源码，聊一聊这种攻击的原理及实现。&lt;/p&gt;
&lt;!--more--&gt;
&lt;h1&gt;哈希表碰撞攻击的基本原理&lt;/h1&gt;
&lt;p&gt;哈希表是一种查找效率极高的数据结构，很多语言都在内部实现了哈希表。PHP中的哈希表是一种极为重要的数据结构，不但用于表示Array数据类型，还在Zend虚拟机内部用于存储上下文环境信息（执行上下文的变量及函数均使用哈希表结构存储）。&lt;/p&gt;
&lt;p&gt;理想情况下哈希表插入和查找操作的时间复杂度均为O(1)，任何一个数据项可以在一个与哈希表长度无关的时间内计算出一个哈希值（key），然后在常量时间内定位到一个桶（术语bucket，表示哈希表中的一个位置）。当然这是理想情况下，因为任何哈希表的长度都是有限的，所以一定存在不同的数据项具有相同哈希值的情况，此时不同数据项被定为到同一个桶，称为碰撞（collision）。哈希表的实现需要解决碰撞问题，碰撞解决大体有两种思路，第一种是根据某种原则将被碰撞数据定为到其它桶，例如线性探测——如果数据在插入时发生了碰撞，则顺序查找这个桶后面的桶，将其放入第一个没有被使用的桶；第二种策略是每个桶不是一个只能容纳单个数据项的位置，而是一个可容纳多个数据的数据结构（例如链表或红黑树），所有碰撞的数据以某种数据结构的形式组织起来。&lt;/p&gt;
&lt;p&gt;不论使用了哪种碰撞解决策略，都导致插入和查找操作的时间复杂度不再是O(1)。以查找为例，不能通过key定位到桶就结束，必须还要比较原始key（即未做哈希之前的key）是否相等，如果不相等，则要使用与插入相同的算法继续查找，直到找到匹配的值或确认数据不在哈希表中。&lt;/p&gt;
&lt;p&gt;PHP是使用单链表存储碰撞的数据，因此实际上PHP哈希表的平均查找复杂度为O(L)，其中L为桶链表的平均长度；而最坏复杂度为O(N)，此时所有数据全部碰撞，哈希表退化成单链表。下图PHP中正常哈希表和退化哈希表的示意图。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/hash-collisions-attack-on-php/1.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;哈希表碰撞攻击就是通过精心构造数据，使得所有数据全部碰撞，人为将哈希表变成一个退化的单链表，此时哈希表各种操作的时间均提升了一个数量级，因此会消耗大量CPU资源，导致系统无法快速响应请求，从而达到拒绝服务攻击（DoS）的目的。&lt;/p&gt;
&lt;p&gt;可以看到，进行哈希碰撞攻击的前提是哈希算法特别容易找出碰撞，如果是MD5或者SHA1那基本就没戏了，幸运的是（也可以说不幸的是）大多数编程语言使用的哈希算法都十分简单（这是为了效率考虑），因此可以不费吹灰之力之力构造出攻击数据。下一节将通过分析Zend相关内核代码，找出攻击哈希表碰撞攻击PHP的方法。&lt;/p&gt;
&lt;h1&gt;Zend哈希表的内部实现&lt;/h1&gt;
&lt;h2&gt;数据结构&lt;/h2&gt;
&lt;p&gt;PHP中使用一个叫Backet的结构体表示桶，同一哈希值的所有桶被组织为一个单链表。哈希表使用HashTable结构体表示。相关源码在zend/Zend_hash.h下：&lt;/p&gt;&lt;pre class=&quot;prettyprint linenums languague-c&quot;&gt;typedef struct bucket {
    ulong h;                        /* Used for numeric indexing */
    uint nKeyLength;
    void *pData;
    void *pDataPtr;
    struct bucket *pListNext;
    struct bucket *pListLast;
    struct bucket *pNext;
    struct bucket *pLast;
    char arKey[1]; /* Must be last element */
} Bucket;

typedef struct _hashtable {
    uint nTableSize;
    uint nTableMask;
    uint nNumOfElements;
    ulong nNextFreeElement;
    Bucket *pInternalPointer;   /* Used for element traversal */
    Bucket *pListHead;
    Bucket *pListTail;
    Bucket **arBuckets;
    dtor_func_t pDestructor;
    zend_bool persistent;
    unsigned char nApplyCount;
    zend_bool bApplyProtection;
#if ZEND_DEBUG
    int inconsistent;
#endif
} HashTable;&lt;/pre&gt;
&lt;p&gt;字段名很清楚的表明其用途，因此不做过多解释。重点明确下面几个字段：Bucket中的“h”用于存储原始key；HashTable中的nTableMask是一个掩码，一般被设为nTableSize - 1，与哈希算法有密切关系，后面讨论哈希算法时会详述；arBuckets指向一个指针数组，其中每个元素是一个指向Bucket链表的头指针。&lt;/p&gt;
&lt;h2&gt;哈希算法&lt;/h2&gt;
&lt;p&gt;PHP哈希表最小容量是8（2^3），最大容量是0x80000000（2^31），并向2的整数次幂圆整（即长度会自动扩展为2的整数次幂，如13个元素的哈希表长度为16；100个元素的哈希表长度为128）。nTableMask被初始化为哈希表长度（圆整后）减1。具体代码在zend/Zend_hash.c的_zend_hash_init函数中，这里截取与本文相关的部分并加上少量注释。&lt;/p&gt;&lt;pre class=&quot;prettyprint linenums languague-c&quot;&gt;ZEND_API int _zend_hash_init(HashTable *ht, uint nSize, hash_func_t pHashFunction, dtor_func_t pDestructor, zend_bool persistent ZEND_FILE_LINE_DC)
{
    uint i = 3;
    Bucket **tmp;

    SET_INCONSISTENT(HT_OK);

    //长度向2的整数次幂圆整
    if (nSize &gt;= 0x80000000) {
        /* prevent overflow */
        ht-&gt;nTableSize = 0x80000000;
    } else {
        while ((1U &lt;&lt; i) &lt; nSize) {
            i++;
        }
        ht-&gt;nTableSize = 1 &lt;&lt; i;
    }

    ht-&gt;nTableMask = ht-&gt;nTableSize - 1;

    /*此处省略若干代码…*/

    return SUCCESS;
}&lt;/pre&gt;
&lt;p&gt;值得一提的是PHP向2的整数次幂取圆整方法非常巧妙，可以背下来在需要的时候使用。&lt;/p&gt;
&lt;p&gt;Zend HashTable的哈希算法异常简单：&lt;/p&gt;
&lt;p&gt;hash(key)=key &amp; nTableMask&lt;/p&gt;
&lt;p&gt;即简单将数据的原始key与HashTable的nTableMask进行按位与即可。&lt;/p&gt;
&lt;p&gt;如果原始key为字符串，则首先使用&lt;a href=&quot;http://blog.csdn.net/chen_alvin/article/details/5846714&quot; target=&quot;_blank&quot;&gt;Times33&lt;/a&gt;算法将字符串转为整形再与nTableMask按位与。&lt;/p&gt;
&lt;p&gt;hash(strkey)=time33(strkey) &amp; nTableMask&lt;/p&gt;
&lt;p&gt;下面是Zend源码中查找哈希表的代码：&lt;/p&gt;&lt;pre  class=&quot;prettyprint linenums languague-c&quot;&gt;ZEND_API int zend_hash_index_find(const HashTable *ht, ulong h, void **pData)
{
    uint nIndex;
    Bucket *p;

    IS_CONSISTENT(ht);

    nIndex = h &amp; ht-&gt;nTableMask;

    p = ht-&gt;arBuckets[nIndex];
    while (p != NULL) {
        if ((p-&gt;h == h) &amp;&amp; (p-&gt;nKeyLength == 0)) {
            *pData = p-&gt;pData;
            return SUCCESS;
        }
        p = p-&gt;pNext;
    }
    return FAILURE;
}

ZEND_API int zend_hash_find(const HashTable *ht, const char *arKey, uint nKeyLength, void **pData)
{
    ulong h;
    uint nIndex;
    Bucket *p;

    IS_CONSISTENT(ht);

    h = zend_inline_hash_func(arKey, nKeyLength);
    nIndex = h &amp; ht-&gt;nTableMask;

    p = ht-&gt;arBuckets[nIndex];
    while (p != NULL) {
        if ((p-&gt;h == h) &amp;&amp; (p-&gt;nKeyLength == nKeyLength)) {
            if (!memcmp(p-&gt;arKey, arKey, nKeyLength)) {
                *pData = p-&gt;pData;
                return SUCCESS;
            }
        }
        p = p-&gt;pNext;
    }
    return FAILURE;
}&lt;/pre&gt;
&lt;p&gt;其中zend_hash_index_find用于查找整数key的情况，zend_hash_find用于查找字符串key。逻辑基本一致，只是字符串key会通过zend_inline_hash_func转为整数key，zend_inline_hash_func封装了times33算法，具体代码就不贴出了。&lt;/p&gt;
&lt;h1&gt;攻击&lt;/h1&gt;
&lt;h2&gt;基本攻击&lt;/h2&gt;
&lt;p&gt;知道了PHP内部哈希表的算法，就可以利用其原理构造用于攻击的数据。一种最简单的方法是利用掩码规律制造碰撞。上文提到Zend HashTable的长度nTableSize会被圆整为2的整数次幂，假设我们构造一个2^16的哈希表，则nTableSize的二进制表示为：1 0000 0000 0000 0000，而nTableMask = nTableSize – 1为：0 1111 1111 1111 1111。接下来，可以以0为初始值，以2^16为步长，制造足够多的数据，可以得到如下推测：&lt;/p&gt;
&lt;p&gt;0000 0000 0000 0000 0000 &amp; 0 1111 1111 1111 1111 = 0&lt;/p&gt;
&lt;p&gt;0001 0000 0000 0000 0000 &amp; 0 1111 1111 1111 1111 = 0&lt;/p&gt;
&lt;p&gt;0010 0000 0000 0000 0000 &amp; 0 1111 1111 1111 1111 = 0&lt;/p&gt;
&lt;p&gt;0011 0000 0000 0000 0000 &amp; 0 1111 1111 1111 1111 = 0&lt;/p&gt;
&lt;p&gt;0100 0000 0000 0000 0000 &amp; 0 1111 1111 1111 1111 = 0&lt;/p&gt;
&lt;p&gt;……&lt;/p&gt;
&lt;p&gt;概况来说只要保证后16位均为0，则与掩码位于后得到的哈希值全部碰撞在位置0。&lt;/p&gt;
&lt;p&gt;下面是利用这个原理写的一段攻击代码：&lt;/p&gt;&lt;pre  class=&quot;prettyprint linenums languague-php&quot;&gt;&lt;?php

$size = pow(2, 16);

$startTime = microtime(true);

$array = array();
for ($key = 0, $maxKey = ($size - 1) * $size; $key &lt;= $maxKey; $key += $size) {
    $array[$key] = 0;
}

$endTime = microtime(true);

echo $endTime - $startTime, ' seconds', &quot;\n&quot;;&lt;/pre&gt;
&lt;p&gt;这段代码在我的VPS上（单CPU，512M内存）上用了近88秒才完成，并且在此期间CPU资源几乎被用尽：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/hash-collisions-attack-on-php/2.png&quot;/&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/hash-collisions-attack-on-php/3.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;而普通的同样大小的哈希表插入仅用时0.036秒：&lt;/p&gt;&lt;pre  class=&quot;prettyprint linenums languague-php&quot;&gt;&lt;?php

$size = pow(2, 16);

$startTime = microtime(true);

$array = array();
for ($key = 0, $maxKey = ($size - 1) * $size; $key &lt;= $size; $key += 1) {
    $array[$key] = 0;
}

$endTime = microtime(true);

echo $endTime - $startTime, ' seconds', &quot;\n&quot;;&lt;/pre&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/hash-collisions-attack-on-php/4.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;可以证明第二段代码插入N个元素的时间在O(N)水平，而第一段攻击代码则需O(N^2)的时间去插入N个元素。&lt;/p&gt;
&lt;h2&gt;POST攻击&lt;/h2&gt;
&lt;p&gt;当然，一般情况下很难遇到攻击者可以直接修改PHP代码的情况，但是攻击者仍可以通过一些方法间接构造哈希表来进行攻击。例如PHP会将接收到的HTTP POST请求中的数据构造为$_POST，而这是一个Array，内部就是通过Zend HashTable表示，因此攻击者只要构造一个含有大量碰撞key的post请求，就可以达到攻击的目的。具体做法不再演示。&lt;/p&gt;
&lt;h1&gt;防护&lt;/h1&gt;
&lt;h2&gt;POST攻击的防护&lt;/h2&gt;
&lt;p&gt;针对POST方式的哈希碰撞攻击，目前PHP的防护措施是控制POST数据的数量。在&gt;=PHP5.3.9的版本中增加了一个配置项max_input_vars，用于标识一次http请求最大接收的参数个数，默认为1000。因此PHP5.3.x的用户可以通过升级至5.3.9来避免哈希碰撞攻击。5.2.x的用户可以使用这个patch：&lt;a href=&quot;http://www.laruence.com/2011/12/30/2440.html&quot;&gt;http://www.laruence.com/2011/12/30/2440.html&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;另外的防护方法是在Web服务器层面进行处理，例如限制http请求body的大小和参数的数量等，这个是现在用的最多的临时处理方案。具体做法与不同Web服务器相关，不再详述。&lt;/p&gt;
&lt;h2&gt;其它防护&lt;/h2&gt;
&lt;p&gt;上面的防护方法只是限制POST数据的数量，而不能彻底解决这个问题。例如，如果某个POST字段是一个json数据类型，会被PHP &lt;a href=&quot;http://cn.php.net/manual/en/function.json-decode.php&quot; target=&quot;_blank&quot;&gt;json_decode&lt;/a&gt;，那么只要构造一个超大的json攻击数据照样可以达到攻击目的。理论上，只要PHP代码中某处构造Array的数据依赖于外部输入，则都可能造成这个问题，因此彻底的解决方案要从Zend底层HashTable的实现动手。一般来说有两种方式，一是限制每个桶链表的最长长度；二是使用其它数据结构如&lt;a href=&quot;http://en.wikipedia.org/wiki/Red%E2%80%93black_tree&quot; target=&quot;_blank&quot;&gt;红黑树&lt;/a&gt;取代链表组织碰撞哈希（并不解决哈希碰撞，只是减轻攻击影响，将N个数据的操作时间从O(N^2)降至O(NlogN)，代价是普通情况下接近O(1)的操作均变为O(logN)）。&lt;/p&gt;
&lt;p&gt;目前使用最多的仍然是POST数据攻击，因此建议生产环境的PHP均进行升级或打补丁。至于从数据结构层面修复这个问题，目前还没有任何方面的消息。&lt;/p&gt;
&lt;h1&gt;参考&lt;/h1&gt;
&lt;p&gt;[1] &lt;a href=&quot;http://nikic.github.com/2011/12/28/Supercolliding-a-PHP-array.html&quot; target=&quot;_blank&quot;&gt;Supercolliding a PHP array&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] &lt;a href=&quot;http://www.laruence.com/2011/12/30/2440.html&quot; target=&quot;_blank&quot;&gt;PHP5.2.*防止Hash冲突拒绝服务攻击的Patch&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3] &lt;a href=&quot;http://www.laruence.com/2011/12/29/2412.html&quot; target=&quot;_blank&quot;&gt;通过构造Hash冲突实现各种语言的拒绝服务攻击&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4] &lt;a href=&quot;http://www.laruence.com/2011/12/30/2435.html&quot; target=&quot;_blank&quot;&gt;PHP数组的Hash冲突实例&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[5] &lt;a href=&quot;http://www.php.net/archive/2011.php&quot; target=&quot;_blank&quot;&gt;PHP 5.4.0 RC4 released&lt;/a&gt;&lt;/p&gt;
</description>
</item>
<item>
<title>MySQL索引背后的数据结构及算法原理</title>
<link>http://blog.codinglabs.org/articles/theory-of-mysql-index.html</link>
<guid>http://blog.codinglabs.org/articles/theory-of-mysql-index.html</guid>
<pubDate>Mon, 17 Oct 2011 16:00:00 GMT</pubDate>
<description>&lt;h1&gt;&lt;a name=&quot;nav-1&quot;&gt;&lt;/a&gt;摘要&lt;/h1&gt;
&lt;p&gt;本文以MySQL数据库为研究对象，讨论与数据库索引相关的一些话题。特别需要说明的是，MySQL支持诸多存储引擎，而各种存储引擎对索引的支持也各不相同，因此MySQL数据库支持多种索引类型，如BTree索引，哈希索引，全文索引等等。为了避免混乱，本文将只关注于BTree索引，因为这是平常使用MySQL时主要打交道的索引，至于哈希索引和全文索引本文暂不讨论。&lt;/p&gt;
&lt;p&gt;文章主要内容分为三个部分。&lt;/p&gt;
&lt;p&gt;第一部分主要从数据结构及算法理论层面讨论MySQL数据库索引的数理基础。&lt;/p&gt;
&lt;p&gt;第二部分结合MySQL数据库中MyISAM和InnoDB数据存储引擎中索引的架构实现讨论聚集索引、非聚集索引及覆盖索引等话题。&lt;/p&gt;
&lt;p&gt;第三部分根据上面的理论基础，讨论MySQL中高性能使用索引的策略。
&lt;!--more--&gt;
&lt;/p&gt;
&lt;h1&gt;&lt;a name=&quot;nav-2&quot;&gt;&lt;/a&gt;数据结构及算法基础&lt;/h1&gt;
&lt;h2&gt;&lt;a name=&quot;nav-2-1&quot;&gt;&lt;/a&gt;索引的本质&lt;/h2&gt;
&lt;p&gt;MySQL官方对索引的定义为：索引（Index）是帮助MySQL高效获取数据的数据结构。提取句子主干，就可以得到索引的本质：索引是数据结构。&lt;/p&gt;
&lt;p&gt;我们知道，数据库查询是数据库的最主要功能之一。我们都希望查询数据的速度能尽可能的快，因此数据库系统的设计者会从查询算法的角度进行优化。最基本的查询算法当然是&lt;a href=&quot;http://en.wikipedia.org/wiki/Linear_search&quot; target=&quot;_blank&quot;&gt;顺序查找&lt;/a&gt;（linear search），这种复杂度为O(n)的算法在数据量很大时显然是糟糕的，好在计算机科学的发展提供了很多更优秀的查找算法，例如&lt;a href=&quot;http://en.wikipedia.org/wiki/Binary_search_algorithm&quot; target=&quot;_blank&quot;&gt;二分查找&lt;/a&gt;（binary search）、&lt;a href=&quot;http://en.wikipedia.org/wiki/Binary_search_tree&quot; target=&quot;_blank&quot;&gt;二叉树查找&lt;/a&gt;（binary tree search）等。如果稍微分析一下会发现，每种查找算法都只能应用于特定的数据结构之上，例如二分查找要求被检索数据有序，而二叉树查找只能应用于&lt;a href=&quot;http://en.wikipedia.org/wiki/Binary_search_tree&quot; target=&quot;_blank&quot;&gt;二叉查找树&lt;/a&gt;上，但是数据本身的组织结构不可能完全满足各种数据结构（例如，理论上不可能同时将两列都按顺序进行组织），所以，在数据之外，数据库系统还维护着满足特定查找算法的数据结构，这些数据结构以某种方式引用（指向）数据，这样就可以在这些数据结构上实现高级查找算法。这种数据结构，就是索引。&lt;/p&gt;
&lt;p&gt;看一个例子：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/theory-of-mysql-index/1.png&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;图1&lt;/p&gt;
&lt;p&gt;图1展示了一种可能的索引方式。左边是数据表，一共有两列七条记录，最左边的是数据记录的物理地址（注意逻辑上相邻的记录在磁盘上也并不是一定物理相邻的）。为了加快Col2的查找，可以维护一个右边所示的二叉查找树，每个节点分别包含索引键值和一个指向对应数据记录物理地址的指针，这样就可以运用二叉查找在\(O(log_2n)\)的复杂度内获取到相应数据。&lt;/p&gt;
&lt;p&gt;虽然这是一个货真价实的索引，但是实际的数据库系统几乎没有使用二叉查找树或其进化品种&lt;a href=&quot;http://en.wikipedia.org/wiki/Red-black_tree&quot; target=&quot;_blank&quot;&gt;红黑树&lt;/a&gt;（red-black tree）实现的，原因会在下文介绍。&lt;/p&gt;
&lt;h2&gt;&lt;a name=&quot;nav-2-2&quot;&gt;&lt;/a&gt;B-Tree和B+Tree&lt;/h2&gt;
&lt;p&gt;目前大部分数据库系统及文件系统都采用B-Tree或其变种B+Tree作为索引结构，在本文的下一节会结合存储器原理及计算机存取原理讨论为什么B-Tree和B+Tree在被如此广泛用于索引，这一节先单纯从数据结构角度描述它们。&lt;/p&gt;
&lt;h3&gt;B-Tree&lt;/h3&gt;
&lt;p&gt;为了描述B-Tree，首先定义一条数据记录为一个二元组[key, data]，key为记录的键值，对于不同数据记录，key是互不相同的；data为数据记录除key外的数据。那么B-Tree是满足下列条件的数据结构：&lt;/p&gt;
&lt;p&gt;d为大于1的一个正整数，称为B-Tree的度。&lt;/p&gt;
&lt;p&gt;h为一个正整数，称为B-Tree的高度。&lt;/p&gt;
&lt;p&gt;每个非叶子节点由n-1个key和n个指针组成，其中d&lt;=n&lt;=2d。&lt;/p&gt;
&lt;p&gt;每个叶子节点最少包含一个key和两个指针，最多包含2d-1个key和2d个指针，叶节点的指针均为null 。&lt;/p&gt;
&lt;p&gt;所有叶节点具有相同的深度，等于树高h。&lt;/p&gt;
&lt;p&gt;key和指针互相间隔，节点两端是指针。&lt;/p&gt;
&lt;p&gt;一个节点中的key从左到右非递减排列。&lt;/p&gt;
&lt;p&gt;所有节点组成树结构。&lt;/p&gt;
&lt;p&gt;每个指针要么为null，要么指向另外一个节点。&lt;/p&gt;
&lt;p&gt;如果某个指针在节点node最左边且不为null，则其指向节点的所有key小于\(v(key_1)\)，其中\(v(key_1)\)为node的第一个key的值。&lt;/p&gt;
&lt;p&gt;如果某个指针在节点node最右边且不为null，则其指向节点的所有key大于\(v(key_m)\)，其中\(v(key_m)\)为node的最后一个key的值。&lt;/p&gt;
&lt;p&gt;如果某个指针在节点node的左右相邻key分别是\(key_i\)和\(key_{i+1}\)且不为null，则其指向节点的所有key小于\(v(key_{i+1})\)且大于\(v(key_i)\)。&lt;/p&gt;
&lt;p&gt;图2是一个d=2的B-Tree示意图。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/theory-of-mysql-index/2.png&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;图2&lt;/p&gt;
&lt;p&gt;由于B-Tree的特性，在B-Tree中按key检索数据的算法非常直观：首先从根节点进行二分查找，如果找到则返回对应节点的data，否则对相应区间的指针指向的节点递归进行查找，直到找到节点或找到null指针，前者查找成功，后者查找失败。B-Tree上查找算法的伪代码如下：&lt;/p&gt;
&lt;pre class=&quot;prettyprint languague-c&quot;&gt;BTree_Search(node, key) {
    if(node == null) return null;
    foreach(node.key)
    {
        if(node.key[i] == key) return node.data[i];
            if(node.key[i] &gt; key) return BTree_Search(point[i]-&gt;node);
    }
    return BTree_Search(point[i+1]-&gt;node);
}
data = BTree_Search(root, my_key);&lt;/pre&gt;
关于B-Tree有一系列有趣的性质，例如一个度为d的B-Tree，设其索引N个key，则其树高h的上限为\(log_d((N+1)/2)\)，检索一个key，其查找节点个数的渐进复杂度为\(O(log_dN)\)。从这点可以看出，B-Tree是一个非常有效率的索引数据结构。&lt;/p&gt;
&lt;p&gt;另外，由于插入删除新的数据记录会破坏B-Tree的性质，因此在插入删除时，需要对树进行一个分裂、合并、转移等操作以保持B-Tree性质，本文不打算完整讨论B-Tree这些内容，因为已经有许多资料详细说明了B-Tree的数学性质及插入删除算法，有兴趣的朋友可以在本文末的参考文献一栏找到相应的资料进行阅读。&lt;/p&gt;
&lt;h3&gt;B+Tree&lt;/h3&gt;
&lt;p&gt;B-Tree有许多变种，其中最常见的是B+Tree，例如MySQL就普遍使用B+Tree实现其索引结构。&lt;/p&gt;
&lt;p&gt;与B-Tree相比，B+Tree有以下不同点：&lt;/p&gt;
&lt;p&gt;每个节点的指针上限为2d而不是2d+1。&lt;/p&gt;
&lt;p&gt;内节点不存储data，只存储key；叶子节点不存储指针。&lt;/p&gt;
&lt;p&gt;图3是一个简单的B+Tree示意。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/theory-of-mysql-index/3.png&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;图3&lt;/p&gt;
&lt;p&gt;由于并不是所有节点都具有相同的域，因此B+Tree中叶节点和内节点一般大小不同。这点与B-Tree不同，虽然B-Tree中不同节点存放的key和指针可能数量不一致，但是每个节点的域和上限是一致的，所以在实现中B-Tree往往对每个节点申请同等大小的空间。&lt;/p&gt;
&lt;p&gt;一般来说，B+Tree比B-Tree更适合实现外存储索引结构，具体原因与外存储器原理及计算机存取原理有关，将在下面讨论。&lt;/p&gt;
&lt;h3&gt;带有顺序访问指针的B+Tree&lt;/h3&gt;
&lt;p&gt;一般在数据库系统或文件系统中使用的B+Tree结构都在经典B+Tree的基础上进行了优化，增加了顺序访问指针。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/theory-of-mysql-index/4.png&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;图4&lt;/p&gt;
&lt;p&gt;如图4所示，在B+Tree的每个叶子节点增加一个指向相邻叶子节点的指针，就形成了带有顺序访问指针的B+Tree。做这个优化的目的是为了提高区间访问的性能，例如图4中如果要查询key为从18到49的所有数据记录，当找到18后，只需顺着节点和指针顺序遍历就可以一次性访问到所有数据节点，极大提到了区间查询效率。&lt;/p&gt;
&lt;p&gt;这一节对B-Tree和B+Tree进行了一个简单的介绍，下一节结合存储器存取原理介绍为什么目前B+Tree是数据库系统实现索引的首选数据结构。&lt;/p&gt;
&lt;h2&gt;&lt;a name=&quot;nav-2-3&quot;&gt;&lt;/a&gt;为什么使用B-Tree（B+Tree）&lt;/h2&gt;
&lt;p&gt;上文说过，红黑树等数据结构也可以用来实现索引，但是文件系统及数据库系统普遍采用B-/+Tree作为索引结构，这一节将结合计算机组成原理相关知识讨论B-/+Tree作为索引的理论基础。&lt;/p&gt;
&lt;p&gt;一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘I/O消耗，相对于内存存取，I/O存取的消耗要高几个数量级，所以评价一个数据结构作为索引的优劣最重要的指标就是在查找过程中磁盘I/O操作次数的渐进复杂度。换句话说，索引的结构组织要尽量减少查找过程中磁盘I/O的存取次数。下面先介绍内存和磁盘存取原理，然后再结合这些原理分析B-/+Tree作为索引的效率。&lt;/p&gt;
&lt;h3&gt;主存存取原理&lt;/h3&gt;
&lt;p&gt;目前计算机使用的主存基本都是随机读写存储器（RAM），现代RAM的结构和存取原理比较复杂，这里本文抛却具体差别，抽象出一个十分简单的存取模型来说明RAM的工作原理。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/theory-of-mysql-index/5.png&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;图5&lt;/p&gt;
&lt;p&gt;从抽象角度看，主存是一系列的存储单元组成的矩阵，每个存储单元存储固定大小的数据。每个存储单元有唯一的地址，现代主存的编址规则比较复杂，这里将其简化成一个二维地址：通过一个行地址和一个列地址可以唯一定位到一个存储单元。图5展示了一个4 x 4的主存模型。&lt;/p&gt;
&lt;p&gt;主存的存取过程如下：&lt;/p&gt;
&lt;p&gt;当系统需要读取主存时，则将地址信号放到地址总线上传给主存，主存读到地址信号后，解析信号并定位到指定存储单元，然后将此存储单元数据放到数据总线上，供其它部件读取。&lt;/p&gt;
&lt;p&gt;写主存的过程类似，系统将要写入单元地址和数据分别放在地址总线和数据总线上，主存读取两个总线的内容，做相应的写操作。&lt;/p&gt;
&lt;p&gt;这里可以看出，主存存取的时间仅与存取次数呈线性关系，因为不存在机械操作，两次存取的数据的“距离”不会对时间有任何影响，例如，先取A0再取A1和先取A0再取D3的时间消耗是一样的。&lt;/p&gt;
&lt;h3&gt;磁盘存取原理&lt;/h3&gt;
&lt;p&gt;上文说过，索引一般以文件形式存储在磁盘上，索引检索需要磁盘I/O操作。与主存不同，磁盘I/O存在机械运动耗费，因此磁盘I/O的时间消耗是巨大的。&lt;/p&gt;
&lt;p&gt;图6是磁盘的整体结构示意图。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/theory-of-mysql-index/6.png&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;图6&lt;/p&gt;
&lt;p&gt;一个磁盘由大小相同且同轴的圆形盘片组成，磁盘可以转动（各个磁盘必须同步转动）。在磁盘的一侧有磁头支架，磁头支架固定了一组磁头，每个磁头负责存取一个磁盘的内容。磁头不能转动，但是可以沿磁盘半径方向运动（实际是斜切向运动），每个磁头同一时刻也必须是同轴的，即从正上方向下看，所有磁头任何时候都是重叠的（不过目前已经有多磁头独立技术，可不受此限制）。&lt;/p&gt;
&lt;p&gt;图7是磁盘结构的示意图。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/theory-of-mysql-index/7.png&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;图7&lt;/p&gt;
&lt;p&gt;盘片被划分成一系列同心环，圆心是盘片中心，每个同心环叫做一个磁道，所有半径相同的磁道组成一个柱面。磁道被沿半径线划分成一个个小的段，每个段叫做一个扇区，每个扇区是磁盘的最小存储单元。为了简单起见，我们下面假设磁盘只有一个盘片和一个磁头。&lt;/p&gt;
&lt;p&gt;当需要从磁盘读取数据时，系统会将数据逻辑地址传给磁盘，磁盘的控制电路按照寻址逻辑将逻辑地址翻译成物理地址，即确定要读的数据在哪个磁道，哪个扇区。为了读取这个扇区的数据，需要将磁头放到这个扇区上方，为了实现这一点，磁头需要移动对准相应磁道，这个过程叫做寻道，所耗费时间叫做寻道时间，然后磁盘旋转将目标扇区旋转到磁头下，这个过程耗费的时间叫做旋转时间。&lt;/p&gt;
&lt;h3&gt;局部性原理与磁盘预读&lt;/h3&gt;
&lt;p&gt;由于存储介质的特性，磁盘本身存取就比主存慢很多，再加上机械运动耗费，磁盘的存取速度往往是主存的几百分分之一，因此为了提高效率，要尽量减少磁盘I/O。为了达到这个目的，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存。这样做的理论依据是计算机科学中著名的局部性原理：&lt;/p&gt;
&lt;p&gt;当一个数据被用到时，其附近的数据也通常会马上被使用。&lt;/p&gt;
&lt;p&gt;程序运行期间所需要的数据通常比较集中。&lt;/p&gt;
&lt;p&gt;由于磁盘顺序读取的效率很高（不需要寻道时间，只需很少的旋转时间），因此对于具有局部性的程序来说，预读可以提高I/O效率。&lt;/p&gt;
&lt;p&gt;预读的长度一般为页（page）的整倍数。页是计算机管理存储器的逻辑块，硬件及操作系统往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（在许多操作系统中，页得大小通常为4k），主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后异常返回，程序继续运行。&lt;/p&gt;
&lt;h3&gt;B-/+Tree索引的性能分析&lt;/h3&gt;
&lt;p&gt;到这里终于可以分析B-/+Tree索引的性能了。&lt;/p&gt;
&lt;p&gt;上文说过一般使用磁盘I/O次数评价索引结构的优劣。先从B-Tree分析，根据B-Tree的定义，可知检索一次最多需要访问h个节点。数据库系统的设计者巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入。为了达到这个目的，在实际实现B-Tree还需要使用如下技巧：&lt;/p&gt;
&lt;p&gt;每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了一个node只需一次I/O。&lt;/p&gt;
&lt;p&gt;B-Tree中一次检索最多需要h-1次I/O（根节点常驻内存），渐进复杂度为\(O(h)=O(log_dN)\)。一般实际应用中，出度d是非常大的数字，通常超过100，因此h非常小（通常不超过3）。&lt;/p&gt;
&lt;p&gt;综上所述，用B-Tree作为索引结构效率是非常高的。&lt;/p&gt;
&lt;p&gt;而红黑树这种结构，h明显要深的多。由于逻辑上很近的节点（父子）物理上可能很远，无法利用局部性，所以红黑树的I/O渐进复杂度也为O(h)，效率明显比B-Tree差很多。&lt;/p&gt;
&lt;p&gt;上文还说过，B+Tree更适合外存索引，原因和内节点出度d有关。从上面分析可以看到，d越大索引的性能越好，而出度的上限取决于节点内key和data的大小：&lt;/p&gt;
&lt;p&gt;\(d_{max}=floor(pagesize / (keysize + datasize + pointsize))\)&lt;/p&gt;
&lt;p&gt;floor表示向下取整。由于B+Tree内节点去掉了data域，因此可以拥有更大的出度，拥有更好的性能。&lt;/p&gt;
&lt;p&gt;这一章从理论角度讨论了与索引相关的数据结构与算法问题，下一章将讨论B+Tree是如何具体实现为MySQL中索引，同时将结合MyISAM和InnDB存储引擎介绍非聚集索引和聚集索引两种不同的索引实现形式。&lt;/p&gt;
&lt;h1&gt;&lt;a name=&quot;nav-3&quot;&gt;&lt;/a&gt;MySQL索引实现&lt;/h1&gt;
&lt;p&gt;在MySQL中，索引属于存储引擎级别的概念，不同存储引擎对索引的实现方式是不同的，本文主要讨论MyISAM和InnoDB两个存储引擎的索引实现方式。&lt;/p&gt;
&lt;h2&gt;&lt;a name=&quot;nav-3-1&quot;&gt;&lt;/a&gt;MyISAM索引实现&lt;/h2&gt;
&lt;p&gt;MyISAM引擎使用B+Tree作为索引结构，叶节点的data域存放的是数据记录的地址。下图是MyISAM索引的原理图：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/theory-of-mysql-index/8.png&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;图8&lt;/p&gt;
&lt;p&gt;这里设表一共有三列，假设我们以Col1为主键，则图8是一个MyISAM表的主索引（Primary key）示意。可以看出MyISAM的索引文件仅仅保存数据记录的地址。在MyISAM中，主索引和辅助索引（Secondary key）在结构上没有任何区别，只是主索引要求key是唯一的，而辅助索引的key可以重复。如果我们在Col2上建立一个辅助索引，则此索引的结构如下图所示：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/theory-of-mysql-index/9.png&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;图9&lt;/p&gt;
&lt;p&gt;同样也是一颗B+Tree，data域保存数据记录的地址。因此，MyISAM中索引检索的算法为首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其data域的值，然后以data域的值为地址，读取相应数据记录。&lt;/p&gt;
&lt;p&gt;MyISAM的索引方式也叫做“非聚集”的，之所以这么称呼是为了与InnoDB的聚集索引区分。&lt;/p&gt;
&lt;h2&gt;&lt;a name=&quot;nav-3-2&quot;&gt;&lt;/a&gt;InnoDB索引实现&lt;/h2&gt;
&lt;p&gt;虽然InnoDB也使用B+Tree作为索引结构，但具体实现方式却与MyISAM截然不同。&lt;/p&gt;
&lt;p&gt;第一个重大区别是InnoDB的数据文件本身就是索引文件。从上文知道，MyISAM索引文件和数据文件是分离的，索引文件仅保存数据记录的地址。而在InnoDB中，表数据文件本身就是按B+Tree组织的一个索引结构，这棵树的叶节点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/theory-of-mysql-index/10.png&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;图10&lt;/p&gt;
&lt;p&gt;图10是InnoDB主索引（同时也是数据文件）的示意图，可以看到叶节点包含了完整的数据记录。这种索引叫做聚集索引。因为InnoDB的数据文件本身要按主键聚集，所以InnoDB要求表必须有主键（MyISAM可以没有），如果没有显式指定，则MySQL系统会自动选择一个可以唯一标识数据记录的列作为主键，如果不存在这种列，则MySQL自动为InnoDB表生成一个隐含字段作为主键，这个字段长度为6个字节，类型为长整形。&lt;/p&gt;
&lt;p&gt;第二个与MyISAM索引的不同是InnoDB的辅助索引data域存储相应记录主键的值而不是地址。换句话说，InnoDB的所有辅助索引都引用主键作为data域。例如，图11为定义在Col3上的一个辅助索引：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/theory-of-mysql-index/11.png&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;图11&lt;/p&gt;
&lt;p&gt;这里以英文字符的ASCII码作为比较准则。聚集索引这种实现方式使得按主键的搜索十分高效，但是辅助索引搜索需要检索两遍索引：首先检索辅助索引获得主键，然后用主键到主索引中检索获得记录。&lt;/p&gt;
&lt;p&gt;了解不同存储引擎的索引实现方式对于正确使用和优化索引都非常有帮助，例如知道了InnoDB的索引实现后，就很容易明白为什么不建议使用过长的字段作为主键，因为所有辅助索引都引用主索引，过长的主索引会令辅助索引变得过大。再例如，用非单调的字段作为主键在InnoDB中不是个好主意，因为InnoDB数据文件本身是一颗B+Tree，非单调的主键会造成在插入新记录时数据文件为了维持B+Tree的特性而频繁的分裂调整，十分低效，而使用自增字段作为主键则是一个很好的选择。&lt;/p&gt;
&lt;p&gt;下一章将具体讨论这些与索引有关的优化策略。&lt;/p&gt;
&lt;h1&gt;&lt;a name=&quot;nav-4&quot;&gt;&lt;/a&gt;索引使用策略及优化&lt;/h1&gt;
&lt;p&gt;MySQL的优化主要分为结构优化（Scheme optimization）和查询优化（Query optimization）。本章讨论的高性能索引策略主要属于结构优化范畴。本章的内容完全基于上文的理论基础，实际上一旦理解了索引背后的机制，那么选择高性能的策略就变成了纯粹的推理，并且可以理解这些策略背后的逻辑。&lt;/p&gt;
&lt;h2&gt;&lt;a name=&quot;nav-4-1&quot;&gt;&lt;/a&gt;示例数据库&lt;/h2&gt;
&lt;p&gt;为了讨论索引策略，需要一个数据量不算小的数据库作为示例。本文选用MySQL官方文档中提供的示例数据库之一：employees。这个数据库关系复杂度适中，且数据量较大。下图是这个数据库的E-R关系图（引用自MySQL官方手册）：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/theory-of-mysql-index/12.png&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;图12&lt;/p&gt;
&lt;p&gt;MySQL官方文档中关于此数据库的页面为&lt;a title=&quot;http://dev.mysql.com/doc/employee/en/employee.html&quot; href=&quot;http://dev.mysql.com/doc/employee/en/employee.html&quot;&gt;http://dev.mysql.com/doc/employee/en/employee.html&lt;/a&gt;。里面详细介绍了此数据库，并提供了下载地址和导入方法，如果有兴趣导入此数据库到自己的MySQL可以参考文中内容。&lt;/p&gt;
&lt;h2&gt;&lt;a name=&quot;nav-4-2&quot;&gt;&lt;/a&gt;最左前缀原理与相关优化&lt;/h2&gt;
&lt;p&gt;高效使用索引的首要条件是知道什么样的查询会使用到索引，这个问题和B+Tree中的“最左前缀原理”有关，下面通过例子说明最左前缀原理。&lt;/p&gt;
&lt;p&gt;这里先说一下联合索引的概念。在上文中，我们都是假设索引只引用了单个的列，实际上，MySQL中的索引可以以一定顺序引用多个列，这种索引叫做联合索引，一般的，一个联合索引是一个有序元组&lt;a1, a2, …, an&gt;，其中各个元素均为数据表的一列，实际上要严格定义索引需要用到关系代数，但是这里我不想讨论太多关系代数的话题，因为那样会显得很枯燥，所以这里就不再做严格定义。另外，单列索引可以看成联合索引元素数为1的特例。&lt;/p&gt;
&lt;p&gt;以employees.titles表为例，下面先查看其上都有哪些索引：&lt;/p&gt;
&lt;pre class=&quot;prettyprint languague-sql&quot;&gt;SHOW INDEX FROM employees.titles;
+--------+------------+----------+--------------+-------------+-----------+-------------+------+------------+
| Table  | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Null | Index_type |
+--------+------------+----------+--------------+-------------+-----------+-------------+------+------------+
| titles |          0 | PRIMARY  |            1 | emp_no      | A         |        NULL |      | BTREE      |
| titles |          0 | PRIMARY  |            2 | title       | A         |        NULL |      | BTREE      |
| titles |          0 | PRIMARY  |            3 | from_date   | A         |      443308 |      | BTREE      |
| titles |          1 | emp_no   |            1 | emp_no      | A         |      443308 |      | BTREE      |
+--------+------------+----------+--------------+-------------+-----------+-------------+------+------------+&lt;/pre&gt;
&lt;p&gt;从结果中可以到titles表的主索引为&lt;emp_no, title, from_date&gt;，还有一个辅助索引&lt;emp_no&gt;。为了避免多个索引使事情变复杂（MySQL的SQL优化器在多索引时行为比较复杂），这里我们将辅助索引drop掉：&lt;/p&gt;
&lt;pre class=&quot;prettyprint languague-sql&quot;&gt;ALTER TABLE employees.titles DROP INDEX emp_no;&lt;/pre&gt;
&lt;p&gt;这样就可以专心分析索引PRIMARY的行为了。&lt;/p&gt;
&lt;h3&gt;情况一：全列匹配。&lt;/h3&gt;
&lt;pre class=&quot;prettyprint languague-sql&quot;&gt;EXPLAIN SELECT * FROM employees.titles WHERE emp_no='10001' AND title='Senior Engineer' AND from_date='1986-06-26';
+----+-------------+--------+-------+---------------+---------+---------+-------------------+------+-------+
| id | select_type | table  | type  | possible_keys | key     | key_len | ref               | rows | Extra |
+----+-------------+--------+-------+---------------+---------+---------+-------------------+------+-------+
|  1 | SIMPLE      | titles | const | PRIMARY       | PRIMARY | 59      | const,const,const |    1 |       |
+----+-------------+--------+-------+---------------+---------+---------+-------------------+------+-------+&lt;/pre&gt;
&lt;p&gt;很明显，当按照索引中所有列进行精确匹配（这里精确匹配指“=”或“IN”匹配）时，索引可以被用到。这里有一点需要注意，理论上索引对顺序是敏感的，但是由于MySQL的查询优化器会自动调整where子句的条件顺序以使用适合的索引，例如我们将where中的条件顺序颠倒：&lt;/p&gt;
&lt;pre class=&quot;prettyprint languague-sql&quot;&gt;EXPLAIN SELECT * FROM employees.titles WHERE from_date='1986-06-26' AND emp_no='10001' AND title='Senior Engineer';
+----+-------------+--------+-------+---------------+---------+---------+-------------------+------+-------+
| id | select_type | table  | type  | possible_keys | key     | key_len | ref               | rows | Extra |
+----+-------------+--------+-------+---------------+---------+---------+-------------------+------+-------+
|  1 | SIMPLE      | titles | const | PRIMARY       | PRIMARY | 59      | const,const,const |    1 |       |
+----+-------------+--------+-------+---------------+---------+---------+-------------------+------+-------+&lt;/pre&gt;
&lt;p&gt;效果是一样的。&lt;/p&gt;
&lt;h3&gt;情况二：最左前缀匹配。&lt;/h3&gt;
&lt;pre class=&quot;prettyprint languague-sql&quot;&gt;EXPLAIN SELECT * FROM employees.titles WHERE emp_no='10001';
+----+-------------+--------+------+---------------+---------+---------+-------+------+-------+
| id | select_type | table  | type | possible_keys | key     | key_len | ref   | rows | Extra |
+----+-------------+--------+------+---------------+---------+---------+-------+------+-------+
|  1 | SIMPLE      | titles | ref  | PRIMARY       | PRIMARY | 4       | const |    1 |       |
+----+-------------+--------+------+---------------+---------+---------+-------+------+-------+&lt;/pre&gt;
&lt;p&gt;当查询条件精确匹配索引的左边连续一个或几个列时，如&lt;emp_no&gt;或&lt;emp_no, title&gt;，所以可以被用到，但是只能用到一部分，即条件所组成的最左前缀。上面的查询从分析结果看用到了PRIMARY索引，但是key_len为4，说明只用到了索引的第一列前缀。&lt;/p&gt;
&lt;h3&gt;情况三：查询条件用到了索引中列的精确匹配，但是中间某个条件未提供。&lt;/h3&gt;
&lt;pre class=&quot;prettyprint languague-sql&quot;&gt;EXPLAIN SELECT * FROM employees.titles WHERE emp_no='10001' AND from_date='1986-06-26';
+----+-------------+--------+------+---------------+---------+---------+-------+------+-------------+
| id | select_type | table  | type | possible_keys | key     | key_len | ref   | rows | Extra       |
+----+-------------+--------+------+---------------+---------+---------+-------+------+-------------+
|  1 | SIMPLE      | titles | ref  | PRIMARY       | PRIMARY | 4       | const |    1 | Using where |
+----+-------------+--------+------+---------------+---------+---------+-------+------+-------------+&lt;/pre&gt;
&lt;p&gt;此时索引使用情况和情况二相同，因为title未提供，所以查询只用到了索引的第一列，而后面的from_date虽然也在索引中，但是由于title不存在而无法和左前缀连接，因此需要对结果进行扫描过滤from_date（这里由于emp_no唯一，所以不存在扫描）。如果想让from_date也使用索引而不是where过滤，可以增加一个辅助索引&lt;emp_no, from_date&gt;，此时上面的查询会使用这个索引。除此之外，还可以使用一种称之为“隔离列”的优化方法，将emp_no与from_date之间的“坑”填上。&lt;/p&gt;
&lt;p&gt;首先我们看下title一共有几种不同的值：&lt;/p&gt;
&lt;pre class=&quot;prettyprint languague-sql&quot;&gt;SELECT DISTINCT(title) FROM employees.titles;
+--------------------+
| title              |
+--------------------+
| Senior Engineer    |
| Staff              |
| Engineer           |
| Senior Staff       |
| Assistant Engineer |
| Technique Leader   |
| Manager            |
+--------------------+&lt;/pre&gt;
&lt;p&gt;只有7种。在这种成为“坑”的列值比较少的情况下，可以考虑用“IN”来填补这个“坑”从而形成最左前缀：&lt;/p&gt;
&lt;pre class=&quot;prettyprint languague-sql&quot;&gt;EXPLAIN SELECT * FROM employees.titles
WHERE emp_no='10001'
AND title IN ('Senior Engineer', 'Staff', 'Engineer', 'Senior Staff', 'Assistant Engineer', 'Technique Leader', 'Manager')
AND from_date='1986-06-26';
+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+
| id | select_type | table  | type  | possible_keys | key     | key_len | ref  | rows | Extra       |
+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+
|  1 | SIMPLE      | titles | range | PRIMARY       | PRIMARY | 59      | NULL |    7 | Using where |
+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+&lt;/pre&gt;
&lt;p&gt;这次key_len为59，说明索引被用全了，但是从type和rows看出IN实际上执行了一个range查询，这里检查了7个key。看下两种查询的性能比较：&lt;/p&gt;
&lt;pre class=&quot;prettyprint languague-sql&quot;&gt;SHOW PROFILES;
+----------+------------+-------------------------------------------------------------------------------+
| Query_ID | Duration   | Query                                                                         |
+----------+------------+-------------------------------------------------------------------------------+
|       10 | 0.00058000 | SELECT * FROM employees.titles WHERE emp_no='10001' AND from_date='1986-06-26'|
|       11 | 0.00052500 | SELECT * FROM employees.titles WHERE emp_no='10001' AND title IN ...          |
+----------+------------+-------------------------------------------------------------------------------+&lt;/pre&gt;
&lt;p&gt;“填坑”后性能提升了一点。如果经过emp_no筛选后余下很多数据，则后者性能优势会更加明显。当然，如果title的值很多，用填坑就不合适了，必须建立辅助索引。&lt;/p&gt;
&lt;h3&gt;情况四：查询条件没有指定索引第一列。&lt;/h3&gt;
&lt;pre class=&quot;prettyprint languague-sql&quot;&gt;EXPLAIN SELECT * FROM employees.titles WHERE from_date='1986-06-26';
+----+-------------+--------+------+---------------+------+---------+------+--------+-------------+
| id | select_type | table  | type | possible_keys | key  | key_len | ref  | rows   | Extra       |
+----+-------------+--------+------+---------------+------+---------+------+--------+-------------+
|  1 | SIMPLE      | titles | ALL  | NULL          | NULL | NULL    | NULL | 443308 | Using where |
+----+-------------+--------+------+---------------+------+---------+------+--------+-------------+&lt;/pre&gt;
&lt;p&gt;由于不是最左前缀，索引这样的查询显然用不到索引。&lt;/p&gt;
&lt;h3&gt;情况五：匹配某列的前缀字符串。&lt;/h3&gt;
&lt;pre class=&quot;prettyprint languague-sql&quot;&gt;EXPLAIN SELECT * FROM employees.titles WHERE emp_no='10001' AND title LIKE 'Senior%';
+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+
| id | select_type | table  | type  | possible_keys | key     | key_len | ref  | rows | Extra       |
+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+
|  1 | SIMPLE      | titles | range | PRIMARY       | PRIMARY | 56      | NULL |    1 | Using where |
+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+&lt;/pre&gt;
&lt;p&gt;此时可以用到索引，&lt;strike&gt;但是如果通配符不是只出现在末尾，则无法使用索引。&lt;/strike&gt;（原文表述有误，如果通配符%不出现在开头，则可以用到索引，但根据具体情况不同可能只会用其中一个前缀）&lt;/p&gt;
&lt;h3&gt;情况六：范围查询。&lt;/h3&gt;
&lt;pre class=&quot;prettyprint languague-sql&quot;&gt;EXPLAIN SELECT * FROM employees.titles WHERE emp_no &lt; '10010' and title='Senior Engineer';
+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+
| id | select_type | table  | type  | possible_keys | key     | key_len | ref  | rows | Extra       |
+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+
|  1 | SIMPLE      | titles | range | PRIMARY       | PRIMARY | 4       | NULL |   16 | Using where |
+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+&lt;/pre&gt;
&lt;p&gt;范围列可以用到索引（必须是最左前缀），但是范围列后面的列无法用到索引。同时，索引最多用于一个范围列，因此如果查询条件中有两个范围列则无法全用到索引。&lt;/p&gt;
&lt;pre class=&quot;prettyprint languague-sql&quot;&gt;EXPLAIN SELECT * FROM employees.titles
WHERE emp_no &lt; '10010'
AND title='Senior Engineer'
AND from_date BETWEEN '1986-01-01' AND '1986-12-31';
+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+
| id | select_type | table  | type  | possible_keys | key     | key_len | ref  | rows | Extra       |
+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+
|  1 | SIMPLE      | titles | range | PRIMARY       | PRIMARY | 4       | NULL |   16 | Using where |
+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+&lt;/pre&gt;
&lt;p&gt;可以看到索引对第二个范围索引无能为力。这里特别要说明MySQL一个有意思的地方，那就是仅用explain可能无法区分范围索引和多值匹配，因为在type中这两者都显示为range。同时，用了“between”并不意味着就是范围查询，例如下面的查询：&lt;/p&gt;
&lt;pre class=&quot;prettyprint languague-sql&quot;&gt;EXPLAIN SELECT * FROM employees.titles
WHERE emp_no BETWEEN '10001' AND '10010'
AND title='Senior Engineer'
AND from_date BETWEEN '1986-01-01' AND '1986-12-31';
+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+
| id | select_type | table  | type  | possible_keys | key     | key_len | ref  | rows | Extra       |
+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+
|  1 | SIMPLE      | titles | range | PRIMARY       | PRIMARY | 59      | NULL |   16 | Using where |
+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+&lt;/pre&gt;
&lt;p&gt;看起来是用了两个范围查询，但作用于emp_no上的“BETWEEN”实际上相当于“IN”，也就是说emp_no实际是多值精确匹配。可以看到这个查询用到了索引全部三个列。因此在MySQL中要谨慎地区分多值匹配和范围匹配，否则会对MySQL的行为产生困惑。&lt;/p&gt;
&lt;h3&gt;情况七：查询条件中含有函数或表达式。&lt;/h3&gt;
&lt;p&gt;很不幸，如果查询条件中含有函数或表达式，则MySQL不会为这列使用索引（虽然某些在数学意义上可以使用）。例如：&lt;/p&gt;
&lt;pre class=&quot;prettyprint languague-sql&quot;&gt;EXPLAIN SELECT * FROM employees.titles WHERE emp_no='10001' AND left(title, 6)='Senior';
+----+-------------+--------+------+---------------+---------+---------+-------+------+-------------+
| id | select_type | table  | type | possible_keys | key     | key_len | ref   | rows | Extra       |
+----+-------------+--------+------+---------------+---------+---------+-------+------+-------------+
|  1 | SIMPLE      | titles | ref  | PRIMARY       | PRIMARY | 4       | const |    1 | Using where |
+----+-------------+--------+------+---------------+---------+---------+-------+------+-------------+&lt;/pre&gt;
&lt;p&gt;虽然这个查询和情况五中功能相同，但是由于使用了函数left，则无法为title列应用索引，而情况五中用LIKE则可以。再如：&lt;p&gt;
&lt;pre class=&quot;prettyprint languague-sql&quot;&gt;EXPLAIN SELECT * FROM employees.titles WHERE emp_no - 1='10000';
+----+-------------+--------+------+---------------+------+---------+------+--------+-------------+
| id | select_type | table  | type | possible_keys | key  | key_len | ref  | rows   | Extra       |
+----+-------------+--------+------+---------------+------+---------+------+--------+-------------+
|  1 | SIMPLE      | titles | ALL  | NULL          | NULL | NULL    | NULL | 443308 | Using where |
+----+-------------+--------+------+---------------+------+---------+------+--------+-------------+&lt;/pre&gt;
&lt;p&gt;显然这个查询等价于查询emp_no为10001的函数，但是由于查询条件是一个表达式，MySQL无法为其使用索引。看来MySQL还没有智能到自动优化常量表达式的程度，因此在写查询语句时尽量避免表达式出现在查询中，而是先手工私下代数运算，转换为无表达式的查询语句。&lt;/p&gt;
&lt;h2&gt;&lt;a name=&quot;nav-4-3&quot;&gt;&lt;/a&gt;索引选择性与前缀索引&lt;/h2&gt;
&lt;p&gt;既然索引可以加快查询速度，那么是不是只要是查询语句需要，就建上索引？答案是否定的。因为索引虽然加快了查询速度，但索引也是有代价的：索引文件本身要消耗存储空间，同时索引会加重插入、删除和修改记录时的负担，另外，MySQL在运行时也要消耗资源维护索引，因此索引并不是越多越好。一般两种情况下不建议建索引。&lt;/p&gt;
&lt;p&gt;第一种情况是表记录比较少，例如一两千条甚至只有几百条记录的表，没必要建索引，让查询做全表扫描就好了。至于多少条记录才算多，这个个人有个人的看法，我个人的经验是以2000作为分界线，记录数不超过 2000可以考虑不建索引，超过2000条可以酌情考虑索引。&lt;/p&gt;
&lt;p&gt;另一种不建议建索引的情况是索引的选择性较低。所谓索引的选择性（Selectivity），是指不重复的索引值（也叫基数，Cardinality）与表记录数（#T）的比值：&lt;/p&gt;
&lt;p&gt;Index Selectivity = Cardinality / #T&lt;/p&gt;
&lt;p&gt;显然选择性的取值范围为(0, 1]，选择性越高的索引价值越大，这是由B+Tree的性质决定的。例如，上文用到的employees.titles表，如果title字段经常被单独查询，是否需要建索引，我们看一下它的选择性：&lt;/p&gt;
&lt;pre class=&quot;prettyprint languague-sql&quot;&gt;SELECT count(DISTINCT(title))/count(*) AS Selectivity FROM employees.titles;
+-------------+
| Selectivity |
+-------------+
|      0.0000 |
+-------------+&lt;/pre&gt;
&lt;p&gt;title的选择性不足0.0001（精确值为0.00001579），所以实在没有什么必要为其单独建索引。&lt;/p&gt;
&lt;p&gt;有一种与索引选择性有关的索引优化策略叫做前缀索引，就是用列的前缀代替整个列作为索引key，当前缀长度合适时，可以做到既使得前缀索引的选择性接近全列索引，同时因为索引key变短而减少了索引文件的大小和维护开销。下面以employees.employees表为例介绍前缀索引的选择和使用。&lt;/p&gt;
&lt;p&gt;从图12可以看到employees表只有一个索引&lt;emp_no&gt;，那么如果我们想按名字搜索一个人，就只能全表扫描了：&lt;/p&gt;
&lt;pre class=&quot;prettyprint languague-sql&quot;&gt;EXPLAIN SELECT * FROM employees.employees WHERE first_name='Eric' AND last_name='Anido';
+----+-------------+-----------+------+---------------+------+---------+------+--------+-------------+
| id | select_type | table     | type | possible_keys | key  | key_len | ref  | rows   | Extra       |
+----+-------------+-----------+------+---------------+------+---------+------+--------+-------------+
|  1 | SIMPLE      | employees | ALL  | NULL          | NULL | NULL    | NULL | 300024 | Using where |
+----+-------------+-----------+------+---------------+------+---------+------+--------+-------------+&lt;/pre&gt;
&lt;p&gt;如果频繁按名字搜索员工，这样显然效率很低，因此我们可以考虑建索引。有两种选择，建&lt;first_name&gt;或&lt;first_name, last_name&gt;，看下两个索引的选择性：&lt;/p&gt;
&lt;pre class=&quot;prettyprint languague-sql&quot;&gt;SELECT count(DISTINCT(first_name))/count(*) AS Selectivity FROM employees.employees;
+-------------+
| Selectivity |
+-------------+
|      0.0042 |
+-------------+
SELECT count(DISTINCT(concat(first_name, last_name)))/count(*) AS Selectivity FROM employees.employees;
+-------------+
| Selectivity |
+-------------+
|      0.9313 |
+-------------+&lt;/pre&gt;
&lt;p&gt;&lt;first_name&gt;显然选择性太低，&lt;first_name, last_name&gt;选择性很好，但是first_name和last_name加起来长度为30，有没有兼顾长度和选择性的办法？可以考虑用first_name和last_name的前几个字符建立索引，例如&lt;first_name, left(last_name, 3)&gt;，看看其选择性：&lt;/p&gt;
&lt;pre class=&quot;prettyprint languague-sql&quot;&gt;SELECT count(DISTINCT(concat(first_name, left(last_name, 3))))/count(*) AS Selectivity FROM employees.employees;
+-------------+
| Selectivity |
+-------------+
|      0.7879 |
+-------------+&lt;/pre&gt;
&lt;p&gt;选择性还不错，但离0.9313还是有点距离，那么把last_name前缀加到4：&lt;/p&gt;
&lt;pre class=&quot;prettyprint languague-sql&quot;&gt;SELECT count(DISTINCT(concat(first_name, left(last_name, 4))))/count(*) AS Selectivity FROM employees.employees;
+-------------+
| Selectivity |
+-------------+
|      0.9007 |
+-------------+&lt;/pre&gt;
&lt;p&gt;这时选择性已经很理想了，而这个索引的长度只有18，比&lt;first_name, last_name&gt;短了接近一半，我们把这个前缀索引 建上：&lt;/p&gt;
&lt;pre class=&quot;prettyprint languague-sql&quot;&gt;ALTER TABLE employees.employees
ADD INDEX `first_name_last_name4` (first_name, last_name(4));&lt;/pre&gt;
&lt;p&gt;此时再执行一遍按名字查询，比较分析一下与建索引前的结果：&lt;/p&gt;
&lt;pre class=&quot;prettyprint languague-sql&quot;&gt;SHOW PROFILES;
+----------+------------+---------------------------------------------------------------------------------+
| Query_ID | Duration   | Query                                                                           |
+----------+------------+---------------------------------------------------------------------------------+
|       87 | 0.11941700 | SELECT * FROM employees.employees WHERE first_name='Eric' AND last_name='Anido' |
|       90 | 0.00092400 | SELECT * FROM employees.employees WHERE first_name='Eric' AND last_name='Anido' |
+----------+------------+---------------------------------------------------------------------------------+&lt;/pre&gt;
&lt;p&gt;性能的提升是显著的，查询速度提高了120多倍。&lt;/p&gt;
&lt;p&gt;前缀索引兼顾索引大小和查询速度，但是其缺点是不能用于ORDER BY和GROUP BY操作，也不能用于Covering index（即当索引本身包含查询所需全部数据时，不再访问数据文件本身）。&lt;/p&gt;
&lt;h2&gt;&lt;a name=&quot;nav-4-4&quot;&gt;&lt;/a&gt;InnoDB的主键选择与插入优化&lt;/h2&gt;
&lt;p&gt;在使用InnoDB存储引擎时，如果没有特别的需要，请永远使用一个与业务无关的自增字段作为主键。&lt;/p&gt;
&lt;p&gt;经常看到有帖子或博客讨论主键选择问题，有人建议使用业务无关的自增主键，有人觉得没有必要，完全可以使用如学号或身份证号这种唯一字段作为主键。不论支持哪种论点，大多数论据都是业务层面的。如果从数据库索引优化角度看，使用InnoDB引擎而不使用自增主键绝对是一个糟糕的主意。&lt;/p&gt;
&lt;p&gt;上文讨论过InnoDB的索引实现，InnoDB使用聚集索引，数据记录本身被存于主索引（一颗B+Tree）的叶子节点上。这就要求同一个叶子节点内（大小为一个内存页或磁盘页）的各条数据记录按主键顺序存放，因此每当有一条新的记录插入时，MySQL会根据其主键将其插入适当的节点和位置，如果页面达到装载因子（InnoDB默认为15/16），则开辟一个新的页（节点）。&lt;/p&gt;
&lt;p&gt;如果表使用自增主键，那么每次插入新的记录，记录就会顺序添加到当前索引节点的后续位置，当一页写满，就会自动开辟一个新的页。如下图所示：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/theory-of-mysql-index/13.png&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;图13&lt;/p&gt;
&lt;p&gt;这样就会形成一个紧凑的索引结构，近似顺序填满。由于每次插入时也不需要移动已有数据，因此效率很高，也不会增加很多开销在维护索引上。&lt;/p&gt;
&lt;p&gt;如果使用非自增主键（如果身份证号或学号等），由于每次插入主键的值近似于随机，因此每次新纪录都要被插到现有索引页得中间某个位置：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/theory-of-mysql-index/14.png&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;图14&lt;/p&gt;
&lt;p&gt;此时MySQL不得不为了将新记录插到合适位置而移动数据，甚至目标页面可能已经被回写到磁盘上而从缓存中清掉，此时又要从磁盘上读回来，这增加了很多开销，同时频繁的移动、分页操作造成了大量的碎片，得到了不够紧凑的索引结构，后续不得不通过OPTIMIZE TABLE来重建表并优化填充页面。&lt;/p&gt;
&lt;p&gt;因此，只要可以，请尽量在InnoDB上采用自增字段做主键。&lt;/p&gt;
&lt;h1&gt;&lt;a name=&quot;nav-5&quot;&gt;&lt;/a&gt;后记&lt;/h1&gt;
&lt;p&gt;这篇文章断断续续写了半个月，主要内容就是上面这些了。不可否认，这篇文章在一定程度上有纸上谈兵之嫌，因为我本人对MySQL的使用属于菜鸟级别，更没有太多数据库调优的经验，在这里大谈数据库索引调优有点大言不惭。就当是我个人的一篇学习笔记了。&lt;/p&gt;
&lt;p&gt;其实数据库索引调优是一项技术活，不能仅仅靠理论，因为实际情况千变万化，而且MySQL本身存在很复杂的机制，如查询优化策略和各种引擎的实现差异等都会使情况变得更加复杂。但同时这些理论是索引调优的基础，只有在明白理论的基础上，才能对调优策略进行合理推断并了解其背后的机制，然后结合实践中不断的实验和摸索，从而真正达到高效使用MySQL索引的目的。&lt;/p&gt;
&lt;p&gt;另外，MySQL索引及其优化涵盖范围非常广，本文只是涉及到其中一部分。如与排序（ORDER BY）相关的索引优化及覆盖索引（Covering index）的话题本文并未涉及，同时除B-Tree索引外MySQL还根据不同引擎支持的哈希索引、全文索引等等本文也并未涉及。如果有机会，希望再对本文未涉及的部分进行补充吧。&lt;/p&gt;
&lt;h1&gt;&lt;a name=&quot;nav-6&quot;&gt;&lt;/a&gt;参考文献&lt;/h1&gt;
&lt;p&gt;[1] Baron Scbwartz等 著，王小东等 译；高性能MySQL（High Performance MySQL）；电子工业出版社，2010&lt;/p&gt;
&lt;p&gt;[2] Michael Kofler 著，杨晓云等 译；MySQL5权威指南（The Definitive Guide to MySQL5）；人民邮电出版社，2006&lt;/p&gt;
&lt;p&gt;[3] 姜承尧 著；MySQL技术内幕-InnoDB存储引擎；机械工业出版社，2011&lt;/p&gt;
&lt;p&gt;[4] D Comer, Ubiquitous B-tree; ACM Computing Surveys (CSUR), 1979&lt;/p&gt;
&lt;p&gt;[5] Codd, E. F. (1970). &quot;A relational model of data for large shared data banks&quot;. Communications of the ACM, , Vol. 13, No. 6, pp. 377-387&lt;/p&gt;
&lt;p&gt;[6] MySQL5.1参考手册 - &lt;a title=&quot;http://dev.mysql.com/doc/refman/5.1/zh/index.html&quot; href=&quot;http://dev.mysql.com/doc/refman/5.1/zh/index.html&quot;&gt;http://dev.mysql.com/doc/refman/5.1/zh/index.html&lt;/a&gt;&lt;/p&gt;
</description>
</item>
<item>
<title>Nginx模块开发入门</title>
<link>http://blog.codinglabs.org/articles/intro-of-nginx-module-development.html</link>
<guid>http://blog.codinglabs.org/articles/intro-of-nginx-module-development.html</guid>
<pubDate>Mon, 17 Oct 2011 16:00:00 GMT</pubDate>
<description>&lt;h1&gt;&lt;a name=&quot;section0&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;
&lt;p&gt;Nginx是当前最流行的HTTP Server之一，根据W3Techs的统计，目前世界排名（根据Alexa）前100万的网站中，&lt;a href=&quot;http://w3techs.com/technologies/overview/web_server/all&quot; target=&quot;_blank&quot;&gt;Nginx的占有率为6.8%&lt;/a&gt;。与Apache相比，&lt;a href=&quot;http://www.joeandmotorboat.com/2008/02/28/apache-vs-nginx-web-server-performance-deathmatch/&quot; target=&quot;_blank&quot;&gt;Nginx在高并发情况下具有巨大的性能优势&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;Nginx属于典型的微内核设计，其内核非常简洁和优雅，同时具有非常高的可扩展性。Nginx最初仅仅主要被用于做反向代理，后来随着HTTP核心的成熟和各种HTTP扩展模块的丰富，Nginx越来越多被用来取代Apache而单独承担HTTP Server的责任，例如目前淘宝内各个部门正越来越多使用Nginx取代Apache，据笔者了解，在腾讯和新浪等公司也存在类似情况。&lt;/p&gt;
&lt;p&gt;同时，大量的第三方扩展模块也令Nginx越来越强大。例如，由淘宝的工程师清无（王晓哲）和春来（章亦春）所开发的&lt;a href=&quot;https://github.com/chaoslawful/lua-nginx-module&quot; target=&quot;_blank&quot;&gt;nginx_lua_module&lt;/a&gt;可以将Lua语言嵌入到Nginx配置中，从而利用Lua极大增强了Nginx本身的编程能力，甚至可以不用配合其它脚本语言（如PHP或Python等），只靠Nginx本身就可以实现复杂业务的处理。而春来所开发的&lt;a href=&quot;https://github.com/agentzh/ngx_openresty&quot; target=&quot;_blank&quot;&gt;ngx_openresty&lt;/a&gt;更是通过集成&lt;a href=&quot;http://luajit.org/&quot; target=&quot;_blank&quot;&gt;LuaJIT&lt;/a&gt;等组件，将Nginx本身变成了一个完全的应用开发平台。目前淘宝数据平台与产品部量子统计的产品都是基于ngx_openresty所开发。对ngxin_lua_module或ngx_openresty感兴趣的朋友可以参考我在关键词上给出的链接，后续我也可能会写一些与其有关的文章。&lt;/p&gt;
&lt;p&gt;本文将会重点关注Nginx模块开发入门及基础。目前Nginx的学习资料非常少，而扩展模块开发相关的资料几乎只有《&lt;a href=&quot;http://www.evanmiller.org/nginx-modules-guide.html&quot; target=&quot;_blank&quot;&gt;Emiller's Guide To Nginx Module Development&lt;/a&gt;》一文，此文十分经典，但是由于Nginx版本的演进，其中少许内容可能有点过时。本文是笔者在研读这篇文章和Nginx源代码的基础上，对自己学习Nginx模块开发的一个总结。本文将通过一个完整的模块开发实例讲解Nginx模块开发的入门内容。&lt;/p&gt;
&lt;p&gt;本文将基于Nginx最新的&lt;a href=&quot;http://nginx.org/download/nginx-1.0.0.tar.gz&quot; target=&quot;_blank&quot;&gt;1.0.0&lt;/a&gt;版本，操作系统环境为Linux（Ubuntu10.10）。
&lt;!--more--&gt;
&lt;/p&gt;
&lt;h1&gt;&lt;a name=&quot;section1&quot;&gt;&lt;/a&gt;Nginx提要&lt;/h1&gt;
&lt;p&gt;开发Nginx扩展当然首要前提是对Nginx有一定的了解，然而本文并不打算详细阐述Nginx的方方面面，诸如Nginx的安装和各种详细配置等内容都可以在Nginx官网的Document中找到，本文在这里只会概括性描述一些后面可能会用到的原理和概念。&lt;/p&gt;
&lt;h2&gt;&lt;a name=&quot;section1-1&quot;&gt;&lt;/a&gt;Nginx在Linux下的安装与运行&lt;/h2&gt;
&lt;p&gt;使用Nginx的第一步是下载Nginx源码包，例如1.0.0的下载地址为&lt;a title=&quot;http://nginx.org/download/nginx-1.0.0.tar.gz&quot; href=&quot;http://nginx.org/download/nginx-1.0.0.tar.gz&quot;&gt;http://nginx.org/download/nginx-1.0.0.tar.gz&lt;/a&gt;。下载完后用tar命令解压缩，进入目录后安装过程与Linux下通常步骤无异，例如我想将Nginx安装到/usr/local/nginx下，则执行如下命令：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-bash&quot;&gt;./configure --prefix=/usr/local/nginx
make
make install&lt;/pre&gt;
&lt;p&gt;安装完成后可以直接使用下面命令启动Nginx：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-bash&quot;&gt;/usr/local/nginx/sbin/nginx&lt;/pre&gt;
&lt;p&gt;Nginx默认以Deamon进程启动，输入下列命令：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-bash&quot;&gt;curl -i http://localhost/&lt;/pre&gt;
&lt;p&gt;就可以检测Nginx是否已经成功运行。或者也可以在浏览器中输入&lt;a href=&quot;http://localhost/&quot;&gt;http://localhost/&lt;/a&gt;，应该可以看到Nginx的欢迎页面了。启动后如果想停止Nginx可以使用：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-bash&quot;&gt;/usr/local/nginx/sbin/nginx -s stop&lt;/pre&gt;
&lt;h2&gt;&lt;a name=&quot;section1-2&quot;&gt;&lt;/a&gt;Nginx配置文件基本结构&lt;/h2&gt;
&lt;p&gt;配置文件可以看做是Nginx的灵魂，Nginx服务在启动时会读入配置文件，而后续几乎一切动作行为都是按照配置文件中的指令进行的，因此如果将Nginx本身看做一个计算机，那么Nginx的配置文件可以看成是全部的程序指令。&lt;/p&gt;
&lt;p&gt;下面是一个Nginx配置文件的实例：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-bash&quot;&gt;#user  nobody;
worker_processes  8;
error_log  logs/error.log;
pid        logs/nginx.pid;
events {
    worker_connections  1024;
}
http {
    include       mime.types;
    default_type  application/octet-stream;

    sendfile        on;
    #tcp_nopush     on;
    keepalive_timeout  65;
    #gzip  on;

    server {
        listen       80;
        server_name  localhost;
        location / {
            root   /home/yefeng/www;
            index  index.html index.htm;
        }
        #error_page  404              /404.html;
        # redirect server error pages to the static page /50x.html
        #
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }
}&lt;/pre&gt;
&lt;p&gt;Nginx配置文件是纯文本文件，你可以用任何文本编辑器如vim或emacs打开它，通常它会在nginx安装目录的conf下，如我的nginx安装在/usr/local/nginx，主配置文件默认放在/usr/local/nginx/conf/nginx.conf。&lt;/p&gt;
&lt;p&gt;其中“#”表示此行是注释，由于笔者为了学习扩展开发安装了一个纯净的Nginx，因此配置文件没有经过太多改动。&lt;/p&gt;
&lt;p&gt;Nginx的配置文件是以block的形式组织的，一个block通常使用大括号“{}”表示。block分为几个层级，整个配置文件为main层级，这是最大的层级；在main层级下可以有event、http等层级，而http中又会有server block，server block中可以包含location block。&lt;/p&gt;
&lt;p&gt;每个层级可以有自己的指令（Directive），例如worker_processes是一个main层级指令，它指定Nginx服务的Worker进程数量。有的指令只能在一个层级中配置，如worker_processes只能存在于main中，而有的指令可以存在于多个层级，在这种情况下，子block会继承父block的配置，同时如果子block配置了与父block不同的指令，则会覆盖掉父block的配置。指令的格式是“指令名 参数1 参数2 … 参数N;”，注意参数间可用任意数量空格分隔，最后要加分号。&lt;/p&gt;
&lt;p&gt;在开发Nginx HTTP扩展模块过程中，需要特别注意的是main、server和location三个层级，因为扩展模块通常允许指定新的配置指令在这三个层级中。&lt;/p&gt;
&lt;p&gt;最后要提到的是配置文件是可以包含的，如上面配置文件中“include mime.types”就包含了mine.types这个配置文件，此文件指定了各种HTTP Content-type。&lt;/p&gt;
&lt;p&gt;一般来说，一个server block表示一个Host，而里面的一个location则代表一个路由映射规则，这两个block可以说是HTTP配置的核心。&lt;/p&gt;
&lt;p&gt;下图是Nginx配置文件通常结构图示。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/intro-of-nginx-module-development/1.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;关于Nginx配置的更多内容请参看Nginx官方文档。&lt;/p&gt;
&lt;h2&gt;&lt;a name=&quot;section1-3&quot;&gt;&lt;/a&gt;Nginx模块工作原理概述&lt;/h2&gt;
&lt;p&gt;（Nginx本身支持多种模块，如HTTP模块、EVENT模块和MAIL模块，本文只讨论HTTP模块）&lt;/p&gt;
&lt;p&gt;Nginx本身做的工作实际很少，当它接到一个HTTP请求时，它仅仅是通过查找配置文件将此次请求映射到一个location block，而此location中所配置的各个指令则会启动不同的模块去完成工作，因此模块可以看做Nginx真正的劳动工作者。通常一个location中的指令会涉及一个handler模块和多个filter模块（当然，多个location可以复用同一个模块）。handler模块负责处理请求，完成响应内容的生成，而filter模块对响应内容进行处理。因此Nginx模块开发分为handler开发和filter开发（本文不考虑load-balancer模块）。下图展示了一次常规请求和响应的过程。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/intro-of-nginx-module-development/2.png&quot;/&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a name=&quot;section2&quot;&gt;&lt;/a&gt;Nginx模块开发实战&lt;/h1&gt;
&lt;p&gt;下面本文展示一个简单的Nginx模块开发全过程，我们开发一个叫echo的handler模块，这个模块功能非常简单，它接收“echo”指令，指令可指定一个字符串参数，模块会输出这个字符串作为HTTP响应。例如，做如下配置：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-bash&quot;&gt;location /echo {
    echo &quot;hello nginx&quot;;
}&lt;/pre&gt;
&lt;p&gt;则访问&lt;a href=&quot;http://hostname/echo&quot;&gt;http://hostname/echo&lt;/a&gt;时会输出hello nginx。&lt;/p&gt;
&lt;p&gt;直观来看，要实现这个功能需要三步：1、读入配置文件中echo指令及其参数；2、进行HTTP包装（添加HTTP头等工作）；3、将结果返回给客户端。下面本文将分部介绍整个模块的开发过程。&lt;/p&gt;
&lt;h2&gt;&lt;a name=&quot;section2-1&quot;&gt;&lt;/a&gt;定义模块配置结构&lt;/h2&gt;
&lt;p&gt;首先我们需要一个结构用于存储从配置文件中读进来的相关指令参数，即模块配置信息结构。根据Nginx模块开发规则，这个结构的命名规则为ngx_http_[module-name]_[main|srv|loc]_conf_t。其中main、srv和loc分别用于表示同一模块在三层block中的配置信息。这里我们的echo模块只需要运行在loc层级下，需要存储一个字符串参数，因此我们可以定义如下的模块配置：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-c&quot;&gt;typedef struct {
    ngx_str_t  ed;
} ngx_http_echo_loc_conf_t;&lt;/pre&gt;
&lt;p&gt;其中字段ed用于存储echo指令指定的需要输出的字符串。注意这里ed的类型，在Nginx模块开发中使用ngx_str_t类型表示字符串，这个类型定义在core/ngx_string中：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-c&quot;&gt;typedef struct {
    size_t      len;
    u_char     *data;
} ngx_str_t;&lt;/pre&gt;
&lt;p&gt;其中两个字段分别表示字符串的长度和数据起始地址。注意在Nginx源代码中对数据类型进行了别称定义，如ngx_int_t为intptr_t的别称，为了保持一致，在开发Nginx模块时也应该使用这些Nginx源码定义的类型而不要使用C原生类型。除了ngx_str_t外，其它三个常用的nginx type分别为：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-c&quot;&gt;typedef intptr_t        ngx_int_t;
typedef uintptr_t       ngx_uint_t;
typedef intptr_t        ngx_flag_t;&lt;/pre&gt;
&lt;p&gt;具体定义请参看core/ngx_config.h。关于intptr_t和uintptr_t请参考C99中的&lt;a href=&quot;http://linux.die.net/include/stdint.h&quot; target=&quot;_blank&quot;&gt;stdint.h&lt;/a&gt;或&lt;a title=&quot;http://linux.die.net/man/3/intptr_t&quot; href=&quot;http://linux.die.net/man/3/intptr_t&quot;&gt;http://linux.die.net/man/3/intptr_t&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a name=&quot;section2-2&quot;&gt;&lt;/a&gt;定义指令&lt;/h2&gt;
&lt;p&gt;一个Nginx模块往往接收一至多个指令，echo模块接收一个指令“echo”。Nginx模块使用一个ngx_command_t数组表示模块所能接收的所有模块，其中每一个元素表示一个条指令。ngx_command_t是ngx_command_s的一个别称（Nginx习惯于使用“_s”后缀命名结构体，然后typedef一个同名“_t”后缀名称作为此结构体的类型名），ngx_command_s定义在core/ngx_config_file.h中：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-c&quot;&gt;struct ngx_command_s {
    ngx_str_t             name;
    ngx_uint_t            type;
    char               *(*set)(ngx_conf_t *cf, ngx_command_t *cmd, void *conf);
    ngx_uint_t            conf;
    ngx_uint_t            offset;
    void                 *post;
};&lt;/pre&gt;
&lt;p&gt;其中name是词条指令的名称，type使用掩码标志位方式配置指令参数，相关可用type定义在core/ngx_config_file.h中：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-c&quot;&gt;#define NGX_CONF_NOARGS      0x00000001
#define NGX_CONF_TAKE1       0x00000002
#define NGX_CONF_TAKE2       0x00000004
#define NGX_CONF_TAKE3       0x00000008
#define NGX_CONF_TAKE4       0x00000010
#define NGX_CONF_TAKE5       0x00000020
#define NGX_CONF_TAKE6       0x00000040
#define NGX_CONF_TAKE7       0x00000080
#define NGX_CONF_MAX_ARGS    8
#define NGX_CONF_TAKE12      (NGX_CONF_TAKE1|NGX_CONF_TAKE2)
#define NGX_CONF_TAKE13      (NGX_CONF_TAKE1|NGX_CONF_TAKE3)
#define NGX_CONF_TAKE23      (NGX_CONF_TAKE2|NGX_CONF_TAKE3)
#define NGX_CONF_TAKE123     (NGX_CONF_TAKE1|NGX_CONF_TAKE2|NGX_CONF_TAKE3)
#define NGX_CONF_TAKE1234    (NGX_CONF_TAKE1|NGX_CONF_TAKE2|NGX_CONF_TAKE3   \
        |NGX_CONF_TAKE4)
#define NGX_CONF_ARGS_NUMBER 0x000000ff
#define NGX_CONF_BLOCK       0x00000100
#define NGX_CONF_FLAG        0x00000200
#define NGX_CONF_ANY         0x00000400
#define NGX_CONF_1MORE       0x00000800
#define NGX_CONF_2MORE       0x00001000
#define NGX_CONF_MULTI       0x00002000&lt;/pre&gt;
&lt;p&gt;其中NGX_CONF_NOARGS表示此指令不接受参数，NGX_CON F_TAKE1-7表示精确接收1-7个，NGX_CONF_TAKE12表示接受1或2个参数，NGX_CONF_1MORE表示至少一个参数，NGX_CONF_FLAG表示接受“on|off”……&lt;/p&gt;
&lt;p&gt;set是一个函数指针，用于指定一个参数转化函数，这个函数一般是将配置文件中相关指令的参数转化成需要的格式并存入配置结构体。Nginx预定义了一些转换函数，可以方便我们调用，这些函数定义在core/ngx_conf_file.h中，一般以“_slot”结尾，例如ngx_conf_set_flag_slot将“on或off”转换为“1或0”，再如ngx_conf_set_str_slot将裸字符串转化为ngx_str_t。&lt;/p&gt;
&lt;p&gt;conf用于指定Nginx相应配置文件内存其实地址，一般可以通过内置常量指定，如NGX_HTTP_LOC_CONF_OFFSET，offset指定此条指令的参数的偏移量。&lt;/p&gt;
&lt;p&gt;下面是echo模块的指令定义：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-c&quot;&gt;static ngx_command_t  ngx_http_echo_commands[] = {
    { ngx_string(&quot;echo&quot;),
        NGX_HTTP_LOC_CONF|NGX_CONF_TAKE1,
        ngx_http_echo,
        NGX_HTTP_LOC_CONF_OFFSET,
        offsetof(ngx_http_echo_loc_conf_t, ed),
        NULL },
        ngx_null_command
};&lt;/pre&gt;
&lt;p&gt;指令数组的命名规则为ngx_http_[module-name]_commands，注意数组最后一个元素要是ngx_null_command结束。&lt;/p&gt;
&lt;p&gt;参数转化函数的代码为：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-c&quot;&gt;static char *
ngx_http_echo(ngx_conf_t *cf, ngx_command_t *cmd, void *conf)
{
    ngx_http_core_loc_conf_t  *clcf;
    clcf = ngx_http_conf_get_module_loc_conf(cf, ngx_http_core_module);
    clcf-&gt;handler = ngx_http_echo_handler;
    ngx_conf_set_str_slot(cf,cmd,conf);
    return NGX_CONF_OK;
}&lt;/pre&gt;
&lt;p&gt;这个函数除了调用ngx_conf_set_str_slot转化echo指令的参数外，还将修改了核心模块配置（也就是这个location的配置），将其handler替换为我们编写的handler：ngx_http_echo_handler。这样就屏蔽了此location的默认handler，使用ngx_http_echo_handler产生HTTP响应。&lt;/p&gt;
&lt;h2&gt;&lt;a name=&quot;section2-3&quot;&gt;&lt;/a&gt;创建合并配置信息&lt;/h2&gt;
&lt;p&gt;下一步是定义模块Context。&lt;/p&gt;
&lt;p&gt;这里首先需要定义一个ngx_http_module_t类型的结构体变量，命名规则为ngx_http_[module-name]_module_ctx，这个结构主要用于定义各个Hook函数。下面是echo模块的context结构：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-c&quot;&gt;static ngx_http_module_t  ngx_http_echo_module_ctx = {
    NULL,                                  /* preconfiguration */
    NULL,                                  /* postconfiguration */
    NULL,                                  /* create main configuration */
    NULL,                                  /* init main configuration */
    NULL,                                  /* create server configuration */
    NULL,                                  /* merge server configuration */
    ngx_http_echo_create_loc_conf,         /* create location configration */
    ngx_http_echo_merge_loc_conf           /* merge location configration */
};&lt;/pre&gt;
&lt;p&gt;可以看到一共有8个Hook注入点，分别会在不同时刻被Nginx调用，由于我们的模块仅仅用于location域，这里将不需要的注入点设为NULL即可。其中create_loc_conf用于初始化一个配置结构体，如为配置结构体分配内存等工作；merge_loc_conf用于将其父block的配置信息合并到此结构体中，也就是实现配置的继承。这两个函数会被Nginx自动调用。注意这里的命名规则：ngx_http_[module-name]_[create|merge]_[main|srv|loc]_conf。&lt;/p&gt;
&lt;p&gt;下面是echo模块这个两个函数的代码：
&lt;pre class=&quot;prettyprint linenums languague-c&quot;&gt;static void *
ngx_http_echo_create_loc_conf(ngx_conf_t *cf)
{
    ngx_http_echo_loc_conf_t  *conf;
    conf = ngx_pcalloc(cf-&gt;pool, sizeof(ngx_http_echo_loc_conf_t));
    if (conf == NULL) {
        return NGX_CONF_ERROR;
    }
    conf-&gt;ed.len = 0;
    conf-&gt;ed.data = NULL;
    return conf;
}
static char *
ngx_http_echo_merge_loc_conf(ngx_conf_t *cf, void *parent, void *child)
{
    ngx_http_echo_loc_conf_t *prev = parent;
    ngx_http_echo_loc_conf_t *conf = child;
    ngx_conf_merge_str_value(conf-&gt;ed, prev-&gt;ed, &quot;&quot;);
    return NGX_CONF_OK;
}&lt;/pre&gt;
&lt;p&gt;其中ngx_pcalloc用于在Nginx内存池中分配一块空间，是pcalloc的一个包装。使用ngx_pcalloc分配的内存空间不必手工free，Nginx会自行管理，在适当是否释放。&lt;/p&gt;
&lt;p&gt;create_loc_conf新建一个ngx_http_echo_loc_conf_t，分配内存，并初始化其中的数据，然后返回这个结构的指针；merge_loc_conf将父block域的配置信息合并到create_loc_conf新建的配置结构体中。&lt;/p&gt;
&lt;p&gt;其中ngx_conf_merge_str_value不是一个函数，而是一个宏，其定义在core/ngx_conf_file.h中：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-c&quot;&gt;#define ngx_conf_merge_str_value(conf, prev, default)                        \
    if (conf.data == NULL) {                                                 \
        if (prev.data) {                                                     \
            conf.len = prev.len;                                             \
            conf.data = prev.data;                                           \
        } else {                                                             \
            conf.len = sizeof(default) - 1;                                  \
            conf.data = (u_char *) default;                                  \
        }                                                                    \
    }&lt;/pre&gt;
&lt;p&gt;同时可以看到，core/ngx_conf_file.h还定义了很多merge value的宏用于merge各种数据。它们的行为比较相似：使用prev填充conf，如果prev的数据为空则使用default填充。&lt;/p&gt;
&lt;h2&gt;&lt;a name=&quot;section2-4&quot;&gt;&lt;/a&gt;编写Handler&lt;/h2&gt;
&lt;p&gt;下面的工作是编写handler。handler可以说是模块中真正干活的代码，它主要有以下四项职责：&lt;/p&gt;
&lt;p&gt;读入模块配置。&lt;/p&gt;
&lt;p&gt;处理功能业务。&lt;/p&gt;
&lt;p&gt;产生HTTP header。&lt;/p&gt;
&lt;p&gt;产生HTTP body。&lt;/p&gt;
&lt;p&gt;下面先贴出echo模块的代码，然后通过分析代码的方式介绍如何实现这四步。这一块的代码比较复杂：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-c&quot;&gt;static ngx_int_t
ngx_http_echo_handler(ngx_http_request_t *r)
{
    ngx_int_t rc;
    ngx_buf_t *b;
    ngx_chain_t out;
    ngx_http_echo_loc_conf_t *elcf;
    elcf = ngx_http_get_module_loc_conf(r, ngx_http_echo_module);
    if(!(r-&gt;method &amp; (NGX_HTTP_HEAD|NGX_HTTP_GET|NGX_HTTP_POST)))
    {
        return NGX_HTTP_NOT_ALLOWED;
    }
    r-&gt;headers_out.content_type.len = sizeof(&quot;text/html&quot;) - 1;
    r-&gt;headers_out.content_type.data = (u_char *) &quot;text/html&quot;;
    r-&gt;headers_out.status = NGX_HTTP_OK;
    r-&gt;headers_out.content_length_n = elcf-&gt;ed.len;
    if(r-&gt;method == NGX_HTTP_HEAD)
    {
        rc = ngx_http_send_header(r);
        if(rc != NGX_OK)
        {
            return rc;
        }
    }
    b = ngx_pcalloc(r-&gt;pool, sizeof(ngx_buf_t));
    if(b == NULL)
    {
        ngx_log_error(NGX_LOG_ERR, r-&gt;connection-&gt;log, 0, &quot;Failed to allocate response buffer.&quot;);
        return NGX_HTTP_INTERNAL_SERVER_ERROR;
    }
    out.buf = b;
    out.next = NULL;
    b-&gt;pos = elcf-&gt;ed.data;
    b-&gt;last = elcf-&gt;ed.data + (elcf-&gt;ed.len);
    b-&gt;memory = 1;
    b-&gt;last_buf = 1;
    rc = ngx_http_send_header(r);
    if(rc != NGX_OK)
    {
        return rc;
    }
    return ngx_http_output_filter(r, &amp;out);
}&lt;/pre&gt;
&lt;p&gt;handler会接收一个ngx_http_request_t指针类型的参数，这个参数指向一个ngx_http_request_t结构体，此结构体存储了这次HTTP请求的一些信息，这个结构定义在http/ngx_http_request.h中：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-c&quot;&gt;struct ngx_http_request_s {
    uint32_t                          signature;         /* &quot;HTTP&quot; */
    ngx_connection_t                 *connection;
    void                            **ctx;
    void                            **main_conf;
    void                            **srv_conf;
    void                            **loc_conf;
    ngx_http_event_handler_pt         read_event_handler;
    ngx_http_event_handler_pt         write_event_handler;
#if (NGX_HTTP_CACHE)
    ngx_http_cache_t                 *cache;
#endif
    ngx_http_upstream_t              *upstream;
    ngx_array_t                      *upstream_states;
    /* of ngx_http_upstream_state_t */
    ngx_pool_t                       *pool;
    ngx_buf_t                        *header_in;
    ngx_http_headers_in_t             headers_in;
    ngx_http_headers_out_t            headers_out;
    ngx_http_request_body_t          *request_body;
    time_t                            lingering_time;
    time_t                            start_sec;
    ngx_msec_t                        start_msec;
    ngx_uint_t                        method;
    ngx_uint_t                        http_version;
    ngx_str_t                         request_line;
    ngx_str_t                         uri;
    ngx_str_t                         args;
    ngx_str_t                         exten;
    ngx_str_t                         unparsed_uri;
    ngx_str_t                         method_name;
    ngx_str_t                         http_protocol;
    ngx_chain_t                      *out;
    ngx_http_request_t               *main;
    ngx_http_request_t               *parent;
    ngx_http_postponed_request_t     *postponed;
    ngx_http_post_subrequest_t       *post_subrequest;
    ngx_http_posted_request_t        *posted_requests;
    ngx_http_virtual_names_t         *virtual_names;
    ngx_int_t                         phase_handler;
    ngx_http_handler_pt               content_handler;
    ngx_uint_t                        access_code;
    ngx_http_variable_value_t        *variables;
    /* ... */
}&lt;/pre&gt;
&lt;p&gt;由于ngx_http_request_s定义比较长，这里我只截取了一部分。可以看到里面有诸如uri，args和request_body等HTTP常用信息。这里需要特别注意的几个字段是headers_in、headers_out和chain，它们分别表示request header、response header和输出数据缓冲区链表（缓冲区链表是Nginx I/O中的重要内容，后面会单独介绍）。&lt;/p&gt;
&lt;p&gt;第一步是获取模块配置信息，这一块只要简单使用ngx_http_get_module_loc_conf就可以了。&lt;/p&gt;
&lt;p&gt;第二步是功能逻辑，因为echo模块非常简单，只是简单输出一个字符串，所以这里没有功能逻辑代码。&lt;/p&gt;
&lt;p&gt;第三步是设置response header。Header内容可以通过填充headers_out实现，我们这里只设置了Content-type和Content-length等基本内容，ngx_http_headers_out_t定义了所有可以设置的HTTP Response Header信息：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-c&quot;&gt;typedef struct {
    ngx_list_t                        headers;
    ngx_uint_t                        status;
    ngx_str_t                         status_line;
    ngx_table_elt_t                  *server;
    ngx_table_elt_t                  *date;
    ngx_table_elt_t                  *content_length;
    ngx_table_elt_t                  *content_encoding;
    ngx_table_elt_t                  *location;
    ngx_table_elt_t                  *refresh;
    ngx_table_elt_t                  *last_modified;
    ngx_table_elt_t                  *content_range;
    ngx_table_elt_t                  *accept_ranges;
    ngx_table_elt_t                  *www_authenticate;
    ngx_table_elt_t                  *expires;
    ngx_table_elt_t                  *etag;
    ngx_str_t                        *override_charset;
    size_t                            content_type_len;
    ngx_str_t                         content_type;
    ngx_str_t                         charset;
    u_char                           *content_type_lowcase;
    ngx_uint_t                        content_type_hash;
    ngx_array_t                       cache_control;
    off_t                             content_length_n;
    time_t                            date_time;
    time_t                            last_modified_time;
} ngx_http_headers_out_t;&lt;/pre&gt;
&lt;p&gt;这里并不包含所有HTTP头信息，如果需要可以使用agentzh（春来）开发的Nginx模块&lt;a href=&quot;http://wiki.nginx.org/HttpHeadersMoreModule&quot; target=&quot;_blank&quot;&gt;HttpHeadersMore&lt;/a&gt;在指令中指定更多的Header头信息。&lt;/p&gt;
&lt;p&gt;设置好头信息后使用ngx_http_send_header就可以将头信息输出，ngx_http_send_header接受一个ngx_http_request_t类型的参数。&lt;/p&gt;
&lt;p&gt;第四步也是最重要的一步是输出Response body。这里首先要了解Nginx的I/O机制，Nginx允许handler一次产生一组输出，可以产生多次，Nginx将输出组织成一个单链表结构，链表中的每个节点是一个chain_t，定义在core/ngx_buf.h：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-c&quot;&gt;struct ngx_chain_s {
    ngx_buf_t    *buf;
    ngx_chain_t  *next;
};&lt;/pre&gt;
&lt;p&gt;其中ngx_chain_t是ngx_chain_s的别名，buf为某个数据缓冲区的指针，next指向下一个链表节点，可以看到这是一个非常简单的链表。ngx_buf_t的定义比较长而且很复杂，这里就不贴出来了，请自行参考core/ngx_buf.h。ngx_but_t中比较重要的是pos和last，分别表示要缓冲区数据在内存中的起始地址和结尾地址，这里我们将配置中字符串传进去，last_buf是一个位域，设为1表示此缓冲区是链表中最后一个元素，为0表示后面还有元素。因为我们只有一组数据，所以缓冲区链表中只有一个节点，如果需要输入多组数据可将各组数据放入不同缓冲区后插入到链表。下图展示了Nginx缓冲链表的结构：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/intro-of-nginx-module-development/3.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;缓冲数据准备好后，用ngx_http_output_filter就可以输出了（会送到filter进行各种过滤处理）。ngx_http_output_filter的第一个参数为ngx_http_request_t结构，第二个为输出链表的起始地址&amp;out。ngx_http_out_put_filter会遍历链表，输出所有数据。&lt;/p&gt;
&lt;p&gt;以上就是handler的所有工作，请对照描述理解上面贴出的handler代码。&lt;/p&gt;
&lt;h2&gt;组合Nginx Module&lt;/h2&gt;
&lt;p&gt;上面完成了Nginx模块各种组件的开发下面就是将这些组合起来了。一个Nginx模块被定义为一个ngx_module_t结构，这个结构的字段很多，不过开头和结尾若干字段一般可以通过Nginx内置的宏去填充，下面是我们echo模块的模块主体定义：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-c&quot;&gt;ngx_module_t  ngx_http_echo_module = {
    NGX_MODULE_V1,
    &amp;ngx_http_echo_module_ctx,             /* module context */
    ngx_http_echo_commands,                /* module directives */
    NGX_HTTP_MODULE,                       /* module type */
    NULL,                                  /* init master */
    NULL,                                  /* init module */
    NULL,                                  /* init process */
    NULL,                                  /* init thread */
    NULL,                                  /* exit thread */
    NULL,                                  /* exit process */
    NULL,                                  /* exit master */
    NGX_MODULE_V1_PADDING
};&lt;/pre&gt;
&lt;p&gt;开头和结尾分别用NGX_MODULE_V1和NGX_MODULE_V1_PADDING 填充了若干字段，就不去深究了。这里主要需要填入的信息从上到下以依次为context、指令数组、模块类型以及若干特定事件的回调处理函数（不需要可以置为NULL），其中内容还是比较好理解的，注意我们的echo是一个HTTP模块，所以这里类型是NGX_HTTP_MODULE，其它可用类型还有NGX_EVENT_MODULE（事件处理模块）和NGX_MAIL_MODULE（邮件模块）。&lt;/p&gt;
&lt;p&gt;这样，整个echo模块就写好了，下面给出echo模块的完整代码：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-c&quot;&gt;/*
* Copyright (C) Eric Zhang
*/
#include &lt;ngx_config.h&gt;
#include &lt;ngx_core.h&gt;
#include &lt;ngx_http.h&gt;
/* Module config */
typedef struct {
    ngx_str_t  ed;
} ngx_http_echo_loc_conf_t;
static char *ngx_http_echo(ngx_conf_t *cf, ngx_command_t *cmd, void *conf);
static void *ngx_http_echo_create_loc_conf(ngx_conf_t *cf);
static char *ngx_http_echo_merge_loc_conf(ngx_conf_t *cf, void *parent, void *child);
/* Directives */
static ngx_command_t  ngx_http_echo_commands[] = {
    { ngx_string(&quot;echo&quot;),
        NGX_HTTP_LOC_CONF|NGX_CONF_TAKE1,
        ngx_http_echo,
        NGX_HTTP_LOC_CONF_OFFSET,
        offsetof(ngx_http_echo_loc_conf_t, ed),
        NULL },
        ngx_null_command
};
/* Http context of the module */
static ngx_http_module_t  ngx_http_echo_module_ctx = {
    NULL,                                  /* preconfiguration */
    NULL,                                  /* postconfiguration */
    NULL,                                  /* create main configuration */
    NULL,                                  /* init main configuration */
    NULL,                                  /* create server configuration */
    NULL,                                  /* merge server configuration */
    ngx_http_echo_create_loc_conf,         /* create location configration */
    ngx_http_echo_merge_loc_conf           /* merge location configration */
};
/* Module */
ngx_module_t  ngx_http_echo_module = {
    NGX_MODULE_V1,
    &amp;ngx_http_echo_module_ctx,             /* module context */
    ngx_http_echo_commands,                /* module directives */
    NGX_HTTP_MODULE,                       /* module type */
    NULL,                                  /* init master */
    NULL,                                  /* init module */
    NULL,                                  /* init process */
    NULL,                                  /* init thread */
    NULL,                                  /* exit thread */
    NULL,                                  /* exit process */
    NULL,                                  /* exit master */
    NGX_MODULE_V1_PADDING
};
/* Handler function */
static ngx_int_t
ngx_http_echo_handler(ngx_http_request_t *r)
{
    ngx_int_t rc;
    ngx_buf_t *b;
    ngx_chain_t out;
    ngx_http_echo_loc_conf_t *elcf;
    elcf = ngx_http_get_module_loc_conf(r, ngx_http_echo_module);
    if(!(r-&gt;method &amp; (NGX_HTTP_HEAD|NGX_HTTP_GET|NGX_HTTP_POST)))
    {
        return NGX_HTTP_NOT_ALLOWED;
    }
    r-&gt;headers_out.content_type.len = sizeof(&quot;text/html&quot;) - 1;
    r-&gt;headers_out.content_type.data = (u_char *) &quot;text/html&quot;;
    r-&gt;headers_out.status = NGX_HTTP_OK;
    r-&gt;headers_out.content_length_n = elcf-&gt;ed.len;
    if(r-&gt;method == NGX_HTTP_HEAD)
    {
        rc = ngx_http_send_header(r);
        if(rc != NGX_OK)
        {
            return rc;
        }
    }
    b = ngx_pcalloc(r-&gt;pool, sizeof(ngx_buf_t));
    if(b == NULL)
    {
        ngx_log_error(NGX_LOG_ERR, r-&gt;connection-&gt;log, 0, &quot;Failed to allocate response buffer.&quot;);
        return NGX_HTTP_INTERNAL_SERVER_ERROR;
    }
    out.buf = b;
    out.next = NULL;
    b-&gt;pos = elcf-&gt;ed.data;
    b-&gt;last = elcf-&gt;ed.data + (elcf-&gt;ed.len);
    b-&gt;memory = 1;
    b-&gt;last_buf = 1;
    rc = ngx_http_send_header(r);
    if(rc != NGX_OK)
    {
        return rc;
    }
    return ngx_http_output_filter(r, &amp;out);
}
static char *
ngx_http_echo(ngx_conf_t *cf, ngx_command_t *cmd, void *conf)
{
    ngx_http_core_loc_conf_t  *clcf;
    clcf = ngx_http_conf_get_module_loc_conf(cf, ngx_http_core_module);
    clcf-&gt;handler = ngx_http_echo_handler;
    ngx_conf_set_str_slot(cf,cmd,conf);
    return NGX_CONF_OK;
}
static void *
ngx_http_echo_create_loc_conf(ngx_conf_t *cf)
{
    ngx_http_echo_loc_conf_t  *conf;
    conf = ngx_pcalloc(cf-&gt;pool, sizeof(ngx_http_echo_loc_conf_t));
    if (conf == NULL) {
        return NGX_CONF_ERROR;
    }
    conf-&gt;ed.len = 0;
    conf-&gt;ed.data = NULL;
    return conf;
}
static char *
ngx_http_echo_merge_loc_conf(ngx_conf_t *cf, void *parent, void *child)
{
    ngx_http_echo_loc_conf_t *prev = parent;
    ngx_http_echo_loc_conf_t *conf = child;
    ngx_conf_merge_str_value(conf-&gt;ed, prev-&gt;ed, &quot;&quot;);
    return NGX_CONF_OK;
}&lt;/pre&gt;
&lt;h1&gt;&lt;a name=&quot;section3&quot;&gt;&lt;/a&gt;Nginx模块的安装&lt;/h1&gt;
&lt;p&gt;Nginx不支持动态链接模块，所以安装模块需要将模块代码与Nginx源代码进行重新编译。安装模块的步骤如下：&lt;/p&gt;
&lt;p&gt;1、编写模块config文件，这个文件需要放在和模块源代码文件放在同一目录下。文件内容如下：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-bash&quot;&gt;ngx_addon_name=模块完整名称
HTTP_MODULES=&quot;$HTTP_MODULES 模块完整名称&quot;
NGX_ADDON_SRCS=&quot;$NGX_ADDON_SRCS $ngx_addon_dir/源代码文件名&quot;&lt;/pre&gt;
&lt;p&gt;2、进入Nginx源代码，使用下面命令编译安装&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-bash&quot;&gt;./configure --prefix=安装目录 --add-module=模块源代码文件目录
make
make install&lt;/pre&gt;
&lt;p&gt;这样就完成安装了，例如，我的源代码文件放在/home/yefeng/ngxdev/ngx_http_echo下，我的config文件为：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-bash&quot;&gt;ngx_addon_name=ngx_http_echo_module
HTTP_MODULES=&quot;$HTTP_MODULES ngx_http_echo_module&quot;
NGX_ADDON_SRCS=&quot;$NGX_ADDON_SRCS $ngx_addon_dir/ngx_http_echo_module.c&quot;&lt;/pre&gt;
&lt;p&gt;编译安装命令为：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-bash&quot;&gt;./configure --prefix=/usr/local/nginx --add-module=/home/yefeng/ngxdev/ngx_http_echo
make
sudo make install&lt;/pre&gt;
&lt;p&gt;这样echo模块就被安装在我的Nginx上了，下面测试一下，修改配置文件，增加以下一项配置：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-bash&quot;&gt;location /echo {
    echo &quot;This is my first nginx module!!!&quot;;
}&lt;/pre&gt;
&lt;p&gt;然后用curl测试一下：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums languague-bash&quot;&gt;curl -i http://localhost/echo&lt;/pre&gt;
&lt;p&gt;结果如下：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/intro-of-nginx-module-development/4.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;可以看到模块已经正常工作了，也可以在浏览器中打开网址，就可以看到结果：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/intro-of-nginx-module-development/5.png&quot;/&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a name=&quot;section4&quot;&gt;&lt;/a&gt;更深入的学习&lt;/h1&gt;
&lt;p&gt;本文只是简要介绍了Nginx模块的开发过程，由于篇幅的原因，不能面面俱到。因为目前Nginx的学习资料很少，如果读者希望更深入学习Nginx的原理及模块开发，那么阅读源代码是最好的办法。在Nginx源代码的core/下放有Nginx的核心代码，对理解Nginx的内部工作机制非常有帮助，http/目录下有Nginx HTTP相关的实现，http/module下放有大量内置http模块，可供读者学习模块的开发，另外在&lt;a title=&quot;http://wiki.nginx.org/3rdPartyModules&quot; href=&quot;http://wiki.nginx.org/3rdPartyModules&quot;&gt;http://wiki.nginx.org/3rdPartyModules&lt;/a&gt;上有大量优秀的第三方模块，也是非常好的学习资料。&lt;/p&gt;
&lt;p&gt;如有意见建议或疑问欢迎发送邮件至&lt;a href=&quot;mailto:ericzhang.buaa@gmail.com&quot;&gt;ericzhang.buaa@gmail.com&lt;/a&gt;。希望本文对您有所帮助！！！&lt;/p&gt;
&lt;h1&gt;&lt;a name=&quot;section5&quot;&gt;&lt;/a&gt;参考文献&lt;/h1&gt;
&lt;p&gt;[1] Evan Miller, Emiller's Guide To Nginx Module Development. &lt;a title=&quot;http://www.evanmiller.org/nginx-modules-guide.html&quot; href=&quot;http://www.evanmiller.org/nginx-modules-guide.html&quot;&gt;http://www.evanmiller.org/nginx-modules-guide.html&lt;/a&gt;, 2009&lt;/p&gt;
&lt;p&gt;[2] &lt;a title=&quot;http://wiki.nginx.org/Configuration&quot; href=&quot;http://wiki.nginx.org/Configuration&quot;&gt;http://wiki.nginx.org/Configuration&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3] Clément Nedelcu, Nginx Http Server. Packt Publishing, 2010&lt;/p&gt;
</description>
</item>

    </channel>
</rss>
